{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f48303-26fe-408a-b5e1-78b7519c9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dde42ec-11da-4d0e-b3ad-4d7219300f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc2910e-a305-4d5d-8616-de067743de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>build</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Transcript_ID</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>PTC_pos_codon</th>\n",
       "      <th>PTC_to_start_codon</th>\n",
       "      <th>...</th>\n",
       "      <th>var_token_idx</th>\n",
       "      <th>NMD_efficiency</th>\n",
       "      <th>t_vaf</th>\n",
       "      <th>t_depth</th>\n",
       "      <th>n_vaf</th>\n",
       "      <th>n_depth</th>\n",
       "      <th>VAF_RNA</th>\n",
       "      <th>depth_RNA</th>\n",
       "      <th>VAF_DNA_RNA_ratio</th>\n",
       "      <th>tpm_unstranded_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>944753</td>\n",
       "      <td>944753</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.2191C&gt;T</td>\n",
       "      <td>p.Gln731Ter</td>\n",
       "      <td>731</td>\n",
       "      <td>2193</td>\n",
       "      <td>...</td>\n",
       "      <td>2240</td>\n",
       "      <td>-0.508648</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1.422717</td>\n",
       "      <td>76.8018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>952113</td>\n",
       "      <td>952113</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.1218G&gt;A</td>\n",
       "      <td>p.Trp406Ter</td>\n",
       "      <td>406</td>\n",
       "      <td>1218</td>\n",
       "      <td>...</td>\n",
       "      <td>1267</td>\n",
       "      <td>1.629509</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.323198</td>\n",
       "      <td>49.1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1255304</td>\n",
       "      <td>1255304</td>\n",
       "      <td>UBE2J2</td>\n",
       "      <td>ENST00000349431</td>\n",
       "      <td>c.679G&gt;T</td>\n",
       "      <td>p.Gly227Ter</td>\n",
       "      <td>227</td>\n",
       "      <td>681</td>\n",
       "      <td>...</td>\n",
       "      <td>898</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.464435</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>47.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1338573</td>\n",
       "      <td>1338573</td>\n",
       "      <td>DVL1</td>\n",
       "      <td>ENST00000378888</td>\n",
       "      <td>c.1288G&gt;T</td>\n",
       "      <td>p.Glu430Ter</td>\n",
       "      <td>430</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>1572</td>\n",
       "      <td>1.505167</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.145511</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.352289</td>\n",
       "      <td>50.0535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1387314</td>\n",
       "      <td>1387314</td>\n",
       "      <td>CCNL2</td>\n",
       "      <td>ENST00000400809</td>\n",
       "      <td>c.1480C&gt;T</td>\n",
       "      <td>p.Arg494Ter</td>\n",
       "      <td>494</td>\n",
       "      <td>1482</td>\n",
       "      <td>...</td>\n",
       "      <td>1485</td>\n",
       "      <td>-0.424007</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.546980</td>\n",
       "      <td>298.0</td>\n",
       "      <td>1.341649</td>\n",
       "      <td>27.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>152920717</td>\n",
       "      <td>152920717</td>\n",
       "      <td>ZNF185</td>\n",
       "      <td>ENST00000370268</td>\n",
       "      <td>c.622C&gt;T</td>\n",
       "      <td>p.Gln208Ter</td>\n",
       "      <td>208</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>658</td>\n",
       "      <td>1.785660</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.290043</td>\n",
       "      <td>52.3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>153650171</td>\n",
       "      <td>153650171</td>\n",
       "      <td>DUSP9</td>\n",
       "      <td>ENST00000342782</td>\n",
       "      <td>c.1021G&gt;T</td>\n",
       "      <td>p.Glu341Ter</td>\n",
       "      <td>341</td>\n",
       "      <td>1023</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>-1.360998</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.568627</td>\n",
       "      <td>16.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154030948</td>\n",
       "      <td>154030948</td>\n",
       "      <td>MECP2</td>\n",
       "      <td>ENST00000303391</td>\n",
       "      <td>c.880C&gt;T</td>\n",
       "      <td>p.Arg294Ter</td>\n",
       "      <td>294</td>\n",
       "      <td>882</td>\n",
       "      <td>...</td>\n",
       "      <td>1129</td>\n",
       "      <td>-1.166585</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.244797</td>\n",
       "      <td>11.5544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154354015</td>\n",
       "      <td>154354015</td>\n",
       "      <td>FLNA</td>\n",
       "      <td>ENST00000369850</td>\n",
       "      <td>c.5586C&gt;A</td>\n",
       "      <td>p.Tyr1862Ter</td>\n",
       "      <td>1862</td>\n",
       "      <td>5586</td>\n",
       "      <td>...</td>\n",
       "      <td>5834</td>\n",
       "      <td>0.332048</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.794408</td>\n",
       "      <td>63.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154402790</td>\n",
       "      <td>154402790</td>\n",
       "      <td>DNASE1L1</td>\n",
       "      <td>ENST00000014935</td>\n",
       "      <td>c.826C&gt;T</td>\n",
       "      <td>p.Gln276Ter</td>\n",
       "      <td>276</td>\n",
       "      <td>828</td>\n",
       "      <td>...</td>\n",
       "      <td>1619</td>\n",
       "      <td>3.569856</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>3.9940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4093 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       build chromosome      start        end Hugo_Symbol    Transcript_ID  \\\n",
       "0     GRCh38       chr1     944753     944753       NOC2L  ENST00000327044   \n",
       "1     GRCh38       chr1     952113     952113       NOC2L  ENST00000327044   \n",
       "2     GRCh38       chr1    1255304    1255304      UBE2J2  ENST00000349431   \n",
       "3     GRCh38       chr1    1338573    1338573        DVL1  ENST00000378888   \n",
       "4     GRCh38       chr1    1387314    1387314       CCNL2  ENST00000400809   \n",
       "...      ...        ...        ...        ...         ...              ...   \n",
       "4088  GRCh38       chrX  152920717  152920717      ZNF185  ENST00000370268   \n",
       "4089  GRCh38       chrX  153650171  153650171       DUSP9  ENST00000342782   \n",
       "4090  GRCh38       chrX  154030948  154030948       MECP2  ENST00000303391   \n",
       "4091  GRCh38       chrX  154354015  154354015        FLNA  ENST00000369850   \n",
       "4092  GRCh38       chrX  154402790  154402790    DNASE1L1  ENST00000014935   \n",
       "\n",
       "          HGVSc         HGVSp  PTC_pos_codon  PTC_to_start_codon  ...  \\\n",
       "0     c.2191C>T   p.Gln731Ter            731                2193  ...   \n",
       "1     c.1218G>A   p.Trp406Ter            406                1218  ...   \n",
       "2      c.679G>T   p.Gly227Ter            227                 681  ...   \n",
       "3     c.1288G>T   p.Glu430Ter            430                1290  ...   \n",
       "4     c.1480C>T   p.Arg494Ter            494                1482  ...   \n",
       "...         ...           ...            ...                 ...  ...   \n",
       "4088   c.622C>T   p.Gln208Ter            208                 624  ...   \n",
       "4089  c.1021G>T   p.Glu341Ter            341                1023  ...   \n",
       "4090   c.880C>T   p.Arg294Ter            294                 882  ...   \n",
       "4091  c.5586C>A  p.Tyr1862Ter           1862                5586  ...   \n",
       "4092   c.826C>T   p.Gln276Ter            276                 828  ...   \n",
       "\n",
       "      var_token_idx  NMD_efficiency     t_vaf  t_depth  n_vaf  n_depth  \\\n",
       "0              2240       -0.508648  0.311111     45.0    0.0     43.0   \n",
       "1              1267        1.629509  0.390244     41.0    0.0     14.0   \n",
       "2               898        0.047557  0.480000     50.0    0.0     32.0   \n",
       "3              1572        1.505167  0.413043     46.0    0.0     63.0   \n",
       "4              1485       -0.424007  0.407692    130.0    0.0    152.0   \n",
       "...             ...             ...       ...      ...    ...      ...   \n",
       "4088            658        1.785660  0.328358     67.0    0.0     65.0   \n",
       "4089           1285       -1.360998  0.389313    131.0    0.0    185.0   \n",
       "4090           1129       -1.166585  0.406977     86.0    0.0     70.0   \n",
       "4091           5834        0.332048  0.904762    189.0    0.0    134.0   \n",
       "4092           1619        3.569856  0.339286     56.0    0.0     66.0   \n",
       "\n",
       "       VAF_RNA  depth_RNA VAF_DNA_RNA_ratio tpm_unstranded_x  \n",
       "0     0.442623      244.0          1.422717          76.8018  \n",
       "1     0.126126      111.0          0.323198          49.1731  \n",
       "2     0.464435      239.0          0.967573          47.4110  \n",
       "3     0.145511      323.0          0.352289          50.0535  \n",
       "4     0.546980      298.0          1.341649          27.1883  \n",
       "...        ...        ...               ...              ...  \n",
       "4088  0.095238       42.0          0.290043          52.3474  \n",
       "4089  1.000000       35.0          2.568627          16.9024  \n",
       "4090  0.913580       81.0          2.244797          11.5544  \n",
       "4091  0.718750       32.0          0.794408          63.2541  \n",
       "4092  0.028571      105.0          0.084211           3.9940  \n",
       "\n",
       "[4093 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tcga_processed.tsv.gz', sep='\\t')\n",
    "\n",
    "### some variants are repeated multiple times\n",
    "df = df.drop(columns=['Cancer_type', 'Cancer_type_count', 'NMF_cluster'])\n",
    "\n",
    "agg_columns = [\n",
    "    'NMD_efficiency', 't_vaf', 't_depth', 'n_vaf', 'n_depth',\n",
    "    'VAF_RNA', 'depth_RNA', 'VAF_DNA_RNA_ratio', 'tpm_unstranded_x'\n",
    "]\n",
    "\n",
    "# Group by all other columns except the ones to aggregate\n",
    "group_by_columns = [col for col in df.columns if col not in agg_columns]\n",
    "\n",
    "# Perform grouping and aggregation without dropping NaNs\n",
    "df_unq = df.groupby(\n",
    "    group_by_columns, dropna=False, as_index=False\n",
    ").agg({col: 'mean' for col in agg_columns})\n",
    "\n",
    "df_unq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f3012-8628-4708-9f98-7a247333993a",
   "metadata": {},
   "source": [
    "### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd76f990-e27c-483b-bdd2-0d2dfe8f6f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000330029:c.112C>T',\n",
       "  'ENST00000263121:c.964C>T',\n",
       "  'ENST00000372720:c.733G>T',\n",
       "  'ENST00000248958:c.487C>T',\n",
       "  'ENST00000371711:c.1159G>T'],\n",
       " 232)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var_ids = df_unq[df_unq['chromosome'].isin(['chr20', 'chr21', 'chr22'])]['var_id'].values.tolist()\n",
    "test_var_ids = list(set(test_var_ids))\n",
    "test_var_ids[0:5], len(test_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921ff515-7955-4fa4-a719-141e346e317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000320936:c.130C>T',\n",
       "  'ENST00000294618:c.4402C>T',\n",
       "  'ENST00000263097:c.906C>G',\n",
       "  'ENST00000160298:c.3418G>T',\n",
       "  'ENST00000040663:c.898C>T'],\n",
       " 210)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_var_ids = df_unq[df_unq['chromosome']=='chr19']['var_id'].values.tolist()\n",
    "val_var_ids = list(set(val_var_ids))\n",
    "val_var_ids[0:5], len(val_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28f8153-7e43-4d33-9f7a-8fbe34ad327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000344936:c.2215G>T',\n",
       "  'ENST00000448903:c.2797G>T',\n",
       "  'ENST00000314340:c.346G>T',\n",
       "  'ENST00000324545:c.2224C>T',\n",
       "  'ENST00000296318:c.1576C>T'],\n",
       " 3651)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_var_ids = list(set(df_unq.var_id.values.tolist()) - set(val_var_ids) - set(test_var_ids))\n",
    "train_var_ids[0:5], len(train_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d265b37-0476-48e3-8e1b-d6d693a47d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries with NMD_efficiency as values\n",
    "test_dict = {}\n",
    "val_dict = {}\n",
    "train_dict = {}\n",
    "var_pos_idx_dict = {}\n",
    "\n",
    "for index, row in df_unq.iterrows():\n",
    "    if row['var_id'] in test_var_ids:\n",
    "        test_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    elif row['var_id'] in val_var_ids:\n",
    "        val_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    else:\n",
    "        train_dict[row['var_id']] = row['NMD_efficiency']\n",
    "\n",
    "    var_pos_idx_dict[row['var_id']] = row['var_token_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500916de-9ea7-482c-a762-6e928d6f913e",
   "metadata": {},
   "source": [
    "### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89804ac4-08b3-4aca-a8a0-cb53ef3b9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tcga_ref_embeds.pkl', 'rb') as file:\n",
    "    ref_embeds = pickle.load(file)\n",
    "\n",
    "with open('../data/tcga_alt_embeds.pkl', 'rb') as file:\n",
    "    alt_embeds = pickle.load(file)\n",
    "\n",
    "# create alt - ref embeds\n",
    "altref_embeds = {}\n",
    "\n",
    "for var_id, alt_embed in alt_embeds.items():\n",
    "    altref_embeds[var_id] = alt_embed - ref_embeds[var_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07cd3e-36bd-4795-bb39-4d1b61ae6b42",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2731070d-47cd-42df-a957-9104061bfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "HIDDEN_DIMS_phi = [8, 8]\n",
    "HIDDEN_DIMS_rho = [4, 4]\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "VALIDATION_INTERVAL = 5  # Perform validation every 5 epochs\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Definition\n",
    "# ---------------------------\n",
    "class DeepSetDataset(Dataset):\n",
    "    def __init__(self, embeds_dict, target_dict, var_pos_idx_dict=None):\n",
    "        self.ids = list(target_dict.keys())\n",
    "        self.embeds = [embeds_dict[i] for i in self.ids]\n",
    "        self.targets = [target_dict[i] for i in self.ids]\n",
    "        self.var_pos_idx = [var_pos_idx_dict[i] if var_pos_idx_dict else None for i in self.ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embed = self.embeds[idx]\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        var_pos_idx = self.var_pos_idx[idx]\n",
    "        return embed, target, var_pos_idx\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeds, targets, var_pos_idx = zip(*batch)\n",
    "    embeds = [torch.as_tensor(embed, dtype=torch.float32) for embed in embeds]\n",
    "    embeds_padded = torch.nn.utils.rnn.pad_sequence(embeds, batch_first=True, padding_value=0.0)\n",
    "    targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    mask = (embeds_padded != 0).any(dim=-1).float()\n",
    "    return embeds_padded, targets, mask, var_pos_idx\n",
    "\n",
    "# ---------------------------\n",
    "# DeepSet Model with Masking\n",
    "# ---------------------------\n",
    "class DeepSetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        phi_hidden_dims: List[int],\n",
    "        rho_hidden_dims: List[int],\n",
    "        output_dim: int = 1,\n",
    "        activation: str = \"ReLU\",\n",
    "        pool: str = \"mean\",\n",
    "        dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        self.pool = pool\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        \n",
    "        # Phi network\n",
    "        phi = []\n",
    "        in_dim = input_dim\n",
    "        for i, dim in enumerate(phi_hidden_dims):\n",
    "            phi.append((f\"phi_linear_{i}\", nn.Linear(in_dim, dim)))\n",
    "            if dropout:\n",
    "                phi.append((f\"phi_dropout_{i}\", self.dropout))\n",
    "            phi.append((f\"phi_activation_{i}\", self.activation))\n",
    "            in_dim = dim\n",
    "        self.phi = nn.Sequential(OrderedDict(phi))\n",
    "        \n",
    "        # Rho network\n",
    "        rho = []\n",
    "        for i, dim in enumerate(rho_hidden_dims[:-1]):\n",
    "            rho.append((f\"rho_linear_{i}\", nn.Linear(in_dim, dim)))\n",
    "            if dropout:\n",
    "                rho.append((f\"rho_dropout_{i}\", self.dropout))\n",
    "            rho.append((f\"rho_activation_{i}\", self.activation))\n",
    "            in_dim = dim\n",
    "        rho.append((f\"rho_linear_{len(rho_hidden_dims) - 1}\", nn.Linear(in_dim, output_dim)))\n",
    "        self.rho = nn.Sequential(OrderedDict(rho))\n",
    "\n",
    "    def forward(self, x, mask, var_pos_idx=None):\n",
    "        x = self.phi(x)\n",
    "        \n",
    "        if self.pool == \"sum\":\n",
    "            x = (x * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        elif self.pool == \"max\":\n",
    "            x = (x * mask.unsqueeze(-1)).masked_fill(mask.unsqueeze(-1) == 0, float('-inf')).max(dim=1).values\n",
    "        elif self.pool == \"mean\":\n",
    "            x = (x * mask.unsqueeze(-1)).sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        elif self.pool == \"token\" and var_pos_idx is not None:\n",
    "            x = torch.stack([x[i, idx] for i, idx in enumerate(var_pos_idx)])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling operation: {self.pool}\")\n",
    "        \n",
    "        return self.rho(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation Function\n",
    "# ---------------------------\n",
    "def evaluate_regression_metrics(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy().flatten()\n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    loss = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(loss)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        spearman_corr = np.nan\n",
    "\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        pearson_corr = np.nan\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'pearson_corr': pearson_corr,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_model(train_dict, val_dict, test_dict, embeds_dict, var_pos_idx_dict=None, pool='mean'):\n",
    "    train_loader = DataLoader(DeepSetDataset(embeds_dict, train_dict, var_pos_idx_dict), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(DeepSetDataset(embeds_dict, val_dict, var_pos_idx_dict), batch_size=32, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(DeepSetDataset(embeds_dict, test_dict, var_pos_idx_dict), batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    model = DeepSetModel(\n",
    "        input_dim=embeds_dict[list(embeds_dict.keys())[0]].shape[1],\n",
    "        phi_hidden_dims=HIDDEN_DIMS_phi,\n",
    "        rho_hidden_dims=HIDDEN_DIMS_rho,\n",
    "        pool=pool,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    metrics = {'epoch': [], 'phase': [], 'loss': [], 'spearman_corr': [], 'pearson_corr':[], 'mae': [], 'rmse': [], 'r2': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "            \n",
    "            all_preds, all_targets = [], []\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for embeds, targets, mask, var_pos_idx in loader:\n",
    "                embeds, targets, mask = embeds.to(device), targets.to(device), mask.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(embeds, mask, var_pos_idx).squeeze()\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * embeds.size(0)\n",
    "                all_preds.append(outputs.detach())\n",
    "                all_targets.append(targets.detach())\n",
    "            \n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            \n",
    "            epoch_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "            epoch_metrics['loss'] = epoch_loss\n",
    "            \n",
    "            # Early stopping for validation phase\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        metrics_df = pd.DataFrame(metrics)\n",
    "                        test_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "                        test_metrics['loss'] = epoch_loss\n",
    "                        return metrics_df, test_metrics\n",
    "            \n",
    "            # Log metrics every 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}] Phase: {phase:5s} | \"\n",
    "                    f\"Loss: {epoch_metrics['loss']:.4f} | \"\n",
    "                    f\"Spearman: {epoch_metrics['spearman_corr']:.4f} | \"\n",
    "                    f\"Pearson: {epoch_metrics['pearson_corr']:.4f} | \"\n",
    "                    f\"MAE: {epoch_metrics['mae']:.4f} | \"\n",
    "                    f\"RMSE: {epoch_metrics['rmse']:.4f} | \"\n",
    "                    f\"R²: {epoch_metrics['r2']:.4f}\"\n",
    "                )\n",
    "                metrics['epoch'].append(epoch)\n",
    "                metrics['phase'].append(phase)\n",
    "                for key, value in epoch_metrics.items():\n",
    "                    metrics[key].append(value)\n",
    "    \n",
    "    # Final Test Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for embeds, targets, mask, var_pos_idx in test_loader:\n",
    "        embeds, targets, mask = embeds.to(device), targets.to(device), mask.to(device)\n",
    "        outputs = model(embeds, mask, var_pos_idx).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item() * embeds.size(0)\n",
    "        all_preds.append(outputs.detach())\n",
    "        all_targets.append(targets.detach())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    test_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "    test_metrics['loss'] = epoch_loss\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab1bc25-546c-4d53-b7c5-8436732ee7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING REF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5912 | Spearman: 0.0930 | Pearson: 0.0777 | MAE: 1.0420 | RMSE: 1.2614 | R²: -0.0055\n",
      "[Epoch 005] Phase: val   | Loss: 1.7104 | Spearman: 0.2835 | Pearson: 0.3280 | MAE: 1.0405 | RMSE: 1.3078 | R²: 0.0251\n",
      "[Epoch 010] Phase: train | Loss: 1.4423 | Spearman: 0.2991 | Pearson: 0.3015 | MAE: 0.9877 | RMSE: 1.2010 | R²: 0.0886\n",
      "[Epoch 010] Phase: val   | Loss: 1.5250 | Spearman: 0.4085 | Pearson: 0.4143 | MAE: 1.0059 | RMSE: 1.2349 | R²: 0.1308\n",
      "[Epoch 015] Phase: train | Loss: 1.4263 | Spearman: 0.3190 | Pearson: 0.3147 | MAE: 0.9816 | RMSE: 1.1943 | R²: 0.0988\n",
      "[Epoch 015] Phase: val   | Loss: 1.4949 | Spearman: 0.4306 | Pearson: 0.4316 | MAE: 1.0048 | RMSE: 1.2227 | R²: 0.1479\n",
      "[Epoch 020] Phase: train | Loss: 1.4333 | Spearman: 0.3255 | Pearson: 0.3081 | MAE: 0.9812 | RMSE: 1.1972 | R²: 0.0944\n",
      "[Epoch 020] Phase: val   | Loss: 1.4692 | Spearman: 0.4449 | Pearson: 0.4445 | MAE: 0.9927 | RMSE: 1.2121 | R²: 0.1626\n",
      "[Epoch 025] Phase: train | Loss: 1.4098 | Spearman: 0.3376 | Pearson: 0.3307 | MAE: 0.9745 | RMSE: 1.1873 | R²: 0.1092\n",
      "[Epoch 025] Phase: val   | Loss: 1.4570 | Spearman: 0.4443 | Pearson: 0.4515 | MAE: 0.9876 | RMSE: 1.2071 | R²: 0.1695\n",
      "[Epoch 030] Phase: train | Loss: 1.4100 | Spearman: 0.3426 | Pearson: 0.3303 | MAE: 0.9738 | RMSE: 1.1874 | R²: 0.1091\n",
      "[Epoch 030] Phase: val   | Loss: 1.4924 | Spearman: 0.4525 | Pearson: 0.4294 | MAE: 0.9789 | RMSE: 1.2216 | R²: 0.1493\n",
      "[Epoch 035] Phase: train | Loss: 1.3969 | Spearman: 0.3473 | Pearson: 0.3430 | MAE: 0.9726 | RMSE: 1.1819 | R²: 0.1173\n",
      "[Epoch 035] Phase: val   | Loss: 1.4646 | Spearman: 0.4506 | Pearson: 0.4533 | MAE: 0.9968 | RMSE: 1.2102 | R²: 0.1652\n",
      "[Epoch 040] Phase: train | Loss: 1.3996 | Spearman: 0.3457 | Pearson: 0.3401 | MAE: 0.9714 | RMSE: 1.1831 | R²: 0.1156\n",
      "[Epoch 040] Phase: val   | Loss: 1.4768 | Spearman: 0.4460 | Pearson: 0.4329 | MAE: 0.9801 | RMSE: 1.2152 | R²: 0.1583\n",
      "Early stopping at epoch 41\n",
      "START TRAINING REF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.6542 | Spearman: 0.0879 | Pearson: 0.0312 | MAE: 1.0520 | RMSE: 1.2861 | R²: -0.0452\n",
      "[Epoch 005] Phase: val   | Loss: 1.7659 | Spearman: 0.3903 | Pearson: 0.3372 | MAE: 1.0372 | RMSE: 1.3289 | R²: -0.0065\n",
      "[Epoch 010] Phase: train | Loss: 1.5485 | Spearman: 0.1905 | Pearson: 0.1796 | MAE: 1.0178 | RMSE: 1.2444 | R²: 0.0215\n",
      "[Epoch 010] Phase: val   | Loss: 1.6555 | Spearman: 0.4538 | Pearson: 0.4502 | MAE: 1.0019 | RMSE: 1.2867 | R²: 0.0564\n",
      "[Epoch 015] Phase: train | Loss: 1.4337 | Spearman: 0.3066 | Pearson: 0.3151 | MAE: 0.9711 | RMSE: 1.1974 | R²: 0.0941\n",
      "[Epoch 015] Phase: val   | Loss: 1.5377 | Spearman: 0.4761 | Pearson: 0.4725 | MAE: 0.9799 | RMSE: 1.2400 | R²: 0.1236\n",
      "[Epoch 020] Phase: train | Loss: 1.4063 | Spearman: 0.3309 | Pearson: 0.3391 | MAE: 0.9594 | RMSE: 1.1859 | R²: 0.1114\n",
      "[Epoch 020] Phase: val   | Loss: 1.5221 | Spearman: 0.4789 | Pearson: 0.4808 | MAE: 0.9689 | RMSE: 1.2337 | R²: 0.1324\n",
      "[Epoch 025] Phase: train | Loss: 1.3871 | Spearman: 0.3424 | Pearson: 0.3559 | MAE: 0.9580 | RMSE: 1.1777 | R²: 0.1235\n",
      "[Epoch 025] Phase: val   | Loss: 1.5056 | Spearman: 0.4682 | Pearson: 0.4612 | MAE: 0.9663 | RMSE: 1.2270 | R²: 0.1418\n",
      "[Epoch 030] Phase: train | Loss: 1.3451 | Spearman: 0.3685 | Pearson: 0.3886 | MAE: 0.9460 | RMSE: 1.1598 | R²: 0.1501\n",
      "[Epoch 030] Phase: val   | Loss: 1.5038 | Spearman: 0.4763 | Pearson: 0.4632 | MAE: 0.9716 | RMSE: 1.2263 | R²: 0.1428\n",
      "[Epoch 035] Phase: train | Loss: 1.3279 | Spearman: 0.3860 | Pearson: 0.4019 | MAE: 0.9377 | RMSE: 1.1523 | R²: 0.1609\n",
      "[Epoch 035] Phase: val   | Loss: 1.4981 | Spearman: 0.4515 | Pearson: 0.4523 | MAE: 0.9773 | RMSE: 1.2240 | R²: 0.1461\n",
      "[Epoch 040] Phase: train | Loss: 1.3000 | Spearman: 0.4026 | Pearson: 0.4236 | MAE: 0.9275 | RMSE: 1.1402 | R²: 0.1786\n",
      "[Epoch 040] Phase: val   | Loss: 1.5031 | Spearman: 0.4557 | Pearson: 0.4596 | MAE: 0.9796 | RMSE: 1.2260 | R²: 0.1433\n",
      "Early stopping at epoch 43\n",
      "START TRAINING REF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.7730 | Spearman: 0.0555 | Pearson: 0.0572 | MAE: 1.0717 | RMSE: 1.3315 | R²: -0.1203\n",
      "[Epoch 005] Phase: val   | Loss: 1.8117 | Spearman: -0.0021 | Pearson: -0.0175 | MAE: 1.0577 | RMSE: 1.3460 | R²: -0.0326\n",
      "[Epoch 010] Phase: train | Loss: 1.6689 | Spearman: 0.0884 | Pearson: 0.1049 | MAE: 1.0528 | RMSE: 1.2919 | R²: -0.0545\n",
      "[Epoch 010] Phase: val   | Loss: 1.7356 | Spearman: 0.1559 | Pearson: 0.1340 | MAE: 1.0531 | RMSE: 1.3174 | R²: 0.0108\n",
      "[Epoch 015] Phase: train | Loss: 1.6006 | Spearman: 0.1366 | Pearson: 0.1501 | MAE: 1.0307 | RMSE: 1.2651 | R²: -0.0114\n",
      "[Epoch 015] Phase: val   | Loss: 1.7191 | Spearman: 0.1627 | Pearson: 0.1450 | MAE: 1.0618 | RMSE: 1.3112 | R²: 0.0201\n",
      "[Epoch 020] Phase: train | Loss: 1.5423 | Spearman: 0.1770 | Pearson: 0.1972 | MAE: 1.0215 | RMSE: 1.2419 | R²: 0.0255\n",
      "[Epoch 020] Phase: val   | Loss: 1.6936 | Spearman: 0.2011 | Pearson: 0.2053 | MAE: 1.0598 | RMSE: 1.3014 | R²: 0.0347\n",
      "[Epoch 025] Phase: train | Loss: 1.4821 | Spearman: 0.2377 | Pearson: 0.2588 | MAE: 1.0000 | RMSE: 1.2174 | R²: 0.0635\n",
      "[Epoch 025] Phase: val   | Loss: 1.6971 | Spearman: 0.2202 | Pearson: 0.1842 | MAE: 1.0658 | RMSE: 1.3027 | R²: 0.0327\n",
      "[Epoch 030] Phase: train | Loss: 1.4602 | Spearman: 0.2625 | Pearson: 0.2833 | MAE: 0.9921 | RMSE: 1.2084 | R²: 0.0773\n",
      "[Epoch 030] Phase: val   | Loss: 1.6898 | Spearman: 0.2223 | Pearson: 0.1939 | MAE: 1.0577 | RMSE: 1.2999 | R²: 0.0369\n",
      "[Epoch 035] Phase: train | Loss: 1.4412 | Spearman: 0.2813 | Pearson: 0.3020 | MAE: 0.9897 | RMSE: 1.2005 | R²: 0.0893\n",
      "[Epoch 035] Phase: val   | Loss: 1.7299 | Spearman: 0.1321 | Pearson: 0.1296 | MAE: 1.0644 | RMSE: 1.3152 | R²: 0.0140\n",
      "Early stopping at epoch 37\n",
      "START TRAINING REF SUM\n",
      "[Epoch 005] Phase: train | Loss: 84.8235 | Spearman: 0.0237 | Pearson: 0.0018 | MAE: 6.3592 | RMSE: 9.2100 | R²: -52.5983\n",
      "[Epoch 005] Phase: val   | Loss: 17.6494 | Spearman: 0.1325 | Pearson: 0.1408 | MAE: 3.4413 | RMSE: 4.2011 | R²: -9.0598\n",
      "[Epoch 010] Phase: train | Loss: 10.3643 | Spearman: 0.0156 | Pearson: 0.0119 | MAE: 2.2252 | RMSE: 3.2194 | R²: -5.5490\n",
      "[Epoch 010] Phase: val   | Loss: 5.7557 | Spearman: 0.1756 | Pearson: 0.1579 | MAE: 1.9403 | RMSE: 2.3991 | R²: -2.2806\n",
      "[Epoch 015] Phase: train | Loss: 3.7427 | Spearman: 0.0433 | Pearson: 0.0156 | MAE: 1.4891 | RMSE: 1.9346 | R²: -1.3649\n",
      "[Epoch 015] Phase: val   | Loss: 3.9386 | Spearman: 0.2118 | Pearson: 0.1995 | MAE: 1.5647 | RMSE: 1.9846 | R²: -1.2449\n",
      "[Epoch 020] Phase: train | Loss: 3.4736 | Spearman: 0.0254 | Pearson: 0.0198 | MAE: 1.4287 | RMSE: 1.8638 | R²: -1.1949\n",
      "[Epoch 020] Phase: val   | Loss: 3.6301 | Spearman: 0.2163 | Pearson: 0.1983 | MAE: 1.4721 | RMSE: 1.9053 | R²: -1.0691\n",
      "[Epoch 025] Phase: train | Loss: 3.2442 | Spearman: 0.0360 | Pearson: 0.0366 | MAE: 1.3826 | RMSE: 1.8012 | R²: -1.0499\n",
      "[Epoch 025] Phase: val   | Loss: 3.3882 | Spearman: 0.2455 | Pearson: 0.1962 | MAE: 1.4006 | RMSE: 1.8407 | R²: -0.9312\n",
      "[Epoch 030] Phase: train | Loss: 3.0860 | Spearman: 0.0257 | Pearson: 0.0385 | MAE: 1.3439 | RMSE: 1.7567 | R²: -0.9500\n",
      "[Epoch 030] Phase: val   | Loss: 3.1625 | Spearman: 0.2522 | Pearson: 0.2091 | MAE: 1.3346 | RMSE: 1.7783 | R²: -0.8026\n",
      "[Epoch 035] Phase: train | Loss: 2.8945 | Spearman: 0.0329 | Pearson: 0.0415 | MAE: 1.2881 | RMSE: 1.7013 | R²: -0.8290\n",
      "[Epoch 035] Phase: val   | Loss: 2.8980 | Spearman: 0.2371 | Pearson: 0.1857 | MAE: 1.2533 | RMSE: 1.7023 | R²: -0.6518\n",
      "[Epoch 040] Phase: train | Loss: 2.6809 | Spearman: 0.0199 | Pearson: 0.0543 | MAE: 1.2369 | RMSE: 1.6373 | R²: -0.6940\n",
      "[Epoch 040] Phase: val   | Loss: 2.7111 | Spearman: 0.2262 | Pearson: 0.2003 | MAE: 1.2082 | RMSE: 1.6465 | R²: -0.5453\n",
      "[Epoch 045] Phase: train | Loss: 2.4399 | Spearman: 0.0471 | Pearson: 0.0625 | MAE: 1.1817 | RMSE: 1.5620 | R²: -0.5417\n",
      "[Epoch 045] Phase: val   | Loss: 2.4138 | Spearman: 0.3068 | Pearson: 0.1799 | MAE: 1.1188 | RMSE: 1.5536 | R²: -0.3758\n",
      "[Epoch 050] Phase: train | Loss: 2.2340 | Spearman: 0.0499 | Pearson: 0.0451 | MAE: 1.1356 | RMSE: 1.4946 | R²: -0.4116\n",
      "[Epoch 050] Phase: val   | Loss: 2.2413 | Spearman: 0.2739 | Pearson: 0.2179 | MAE: 1.0841 | RMSE: 1.4971 | R²: -0.2775\n"
     ]
    }
   ],
   "source": [
    "### Ref embeds\n",
    "\n",
    "print(\"START TRAINING REF MEAN\")\n",
    "ref_mean_metrics_df, ref_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "ref_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset/ref_mean_history_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF MAX\")\n",
    "ref_max_metrics_df, ref_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "ref_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset/ref_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF TOKEN\")\n",
    "ref_token_metrics_df, ref_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "ref_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset/ref_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF SUM\")\n",
    "ref_sum_metrics_df, ref_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "ref_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset/ref_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe4d1d9-ab8e-4751-9113-413d9464b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALT MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.6969 | Spearman: 0.0598 | Pearson: 0.0301 | MAE: 1.0633 | RMSE: 1.3027 | R²: -0.0722\n",
      "[Epoch 005] Phase: val   | Loss: 1.7039 | Spearman: 0.3763 | Pearson: 0.3787 | MAE: 1.0497 | RMSE: 1.3053 | R²: 0.0288\n",
      "[Epoch 010] Phase: train | Loss: 1.5557 | Spearman: 0.1841 | Pearson: 0.1876 | MAE: 1.0194 | RMSE: 1.2473 | R²: 0.0170\n",
      "[Epoch 010] Phase: val   | Loss: 1.5447 | Spearman: 0.4373 | Pearson: 0.4366 | MAE: 0.9931 | RMSE: 1.2429 | R²: 0.1195\n",
      "[Epoch 015] Phase: train | Loss: 1.4891 | Spearman: 0.2725 | Pearson: 0.2714 | MAE: 0.9895 | RMSE: 1.2203 | R²: 0.0591\n",
      "[Epoch 015] Phase: val   | Loss: 1.4485 | Spearman: 0.4581 | Pearson: 0.4572 | MAE: 0.9486 | RMSE: 1.2035 | R²: 0.1744\n",
      "[Epoch 020] Phase: train | Loss: 1.4246 | Spearman: 0.3320 | Pearson: 0.3294 | MAE: 0.9621 | RMSE: 1.1935 | R²: 0.0999\n",
      "[Epoch 020] Phase: val   | Loss: 1.4038 | Spearman: 0.4647 | Pearson: 0.4683 | MAE: 0.9319 | RMSE: 1.1848 | R²: 0.1998\n",
      "[Epoch 025] Phase: train | Loss: 1.4046 | Spearman: 0.3500 | Pearson: 0.3440 | MAE: 0.9565 | RMSE: 1.1851 | R²: 0.1125\n",
      "[Epoch 025] Phase: val   | Loss: 1.3781 | Spearman: 0.4782 | Pearson: 0.4791 | MAE: 0.9180 | RMSE: 1.1739 | R²: 0.2145\n",
      "[Epoch 030] Phase: train | Loss: 1.3866 | Spearman: 0.3621 | Pearson: 0.3548 | MAE: 0.9532 | RMSE: 1.1775 | R²: 0.1238\n",
      "[Epoch 030] Phase: val   | Loss: 1.3909 | Spearman: 0.4847 | Pearson: 0.4846 | MAE: 0.9238 | RMSE: 1.1793 | R²: 0.2072\n",
      "[Epoch 035] Phase: train | Loss: 1.3814 | Spearman: 0.3560 | Pearson: 0.3591 | MAE: 0.9537 | RMSE: 1.1754 | R²: 0.1271\n",
      "[Epoch 035] Phase: val   | Loss: 1.4158 | Spearman: 0.4904 | Pearson: 0.4828 | MAE: 0.9185 | RMSE: 1.1899 | R²: 0.1930\n",
      "[Epoch 040] Phase: train | Loss: 1.3469 | Spearman: 0.3885 | Pearson: 0.3875 | MAE: 0.9364 | RMSE: 1.1606 | R²: 0.1489\n",
      "[Epoch 040] Phase: val   | Loss: 1.3536 | Spearman: 0.5007 | Pearson: 0.4929 | MAE: 0.9154 | RMSE: 1.1635 | R²: 0.2285\n",
      "[Epoch 045] Phase: train | Loss: 1.3468 | Spearman: 0.3881 | Pearson: 0.3869 | MAE: 0.9386 | RMSE: 1.1605 | R²: 0.1490\n",
      "[Epoch 045] Phase: val   | Loss: 1.3497 | Spearman: 0.5044 | Pearson: 0.4949 | MAE: 0.9143 | RMSE: 1.1618 | R²: 0.2307\n",
      "[Epoch 050] Phase: train | Loss: 1.3375 | Spearman: 0.3896 | Pearson: 0.3941 | MAE: 0.9374 | RMSE: 1.1565 | R²: 0.1548\n",
      "[Epoch 050] Phase: val   | Loss: 1.3405 | Spearman: 0.5081 | Pearson: 0.4959 | MAE: 0.9136 | RMSE: 1.1578 | R²: 0.2359\n",
      "START TRAINING ALT MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.7127 | Spearman: 0.0370 | Pearson: 0.0826 | MAE: 1.0473 | RMSE: 1.3087 | R²: -0.0822\n",
      "[Epoch 005] Phase: val   | Loss: 1.7891 | Spearman: 0.3243 | Pearson: 0.3591 | MAE: 1.0300 | RMSE: 1.3376 | R²: -0.0198\n",
      "[Epoch 010] Phase: train | Loss: 1.5834 | Spearman: 0.1664 | Pearson: 0.2257 | MAE: 1.0204 | RMSE: 1.2583 | R²: -0.0005\n",
      "[Epoch 010] Phase: val   | Loss: 1.6611 | Spearman: 0.4150 | Pearson: 0.4103 | MAE: 1.0117 | RMSE: 1.2888 | R²: 0.0532\n",
      "[Epoch 015] Phase: train | Loss: 1.4923 | Spearman: 0.2252 | Pearson: 0.2832 | MAE: 0.9935 | RMSE: 1.2216 | R²: 0.0571\n",
      "[Epoch 015] Phase: val   | Loss: 1.5687 | Spearman: 0.4406 | Pearson: 0.4546 | MAE: 0.9939 | RMSE: 1.2525 | R²: 0.1059\n",
      "[Epoch 020] Phase: train | Loss: 1.4573 | Spearman: 0.2248 | Pearson: 0.2944 | MAE: 0.9859 | RMSE: 1.2072 | R²: 0.0792\n",
      "[Epoch 020] Phase: val   | Loss: 1.5638 | Spearman: 0.4753 | Pearson: 0.4549 | MAE: 1.0091 | RMSE: 1.2505 | R²: 0.1087\n",
      "[Epoch 025] Phase: train | Loss: 1.4143 | Spearman: 0.2753 | Pearson: 0.3330 | MAE: 0.9718 | RMSE: 1.1892 | R²: 0.1064\n",
      "[Epoch 025] Phase: val   | Loss: 1.5407 | Spearman: 0.4237 | Pearson: 0.4364 | MAE: 0.9986 | RMSE: 1.2412 | R²: 0.1218\n",
      "[Epoch 030] Phase: train | Loss: 1.4186 | Spearman: 0.2572 | Pearson: 0.3233 | MAE: 0.9742 | RMSE: 1.1910 | R²: 0.1036\n",
      "[Epoch 030] Phase: val   | Loss: 1.5474 | Spearman: 0.4065 | Pearson: 0.4199 | MAE: 1.0053 | RMSE: 1.2439 | R²: 0.1180\n",
      "Early stopping at epoch 33\n",
      "START TRAINING ALT TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 2.1155 | Spearman: 0.0395 | Pearson: 0.0480 | MAE: 1.1411 | RMSE: 1.4545 | R²: -0.3368\n",
      "[Epoch 005] Phase: val   | Loss: 1.9186 | Spearman: 0.1567 | Pearson: 0.0753 | MAE: 1.0397 | RMSE: 1.3851 | R²: -0.0936\n",
      "[Epoch 010] Phase: train | Loss: 1.9041 | Spearman: 0.0771 | Pearson: 0.0822 | MAE: 1.0900 | RMSE: 1.3799 | R²: -0.2031\n",
      "[Epoch 010] Phase: val   | Loss: 1.8059 | Spearman: 0.2657 | Pearson: 0.1892 | MAE: 1.0237 | RMSE: 1.3438 | R²: -0.0293\n",
      "[Epoch 015] Phase: train | Loss: 1.7687 | Spearman: 0.0992 | Pearson: 0.0941 | MAE: 1.0512 | RMSE: 1.3299 | R²: -0.1176\n",
      "[Epoch 015] Phase: val   | Loss: 1.7339 | Spearman: 0.2905 | Pearson: 0.2328 | MAE: 1.0255 | RMSE: 1.3168 | R²: 0.0117\n",
      "[Epoch 020] Phase: train | Loss: 1.6434 | Spearman: 0.1374 | Pearson: 0.1381 | MAE: 1.0384 | RMSE: 1.2820 | R²: -0.0385\n",
      "[Epoch 020] Phase: val   | Loss: 1.7247 | Spearman: 0.2930 | Pearson: 0.2061 | MAE: 1.0343 | RMSE: 1.3133 | R²: 0.0170\n",
      "[Epoch 025] Phase: train | Loss: 1.5769 | Spearman: 0.1554 | Pearson: 0.1644 | MAE: 1.0233 | RMSE: 1.2557 | R²: 0.0036\n",
      "[Epoch 025] Phase: val   | Loss: 1.6854 | Spearman: 0.2881 | Pearson: 0.2105 | MAE: 1.0524 | RMSE: 1.2982 | R²: 0.0394\n",
      "[Epoch 030] Phase: train | Loss: 1.5001 | Spearman: 0.2035 | Pearson: 0.2424 | MAE: 1.0053 | RMSE: 1.2248 | R²: 0.0521\n",
      "[Epoch 030] Phase: val   | Loss: 1.7205 | Spearman: 0.2226 | Pearson: 0.1414 | MAE: 1.0512 | RMSE: 1.3117 | R²: 0.0193\n",
      "[Epoch 035] Phase: train | Loss: 1.4912 | Spearman: 0.2430 | Pearson: 0.2555 | MAE: 1.0001 | RMSE: 1.2212 | R²: 0.0577\n",
      "Early stopping at epoch 35\n",
      "START TRAINING ALT SUM\n",
      "[Epoch 005] Phase: train | Loss: 2.6360 | Spearman: -0.0071 | Pearson: -0.0033 | MAE: 1.2229 | RMSE: 1.6236 | R²: -0.6657\n",
      "[Epoch 005] Phase: val   | Loss: 2.6189 | Spearman: nan | Pearson: nan | MAE: 1.1608 | RMSE: 1.6183 | R²: -0.4927\n",
      "[Epoch 010] Phase: train | Loss: 2.5993 | Spearman: -0.0023 | Pearson: 0.0174 | MAE: 1.2143 | RMSE: 1.6122 | R²: -0.6424\n",
      "[Epoch 010] Phase: val   | Loss: 2.5912 | Spearman: nan | Pearson: nan | MAE: 1.1540 | RMSE: 1.6097 | R²: -0.4770\n",
      "[Epoch 015] Phase: train | Loss: 2.5575 | Spearman: -0.0191 | Pearson: -0.0028 | MAE: 1.2048 | RMSE: 1.5992 | R²: -0.6160\n",
      "[Epoch 015] Phase: val   | Loss: 2.5501 | Spearman: nan | Pearson: nan | MAE: 1.1442 | RMSE: 1.5969 | R²: -0.4535\n",
      "[Epoch 020] Phase: train | Loss: 2.4964 | Spearman: -0.0099 | Pearson: -0.0110 | MAE: 1.1902 | RMSE: 1.5800 | R²: -0.5774\n",
      "[Epoch 020] Phase: val   | Loss: 2.4951 | Spearman: nan | Pearson: nan | MAE: 1.1319 | RMSE: 1.5796 | R²: -0.4222\n",
      "[Epoch 025] Phase: train | Loss: 2.4236 | Spearman: -0.0199 | Pearson: -0.0196 | MAE: 1.1742 | RMSE: 1.5568 | R²: -0.5314\n",
      "[Epoch 025] Phase: val   | Loss: 2.4256 | Spearman: nan | Pearson: nan | MAE: 1.1170 | RMSE: 1.5574 | R²: -0.3825\n",
      "[Epoch 030] Phase: train | Loss: 2.3375 | Spearman: -0.0172 | Pearson: -0.0188 | MAE: 1.1559 | RMSE: 1.5289 | R²: -0.4770\n",
      "[Epoch 030] Phase: val   | Loss: 2.3421 | Spearman: nan | Pearson: nan | MAE: 1.1015 | RMSE: 1.5304 | R²: -0.3350\n",
      "[Epoch 035] Phase: train | Loss: 2.2280 | Spearman: 0.0159 | Pearson: -0.0044 | MAE: 1.1329 | RMSE: 1.4926 | R²: -0.4078\n",
      "[Epoch 035] Phase: val   | Loss: 2.2469 | Spearman: nan | Pearson: nan | MAE: 1.0848 | RMSE: 1.4990 | R²: -0.2807\n",
      "[Epoch 040] Phase: train | Loss: 2.1065 | Spearman: 0.0023 | Pearson: -0.0155 | MAE: 1.1092 | RMSE: 1.4514 | R²: -0.3310\n",
      "[Epoch 040] Phase: val   | Loss: 2.1427 | Spearman: nan | Pearson: nan | MAE: 1.0682 | RMSE: 1.4638 | R²: -0.2213\n",
      "[Epoch 045] Phase: train | Loss: 1.9835 | Spearman: 0.0207 | Pearson: 0.0188 | MAE: 1.0865 | RMSE: 1.4084 | R²: -0.2533\n",
      "[Epoch 045] Phase: val   | Loss: 2.0381 | Spearman: nan | Pearson: nan | MAE: 1.0525 | RMSE: 1.4276 | R²: -0.1617\n",
      "[Epoch 050] Phase: train | Loss: 1.8680 | Spearman: 0.0062 | Pearson: -0.0057 | MAE: 1.0680 | RMSE: 1.3667 | R²: -0.1803\n",
      "[Epoch 050] Phase: val   | Loss: 1.9409 | Spearman: nan | Pearson: nan | MAE: 1.0444 | RMSE: 1.3932 | R²: -0.1063\n"
     ]
    }
   ],
   "source": [
    "### Alt embeds\n",
    "\n",
    "print(\"START TRAINING ALT MEAN\")\n",
    "alt_mean_metrics_df, alt_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "alt_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset/alt_mean_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT MAX\")\n",
    "alt_max_metrics_df, alt_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "alt_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset/alt_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT TOKEN\")\n",
    "alt_token_metrics_df, alt_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "alt_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset/alt_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT SUM\")\n",
    "alt_sum_metrics_df, alt_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "alt_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset/alt_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd77d45-1703-4072-834f-96b0d8cf0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALTREF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5918 | Spearman: 0.0728 | Pearson: 0.0310 | MAE: 1.0506 | RMSE: 1.2616 | R²: -0.0058\n",
      "[Epoch 005] Phase: val   | Loss: 1.7403 | Spearman: 0.3977 | Pearson: 0.3455 | MAE: 1.0793 | RMSE: 1.3192 | R²: 0.0080\n",
      "[Epoch 010] Phase: train | Loss: 1.5116 | Spearman: 0.2476 | Pearson: 0.2381 | MAE: 1.0206 | RMSE: 1.2295 | R²: 0.0449\n",
      "[Epoch 010] Phase: val   | Loss: 1.6710 | Spearman: 0.3605 | Pearson: 0.3105 | MAE: 1.0494 | RMSE: 1.2927 | R²: 0.0475\n",
      "[Epoch 015] Phase: train | Loss: 1.3916 | Spearman: 0.3841 | Pearson: 0.3670 | MAE: 0.9686 | RMSE: 1.1797 | R²: 0.1207\n",
      "[Epoch 015] Phase: val   | Loss: 1.5631 | Spearman: 0.4143 | Pearson: 0.3714 | MAE: 1.0065 | RMSE: 1.2502 | R²: 0.1091\n",
      "[Epoch 020] Phase: train | Loss: 1.3376 | Spearman: 0.4373 | Pearson: 0.4014 | MAE: 0.9425 | RMSE: 1.1565 | R²: 0.1548\n",
      "[Epoch 020] Phase: val   | Loss: 1.5185 | Spearman: 0.4235 | Pearson: 0.3837 | MAE: 0.9751 | RMSE: 1.2323 | R²: 0.1345\n",
      "[Epoch 025] Phase: train | Loss: 1.3028 | Spearman: 0.4521 | Pearson: 0.4252 | MAE: 0.9275 | RMSE: 1.1414 | R²: 0.1768\n",
      "[Epoch 025] Phase: val   | Loss: 1.4878 | Spearman: 0.4512 | Pearson: 0.4020 | MAE: 0.9628 | RMSE: 1.2197 | R²: 0.1520\n",
      "[Epoch 030] Phase: train | Loss: 1.3083 | Spearman: 0.4513 | Pearson: 0.4185 | MAE: 0.9257 | RMSE: 1.1438 | R²: 0.1733\n",
      "[Epoch 030] Phase: val   | Loss: 1.4609 | Spearman: 0.4737 | Pearson: 0.4203 | MAE: 0.9596 | RMSE: 1.2087 | R²: 0.1673\n",
      "[Epoch 035] Phase: train | Loss: 1.2562 | Spearman: 0.4900 | Pearson: 0.4573 | MAE: 0.9082 | RMSE: 1.1208 | R²: 0.2062\n",
      "[Epoch 035] Phase: val   | Loss: 1.4618 | Spearman: 0.4775 | Pearson: 0.4146 | MAE: 0.9481 | RMSE: 1.2091 | R²: 0.1668\n",
      "[Epoch 040] Phase: train | Loss: 1.2381 | Spearman: 0.4912 | Pearson: 0.4692 | MAE: 0.9002 | RMSE: 1.1127 | R²: 0.2177\n",
      "[Epoch 040] Phase: val   | Loss: 1.4381 | Spearman: 0.4880 | Pearson: 0.4302 | MAE: 0.9468 | RMSE: 1.1992 | R²: 0.1803\n",
      "[Epoch 045] Phase: train | Loss: 1.2236 | Spearman: 0.5122 | Pearson: 0.4785 | MAE: 0.8914 | RMSE: 1.1062 | R²: 0.2268\n",
      "[Epoch 045] Phase: val   | Loss: 1.4273 | Spearman: 0.4958 | Pearson: 0.4363 | MAE: 0.9390 | RMSE: 1.1947 | R²: 0.1865\n",
      "[Epoch 050] Phase: train | Loss: 1.2251 | Spearman: 0.5037 | Pearson: 0.4773 | MAE: 0.8872 | RMSE: 1.1068 | R²: 0.2259\n",
      "[Epoch 050] Phase: val   | Loss: 1.4347 | Spearman: 0.4964 | Pearson: 0.4288 | MAE: 0.9349 | RMSE: 1.1978 | R²: 0.1822\n",
      "START TRAINING ALTREF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.6062 | Spearman: 0.0619 | Pearson: 0.0681 | MAE: 1.0442 | RMSE: 1.2674 | R²: -0.0149\n",
      "[Epoch 005] Phase: val   | Loss: 1.7334 | Spearman: 0.1614 | Pearson: 0.1758 | MAE: 1.0479 | RMSE: 1.3166 | R²: 0.0120\n",
      "[Epoch 010] Phase: train | Loss: 1.5537 | Spearman: 0.1445 | Pearson: 0.1455 | MAE: 1.0345 | RMSE: 1.2465 | R²: 0.0182\n",
      "[Epoch 010] Phase: val   | Loss: 1.6866 | Spearman: 0.2054 | Pearson: 0.2344 | MAE: 1.0509 | RMSE: 1.2987 | R²: 0.0387\n",
      "[Epoch 015] Phase: train | Loss: 1.5260 | Spearman: 0.1879 | Pearson: 0.1921 | MAE: 1.0233 | RMSE: 1.2353 | R²: 0.0358\n",
      "[Epoch 015] Phase: val   | Loss: 1.6761 | Spearman: 0.2342 | Pearson: 0.2278 | MAE: 1.0558 | RMSE: 1.2947 | R²: 0.0446\n",
      "[Epoch 020] Phase: train | Loss: 1.4982 | Spearman: 0.2311 | Pearson: 0.2313 | MAE: 1.0147 | RMSE: 1.2240 | R²: 0.0533\n",
      "[Epoch 020] Phase: val   | Loss: 1.6502 | Spearman: 0.2910 | Pearson: 0.2728 | MAE: 1.0429 | RMSE: 1.2846 | R²: 0.0594\n",
      "[Epoch 025] Phase: train | Loss: 1.4559 | Spearman: 0.2738 | Pearson: 0.2841 | MAE: 0.9972 | RMSE: 1.2066 | R²: 0.0800\n",
      "[Epoch 025] Phase: val   | Loss: 1.6476 | Spearman: 0.2884 | Pearson: 0.2697 | MAE: 1.0379 | RMSE: 1.2836 | R²: 0.0609\n",
      "[Epoch 030] Phase: train | Loss: 1.4204 | Spearman: 0.3194 | Pearson: 0.3217 | MAE: 0.9813 | RMSE: 1.1918 | R²: 0.1025\n",
      "[Epoch 030] Phase: val   | Loss: 1.6038 | Spearman: 0.3526 | Pearson: 0.3241 | MAE: 1.0300 | RMSE: 1.2664 | R²: 0.0858\n",
      "[Epoch 035] Phase: train | Loss: 1.3896 | Spearman: 0.3386 | Pearson: 0.3500 | MAE: 0.9696 | RMSE: 1.1788 | R²: 0.1219\n",
      "[Epoch 035] Phase: val   | Loss: 1.5898 | Spearman: 0.3675 | Pearson: 0.3358 | MAE: 1.0146 | RMSE: 1.2609 | R²: 0.0938\n",
      "[Epoch 040] Phase: train | Loss: 1.3715 | Spearman: 0.3611 | Pearson: 0.3654 | MAE: 0.9569 | RMSE: 1.1711 | R²: 0.1334\n",
      "[Epoch 040] Phase: val   | Loss: 1.6125 | Spearman: 0.3809 | Pearson: 0.3156 | MAE: 1.0060 | RMSE: 1.2698 | R²: 0.0809\n",
      "[Epoch 045] Phase: train | Loss: 1.3489 | Spearman: 0.3727 | Pearson: 0.3845 | MAE: 0.9524 | RMSE: 1.1614 | R²: 0.1477\n",
      "[Epoch 045] Phase: val   | Loss: 1.5683 | Spearman: 0.4038 | Pearson: 0.3468 | MAE: 0.9972 | RMSE: 1.2523 | R²: 0.1061\n",
      "[Epoch 050] Phase: train | Loss: 1.3069 | Spearman: 0.4175 | Pearson: 0.4182 | MAE: 0.9355 | RMSE: 1.1432 | R²: 0.1742\n",
      "[Epoch 050] Phase: val   | Loss: 1.5734 | Spearman: 0.3902 | Pearson: 0.3311 | MAE: 0.9902 | RMSE: 1.2544 | R²: 0.1032\n",
      "START TRAINING ALTREF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.8551 | Spearman: 0.0531 | Pearson: 0.0514 | MAE: 1.0862 | RMSE: 1.3620 | R²: -0.1722\n",
      "[Epoch 005] Phase: val   | Loss: 1.8192 | Spearman: 0.0923 | Pearson: 0.0822 | MAE: 1.0349 | RMSE: 1.3488 | R²: -0.0369\n",
      "[Epoch 010] Phase: train | Loss: 1.6894 | Spearman: 0.0853 | Pearson: 0.0956 | MAE: 1.0515 | RMSE: 1.2998 | R²: -0.0675\n",
      "[Epoch 010] Phase: val   | Loss: 1.7529 | Spearman: 0.1388 | Pearson: 0.1239 | MAE: 1.0372 | RMSE: 1.3240 | R²: 0.0009\n",
      "[Epoch 015] Phase: train | Loss: 1.6078 | Spearman: 0.1147 | Pearson: 0.1305 | MAE: 1.0402 | RMSE: 1.2680 | R²: -0.0159\n",
      "[Epoch 015] Phase: val   | Loss: 1.6930 | Spearman: 0.2577 | Pearson: 0.2349 | MAE: 1.0467 | RMSE: 1.3012 | R²: 0.0350\n",
      "[Epoch 020] Phase: train | Loss: 1.5822 | Spearman: 0.1212 | Pearson: 0.1368 | MAE: 1.0337 | RMSE: 1.2578 | R²: 0.0003\n",
      "[Epoch 020] Phase: val   | Loss: 1.6864 | Spearman: 0.2609 | Pearson: 0.2415 | MAE: 1.0497 | RMSE: 1.2986 | R²: 0.0388\n",
      "[Epoch 025] Phase: train | Loss: 1.5693 | Spearman: 0.1145 | Pearson: 0.1480 | MAE: 1.0365 | RMSE: 1.2527 | R²: 0.0084\n",
      "[Epoch 025] Phase: val   | Loss: 1.6971 | Spearman: 0.2320 | Pearson: 0.2170 | MAE: 1.0425 | RMSE: 1.3027 | R²: 0.0327\n",
      "[Epoch 030] Phase: train | Loss: 1.5243 | Spearman: 0.1746 | Pearson: 0.2035 | MAE: 1.0171 | RMSE: 1.2346 | R²: 0.0368\n",
      "[Epoch 030] Phase: val   | Loss: 1.7006 | Spearman: 0.2105 | Pearson: 0.1885 | MAE: 1.0501 | RMSE: 1.3041 | R²: 0.0307\n",
      "[Epoch 035] Phase: train | Loss: 1.5101 | Spearman: 0.1900 | Pearson: 0.2227 | MAE: 1.0107 | RMSE: 1.2289 | R²: 0.0458\n",
      "[Epoch 035] Phase: val   | Loss: 1.6886 | Spearman: 0.2312 | Pearson: 0.2086 | MAE: 1.0492 | RMSE: 1.2995 | R²: 0.0375\n",
      "Early stopping at epoch 38\n",
      "START TRAINING ALTREF SUM\n",
      "[Epoch 005] Phase: train | Loss: 1.9918 | Spearman: -0.0033 | Pearson: -0.0232 | MAE: 1.0872 | RMSE: 1.4113 | R²: -0.2586\n",
      "[Epoch 005] Phase: val   | Loss: 1.9615 | Spearman: -0.0993 | Pearson: -0.1558 | MAE: 1.0710 | RMSE: 1.4005 | R²: -0.1180\n",
      "[Epoch 010] Phase: train | Loss: 1.8090 | Spearman: 0.0308 | Pearson: 0.0103 | MAE: 1.0673 | RMSE: 1.3450 | R²: -0.1431\n",
      "[Epoch 010] Phase: val   | Loss: 1.9450 | Spearman: -0.0863 | Pearson: -0.1444 | MAE: 1.0591 | RMSE: 1.3946 | R²: -0.1086\n",
      "[Epoch 015] Phase: train | Loss: 1.8088 | Spearman: 0.0099 | Pearson: -0.0048 | MAE: 1.0686 | RMSE: 1.3449 | R²: -0.1430\n",
      "[Epoch 015] Phase: val   | Loss: 1.9254 | Spearman: -0.0829 | Pearson: -0.1400 | MAE: 1.0547 | RMSE: 1.3876 | R²: -0.0974\n",
      "[Epoch 020] Phase: train | Loss: 1.7627 | Spearman: 0.0228 | Pearson: -0.0092 | MAE: 1.0621 | RMSE: 1.3277 | R²: -0.1138\n",
      "[Epoch 020] Phase: val   | Loss: 1.8966 | Spearman: -0.0834 | Pearson: -0.1404 | MAE: 1.0557 | RMSE: 1.3772 | R²: -0.0810\n",
      "[Epoch 025] Phase: train | Loss: 1.7353 | Spearman: 0.0035 | Pearson: -0.0254 | MAE: 1.0579 | RMSE: 1.3173 | R²: -0.0965\n",
      "[Epoch 025] Phase: val   | Loss: 1.8634 | Spearman: -0.0757 | Pearson: -0.1320 | MAE: 1.0714 | RMSE: 1.3651 | R²: -0.0621\n",
      "[Epoch 030] Phase: train | Loss: 1.6921 | Spearman: 0.0148 | Pearson: -0.0192 | MAE: 1.0512 | RMSE: 1.3008 | R²: -0.0692\n",
      "[Epoch 030] Phase: val   | Loss: 1.8342 | Spearman: -0.0766 | Pearson: -0.1341 | MAE: 1.0569 | RMSE: 1.3543 | R²: -0.0455\n",
      "[Epoch 035] Phase: train | Loss: 1.6634 | Spearman: 0.0126 | Pearson: -0.0232 | MAE: 1.0501 | RMSE: 1.2897 | R²: -0.0511\n",
      "[Epoch 035] Phase: val   | Loss: 1.8073 | Spearman: -0.0762 | Pearson: -0.1349 | MAE: 1.0617 | RMSE: 1.3444 | R²: -0.0302\n",
      "[Epoch 040] Phase: train | Loss: 1.6243 | Spearman: 0.0310 | Pearson: -0.0005 | MAE: 1.0475 | RMSE: 1.2745 | R²: -0.0263\n",
      "[Epoch 040] Phase: val   | Loss: 1.7842 | Spearman: -0.0672 | Pearson: -0.1180 | MAE: 1.0662 | RMSE: 1.3357 | R²: -0.0170\n",
      "[Epoch 045] Phase: train | Loss: 1.6022 | Spearman: 0.0425 | Pearson: 0.0166 | MAE: 1.0468 | RMSE: 1.2658 | R²: -0.0124\n",
      "[Epoch 045] Phase: val   | Loss: 1.7680 | Spearman: -0.0498 | Pearson: -0.0988 | MAE: 1.0711 | RMSE: 1.3297 | R²: -0.0077\n",
      "[Epoch 050] Phase: train | Loss: 1.5826 | Spearman: 0.0991 | Pearson: 0.0577 | MAE: 1.0446 | RMSE: 1.2580 | R²: -0.0000\n",
      "[Epoch 050] Phase: val   | Loss: 1.7569 | Spearman: 0.0356 | Pearson: -0.0081 | MAE: 1.0776 | RMSE: 1.3255 | R²: -0.0014\n"
     ]
    }
   ],
   "source": [
    "### Alt - Ref embeds\n",
    "\n",
    "print(\"START TRAINING ALTREF MEAN\")\n",
    "altref_mean_metrics_df, altref_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "altref_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset/altref_mean_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF MAX\")\n",
    "altref_max_metrics_df, altref_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "altref_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset/altref_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF TOKEN\")\n",
    "altref_token_metrics_df, altref_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "altref_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset/altref_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF SUM\")\n",
    "altref_sum_metrics_df, altref_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "altref_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset/altref_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8676dd98-3ad8-4ca9-897e-653d4dd95efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.452732</td>\n",
       "      <td>1.337618</td>\n",
       "      <td>1.377145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.450857</td>\n",
       "      <td>0.474312</td>\n",
       "      <td>0.479753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.447302</td>\n",
       "      <td>0.445301</td>\n",
       "      <td>0.401554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.982753</td>\n",
       "      <td>0.888658</td>\n",
       "      <td>0.916866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.205294</td>\n",
       "      <td>1.156554</td>\n",
       "      <td>1.173518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.171968</td>\n",
       "      <td>0.177735</td>\n",
       "      <td>0.153436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.452732  1.337618  1.377145\n",
       "spearman_corr  0.450857  0.474312  0.479753\n",
       "pearson_corr   0.447302  0.445301  0.401554\n",
       "mae            0.982753  0.888658  0.916866\n",
       "rmse           1.205294  1.156554  1.173518\n",
       "r2             0.171968  0.177735  0.153436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = pd.DataFrame({ 'ref':ref_mean_test_metrics,\n",
    "                'alt': alt_mean_test_metrics, \n",
    "                'altref': altref_mean_test_metrics})\n",
    "\n",
    "mean_metrics.to_csv('../res/metrics/test_deepset/mean_metrics.csv')\n",
    "mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2894507d-b0c9-48fa-bf6a-137d9e77b464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.509301</td>\n",
       "      <td>1.535648</td>\n",
       "      <td>1.672286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.448156</td>\n",
       "      <td>0.418089</td>\n",
       "      <td>0.191863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.458341</td>\n",
       "      <td>0.429344</td>\n",
       "      <td>0.139085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.981189</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>1.007356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.228536</td>\n",
       "      <td>1.239213</td>\n",
       "      <td>1.293169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.139725</td>\n",
       "      <td>0.124707</td>\n",
       "      <td>-0.027994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.509301  1.535648  1.672286\n",
       "spearman_corr  0.448156  0.418089  0.191863\n",
       "pearson_corr   0.458341  0.429344  0.139085\n",
       "mae            0.981189  0.996700  1.007356\n",
       "rmse           1.228536  1.239213  1.293169\n",
       "r2             0.139725  0.124707 -0.027994"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_metrics = pd.DataFrame({ 'ref':ref_max_test_metrics,\n",
    "                'alt': alt_max_test_metrics, \n",
    "                'altref': altref_max_test_metrics})\n",
    "\n",
    "max_metrics.to_csv('../res/metrics/test_deepset/max_metrics.csv')\n",
    "max_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "489e15e2-f48f-490d-8975-64ee98d4a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.699620</td>\n",
       "      <td>1.726859</td>\n",
       "      <td>1.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.198805</td>\n",
       "      <td>0.192727</td>\n",
       "      <td>0.213693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.181129</td>\n",
       "      <td>0.133779</td>\n",
       "      <td>0.171494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.067796</td>\n",
       "      <td>1.056331</td>\n",
       "      <td>1.048893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.303695</td>\n",
       "      <td>1.314100</td>\n",
       "      <td>1.307325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.031246</td>\n",
       "      <td>0.015721</td>\n",
       "      <td>0.025843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.699620  1.726859  1.709100\n",
       "spearman_corr  0.198805  0.192727  0.213693\n",
       "pearson_corr   0.181129  0.133779  0.171494\n",
       "mae            1.067796  1.056331  1.048893\n",
       "rmse           1.303695  1.314100  1.307325\n",
       "r2             0.031246  0.015721  0.025843"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_metrics = pd.DataFrame({'ref':ref_token_test_metrics,\n",
    "                'alt': alt_token_test_metrics, \n",
    "                'altref': altref_token_test_metrics})\n",
    "\n",
    "token_metrics.to_csv('../res/metrics/test_deepset/token_metrics.csv')\n",
    "token_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "983f9dfd-18e8-422d-9e23-f4708a103184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>2.766022</td>\n",
       "      <td>2.096702</td>\n",
       "      <td>1.687239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.171579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.005422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.124623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.258030</td>\n",
       "      <td>1.096062</td>\n",
       "      <td>1.044315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.663136</td>\n",
       "      <td>1.447999</td>\n",
       "      <td>1.298938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-0.700339</td>\n",
       "      <td>-0.288892</td>\n",
       "      <td>-0.037185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           2.766022  2.096702  1.687239\n",
       "spearman_corr  0.171579       NaN -0.005422\n",
       "pearson_corr   0.124623       NaN -0.053877\n",
       "mae            1.258030  1.096062  1.044315\n",
       "rmse           1.663136  1.447999  1.298938\n",
       "r2            -0.700339 -0.288892 -0.037185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_metrics = pd.DataFrame({'ref':ref_sum_test_metrics,\n",
    "                'alt': alt_sum_test_metrics, \n",
    "                'altref': altref_sum_test_metrics})\n",
    "\n",
    "sum_metrics.to_csv('../res/metrics/test_deepset/sum_metrics.csv')\n",
    "sum_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
