{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dad261-6c23-4283-a62c-2283cee824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedd779f-484d-4fd4-9fb1-8fe67c853ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50fb1aab-1665-4c5a-ba17-6cc7e966f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>build</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Transcript_ID</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>PTC_pos_codon</th>\n",
       "      <th>PTC_to_start_codon</th>\n",
       "      <th>...</th>\n",
       "      <th>var_token_idx</th>\n",
       "      <th>NMD_efficiency</th>\n",
       "      <th>t_vaf</th>\n",
       "      <th>t_depth</th>\n",
       "      <th>n_vaf</th>\n",
       "      <th>n_depth</th>\n",
       "      <th>VAF_RNA</th>\n",
       "      <th>depth_RNA</th>\n",
       "      <th>VAF_DNA_RNA_ratio</th>\n",
       "      <th>tpm_unstranded_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>944753</td>\n",
       "      <td>944753</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.2191C&gt;T</td>\n",
       "      <td>p.Gln731Ter</td>\n",
       "      <td>731</td>\n",
       "      <td>2193</td>\n",
       "      <td>...</td>\n",
       "      <td>2240</td>\n",
       "      <td>-0.508648</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1.422717</td>\n",
       "      <td>76.8018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>952113</td>\n",
       "      <td>952113</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.1218G&gt;A</td>\n",
       "      <td>p.Trp406Ter</td>\n",
       "      <td>406</td>\n",
       "      <td>1218</td>\n",
       "      <td>...</td>\n",
       "      <td>1267</td>\n",
       "      <td>1.629509</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.323198</td>\n",
       "      <td>49.1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1255304</td>\n",
       "      <td>1255304</td>\n",
       "      <td>UBE2J2</td>\n",
       "      <td>ENST00000349431</td>\n",
       "      <td>c.679G&gt;T</td>\n",
       "      <td>p.Gly227Ter</td>\n",
       "      <td>227</td>\n",
       "      <td>681</td>\n",
       "      <td>...</td>\n",
       "      <td>898</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.464435</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>47.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1338573</td>\n",
       "      <td>1338573</td>\n",
       "      <td>DVL1</td>\n",
       "      <td>ENST00000378888</td>\n",
       "      <td>c.1288G&gt;T</td>\n",
       "      <td>p.Glu430Ter</td>\n",
       "      <td>430</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>1572</td>\n",
       "      <td>1.505167</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.145511</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.352289</td>\n",
       "      <td>50.0535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1387314</td>\n",
       "      <td>1387314</td>\n",
       "      <td>CCNL2</td>\n",
       "      <td>ENST00000400809</td>\n",
       "      <td>c.1480C&gt;T</td>\n",
       "      <td>p.Arg494Ter</td>\n",
       "      <td>494</td>\n",
       "      <td>1482</td>\n",
       "      <td>...</td>\n",
       "      <td>1485</td>\n",
       "      <td>-0.424007</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.546980</td>\n",
       "      <td>298.0</td>\n",
       "      <td>1.341649</td>\n",
       "      <td>27.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>152920717</td>\n",
       "      <td>152920717</td>\n",
       "      <td>ZNF185</td>\n",
       "      <td>ENST00000370268</td>\n",
       "      <td>c.622C&gt;T</td>\n",
       "      <td>p.Gln208Ter</td>\n",
       "      <td>208</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>658</td>\n",
       "      <td>1.785660</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.290043</td>\n",
       "      <td>52.3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>153650171</td>\n",
       "      <td>153650171</td>\n",
       "      <td>DUSP9</td>\n",
       "      <td>ENST00000342782</td>\n",
       "      <td>c.1021G&gt;T</td>\n",
       "      <td>p.Glu341Ter</td>\n",
       "      <td>341</td>\n",
       "      <td>1023</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>-1.360998</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.568627</td>\n",
       "      <td>16.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154030948</td>\n",
       "      <td>154030948</td>\n",
       "      <td>MECP2</td>\n",
       "      <td>ENST00000303391</td>\n",
       "      <td>c.880C&gt;T</td>\n",
       "      <td>p.Arg294Ter</td>\n",
       "      <td>294</td>\n",
       "      <td>882</td>\n",
       "      <td>...</td>\n",
       "      <td>1129</td>\n",
       "      <td>-1.166585</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.244797</td>\n",
       "      <td>11.5544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154354015</td>\n",
       "      <td>154354015</td>\n",
       "      <td>FLNA</td>\n",
       "      <td>ENST00000369850</td>\n",
       "      <td>c.5586C&gt;A</td>\n",
       "      <td>p.Tyr1862Ter</td>\n",
       "      <td>1862</td>\n",
       "      <td>5586</td>\n",
       "      <td>...</td>\n",
       "      <td>5834</td>\n",
       "      <td>0.332048</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.794408</td>\n",
       "      <td>63.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154402790</td>\n",
       "      <td>154402790</td>\n",
       "      <td>DNASE1L1</td>\n",
       "      <td>ENST00000014935</td>\n",
       "      <td>c.826C&gt;T</td>\n",
       "      <td>p.Gln276Ter</td>\n",
       "      <td>276</td>\n",
       "      <td>828</td>\n",
       "      <td>...</td>\n",
       "      <td>1619</td>\n",
       "      <td>3.569856</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>3.9940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4093 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       build chromosome      start        end Hugo_Symbol    Transcript_ID  \\\n",
       "0     GRCh38       chr1     944753     944753       NOC2L  ENST00000327044   \n",
       "1     GRCh38       chr1     952113     952113       NOC2L  ENST00000327044   \n",
       "2     GRCh38       chr1    1255304    1255304      UBE2J2  ENST00000349431   \n",
       "3     GRCh38       chr1    1338573    1338573        DVL1  ENST00000378888   \n",
       "4     GRCh38       chr1    1387314    1387314       CCNL2  ENST00000400809   \n",
       "...      ...        ...        ...        ...         ...              ...   \n",
       "4088  GRCh38       chrX  152920717  152920717      ZNF185  ENST00000370268   \n",
       "4089  GRCh38       chrX  153650171  153650171       DUSP9  ENST00000342782   \n",
       "4090  GRCh38       chrX  154030948  154030948       MECP2  ENST00000303391   \n",
       "4091  GRCh38       chrX  154354015  154354015        FLNA  ENST00000369850   \n",
       "4092  GRCh38       chrX  154402790  154402790    DNASE1L1  ENST00000014935   \n",
       "\n",
       "          HGVSc         HGVSp  PTC_pos_codon  PTC_to_start_codon  ...  \\\n",
       "0     c.2191C>T   p.Gln731Ter            731                2193  ...   \n",
       "1     c.1218G>A   p.Trp406Ter            406                1218  ...   \n",
       "2      c.679G>T   p.Gly227Ter            227                 681  ...   \n",
       "3     c.1288G>T   p.Glu430Ter            430                1290  ...   \n",
       "4     c.1480C>T   p.Arg494Ter            494                1482  ...   \n",
       "...         ...           ...            ...                 ...  ...   \n",
       "4088   c.622C>T   p.Gln208Ter            208                 624  ...   \n",
       "4089  c.1021G>T   p.Glu341Ter            341                1023  ...   \n",
       "4090   c.880C>T   p.Arg294Ter            294                 882  ...   \n",
       "4091  c.5586C>A  p.Tyr1862Ter           1862                5586  ...   \n",
       "4092   c.826C>T   p.Gln276Ter            276                 828  ...   \n",
       "\n",
       "      var_token_idx  NMD_efficiency     t_vaf  t_depth  n_vaf  n_depth  \\\n",
       "0              2240       -0.508648  0.311111     45.0    0.0     43.0   \n",
       "1              1267        1.629509  0.390244     41.0    0.0     14.0   \n",
       "2               898        0.047557  0.480000     50.0    0.0     32.0   \n",
       "3              1572        1.505167  0.413043     46.0    0.0     63.0   \n",
       "4              1485       -0.424007  0.407692    130.0    0.0    152.0   \n",
       "...             ...             ...       ...      ...    ...      ...   \n",
       "4088            658        1.785660  0.328358     67.0    0.0     65.0   \n",
       "4089           1285       -1.360998  0.389313    131.0    0.0    185.0   \n",
       "4090           1129       -1.166585  0.406977     86.0    0.0     70.0   \n",
       "4091           5834        0.332048  0.904762    189.0    0.0    134.0   \n",
       "4092           1619        3.569856  0.339286     56.0    0.0     66.0   \n",
       "\n",
       "       VAF_RNA  depth_RNA VAF_DNA_RNA_ratio tpm_unstranded_x  \n",
       "0     0.442623      244.0          1.422717          76.8018  \n",
       "1     0.126126      111.0          0.323198          49.1731  \n",
       "2     0.464435      239.0          0.967573          47.4110  \n",
       "3     0.145511      323.0          0.352289          50.0535  \n",
       "4     0.546980      298.0          1.341649          27.1883  \n",
       "...        ...        ...               ...              ...  \n",
       "4088  0.095238       42.0          0.290043          52.3474  \n",
       "4089  1.000000       35.0          2.568627          16.9024  \n",
       "4090  0.913580       81.0          2.244797          11.5544  \n",
       "4091  0.718750       32.0          0.794408          63.2541  \n",
       "4092  0.028571      105.0          0.084211           3.9940  \n",
       "\n",
       "[4093 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tcga_processed.tsv.gz', sep='\\t')\n",
    "\n",
    "### some variants are repeated multiple times\n",
    "df = df.drop(columns=['Cancer_type', 'Cancer_type_count', 'NMF_cluster'])\n",
    "\n",
    "agg_columns = [\n",
    "    'NMD_efficiency', 't_vaf', 't_depth', 'n_vaf', 'n_depth',\n",
    "    'VAF_RNA', 'depth_RNA', 'VAF_DNA_RNA_ratio', 'tpm_unstranded_x'\n",
    "]\n",
    "\n",
    "# Group by all other columns except the ones to aggregate\n",
    "group_by_columns = [col for col in df.columns if col not in agg_columns]\n",
    "\n",
    "# Perform grouping and aggregation without dropping NaNs\n",
    "df_unq = df.groupby(\n",
    "    group_by_columns, dropna=False, as_index=False\n",
    ").agg({col: 'mean' for col in agg_columns})\n",
    "\n",
    "df_unq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1405b1-a9ba-4e2d-bd60-55522987acb8",
   "metadata": {},
   "source": [
    "### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745038bb-f2f7-4cf9-aa43-369cb7f02ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000397527:c.3463C>T',\n",
       "  'ENST00000375687:c.2077C>T',\n",
       "  'ENST00000314328:c.2410C>T',\n",
       "  'ENST00000449058:c.1366C>T',\n",
       "  'ENST00000217244:c.916C>T'],\n",
       " 232)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var_ids = df_unq[df_unq['chromosome'].isin(['chr20', 'chr21', 'chr22'])]['var_id'].values.tolist()\n",
    "test_var_ids = list(set(test_var_ids))\n",
    "test_var_ids[0:5], len(test_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8deaa2-95af-46d3-9bbf-15d364abaee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000344099:c.1099C>T',\n",
       "  'ENST00000420124:c.2992C>T',\n",
       "  'ENST00000313434:c.1195C>T',\n",
       "  'ENST00000300853:c.130C>T',\n",
       "  'ENST00000317683:c.62C>A'],\n",
       " 210)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_var_ids = df_unq[df_unq['chromosome']=='chr19']['var_id'].values.tolist()\n",
    "val_var_ids = list(set(val_var_ids))\n",
    "val_var_ids[0:5], len(val_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5493303-3a84-4500-8310-d1ebb64ec9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000360663:c.1228C>T',\n",
       "  'ENST00000275235:c.355A>T',\n",
       "  'ENST00000249776:c.169G>T',\n",
       "  'ENST00000458591:c.1600G>T',\n",
       "  'ENST00000521381:c.1740C>A'],\n",
       " 3651)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_var_ids = list(set(df_unq.var_id.values.tolist()) - set(val_var_ids) - set(test_var_ids))\n",
    "train_var_ids[0:5], len(train_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8cfdf6-c661-461f-8975-f1db5737629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries with NMD_efficiency as values\n",
    "test_dict = {}\n",
    "val_dict = {}\n",
    "train_dict = {}\n",
    "var_pos_idx_dict = {}\n",
    "\n",
    "for index, row in df_unq.iterrows():\n",
    "    if row['var_id'] in test_var_ids:\n",
    "        test_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    elif row['var_id'] in val_var_ids:\n",
    "        val_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    else:\n",
    "        train_dict[row['var_id']] = row['NMD_efficiency']\n",
    "\n",
    "    var_pos_idx_dict[row['var_id']] = row['var_token_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df337a-1513-4344-b2dd-ecaa6f9f6aa3",
   "metadata": {},
   "source": [
    "### loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7281b2da-4d4e-41e6-b55f-da2649f2df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def aggregate_tensors(embed_dict, value_dict, aggregation, var_pos_idx_dict=None):\n",
    "    \"\"\"\n",
    "    Aggregates tensors based on the specified aggregation method.\n",
    "    \n",
    "    Args:\n",
    "        embed_dict (dict): Dictionary where keys are IDs and values are 2D tensors.\n",
    "        value_dict (dict): Dictionary where keys are IDs and values are single numbers.\n",
    "        aggregation (str): Aggregation method ('mean', 'max', 'sum', 'product', 'token').\n",
    "        var_pos_idx_dict (dict, optional): Dictionary where keys are IDs and values are token indices for aggregation.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with aggregated values.\n",
    "    \"\"\"\n",
    "    if aggregation not in ['mean', 'max', 'sum', 'product', 'token']:\n",
    "        raise ValueError(\"Invalid aggregation method. Choose from 'mean', 'max', 'sum', 'product', 'token'.\")\n",
    "    \n",
    "    if aggregation == 'token' and var_pos_idx_dict is None:\n",
    "        raise ValueError(\"'var_pos_idx_dict' must be provided when aggregation is 'token'.\")\n",
    "    \n",
    "    aggregated_dict = {}\n",
    "    for key, tensor in embed_dict.items():\n",
    "        if key not in value_dict:\n",
    "            continue  # Ensure matching keys\n",
    "        \n",
    "        if aggregation == 'mean':\n",
    "            aggregated_dict[key] = torch.mean(tensor, dim=0)\n",
    "        elif aggregation == 'max':\n",
    "            aggregated_dict[key], _ = torch.max(tensor, dim=0)\n",
    "        elif aggregation == 'sum':\n",
    "            aggregated_dict[key] = torch.sum(tensor, dim=0)\n",
    "        elif aggregation == 'product':\n",
    "            aggregated_dict[key] = torch.prod(tensor, dim=0)\n",
    "        elif aggregation == 'token':\n",
    "            if key not in var_pos_idx_dict:\n",
    "                raise KeyError(f\"'var_pos_idx_dict' must contain a token index for key '{key}'\")\n",
    "            \n",
    "            var_pos_idx = var_pos_idx_dict[key]\n",
    "            if var_pos_idx >= tensor.shape[0]:\n",
    "                raise IndexError(f\"Index {var_pos_idx} out of range for tensor with shape {tensor.shape}\")\n",
    "            aggregated_dict[key] = tensor[var_pos_idx]\n",
    "    \n",
    "    return aggregated_dict\n",
    "\n",
    "\n",
    "class AggregatedDataset(Dataset):\n",
    "    def __init__(self, aggregated_tensors, value_dict):\n",
    "        self.data = []\n",
    "        for key, tensor in aggregated_tensors.items():\n",
    "            if key in value_dict:\n",
    "                self.data.append((tensor, value_dict[key]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def create_data_loader(embed_dict, value_dict, aggregation, var_pos_idx_dict=None, shuffle=False, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for the aggregated dataset.\n",
    "    \n",
    "    Args:\n",
    "        embed_dict (dict): Dictionary of embedding tensors.\n",
    "        value_dict (dict): Dictionary of values associated with embeddings.\n",
    "        aggregation (str): Aggregation method.\n",
    "        var_pos_idx_dict (dict, optional): Token indices for 'token' aggregation.\n",
    "        shuffle (bool): Whether to shuffle the dataset.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader instance.\n",
    "    \"\"\"\n",
    "    if aggregation == 'token':\n",
    "        agg_tensor = aggregate_tensors(embed_dict, value_dict, aggregation, var_pos_idx_dict=var_pos_idx_dict)\n",
    "    else:\n",
    "        agg_tensor = aggregate_tensors(embed_dict, value_dict, aggregation)\n",
    "    \n",
    "    agg_dataset = AggregatedDataset(agg_tensor, value_dict)\n",
    "    dataloader = DataLoader(agg_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e44ba-5620-4433-9603-50bd773c5e5e",
   "metadata": {},
   "source": [
    "### ref (mean, max, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a861fd5a-81bd-4bbb-b91c-af2b8969e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tcga_ref_embeds.pkl', 'rb') as file:\n",
    "    ref_embeds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536caec7-74c1-46e5-baca-d5850cc245d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean aggregation\n",
    "train_mean_loader_ref = create_data_loader(ref_embeds, train_dict, 'mean', shuffle=True)\n",
    "val_mean_loader_ref = create_data_loader(ref_embeds, val_dict, 'mean', shuffle=False)\n",
    "test_mean_loader_ref = create_data_loader(ref_embeds, test_dict, 'mean', shuffle=False)\n",
    "\n",
    "# Max aggregation\n",
    "train_max_loader_ref = create_data_loader(ref_embeds, train_dict, 'max', shuffle=True)\n",
    "val_max_loader_ref = create_data_loader(ref_embeds, val_dict, 'max', shuffle=False)\n",
    "test_max_loader_ref = create_data_loader(ref_embeds, test_dict, 'max', shuffle=False)\n",
    "\n",
    "# Token aggregation\n",
    "train_token_loader_ref = create_data_loader(ref_embeds, train_dict, 'token', shuffle=True, var_pos_idx_dict=var_pos_idx_dict)\n",
    "val_token_loader_ref = create_data_loader(ref_embeds, val_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "test_token_loader_ref = create_data_loader(ref_embeds, test_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "\n",
    "# Sum aggregation\n",
    "train_sum_loader_ref = create_data_loader(ref_embeds, train_dict, 'sum', shuffle=True)\n",
    "val_sum_loader_ref = create_data_loader(ref_embeds, val_dict, 'sum', shuffle=False)\n",
    "test_sum_loader_ref = create_data_loader(ref_embeds, test_dict, 'sum', shuffle=False)\n",
    "\n",
    "# Product aggregation\n",
    "train_product_loader_ref = create_data_loader(ref_embeds, train_dict, 'product', shuffle=True)\n",
    "val_product_loader_ref = create_data_loader(ref_embeds, val_dict, 'product', shuffle=False)\n",
    "test_product_loader_ref = create_data_loader(ref_embeds, test_dict, 'product', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e8a2c-61e4-4254-8e85-971a540efe97",
   "metadata": {},
   "source": [
    "### alt (mean, max, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391346a1-e735-4751-b283-e6582742792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tcga_alt_embeds.pkl', 'rb') as file:\n",
    "    alt_embeds = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7c1aaa-4804-4800-ba73-21fc45e91a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean aggregation\n",
    "train_mean_loader_alt = create_data_loader(alt_embeds, train_dict, 'mean', shuffle=True)\n",
    "val_mean_loader_alt = create_data_loader(alt_embeds, val_dict, 'mean', shuffle=False)\n",
    "test_mean_loader_alt = create_data_loader(alt_embeds, test_dict, 'mean', shuffle=False)\n",
    "\n",
    "# Max aggregation\n",
    "train_max_loader_alt = create_data_loader(alt_embeds, train_dict, 'max', shuffle=True)\n",
    "val_max_loader_alt = create_data_loader(alt_embeds, val_dict, 'max', shuffle=False)\n",
    "test_max_loader_alt = create_data_loader(alt_embeds, test_dict, 'max', shuffle=False)\n",
    "\n",
    "# Token aggregation\n",
    "train_token_loader_alt = create_data_loader(alt_embeds, train_dict, 'token', shuffle=True, var_pos_idx_dict=var_pos_idx_dict)\n",
    "val_token_loader_alt = create_data_loader(alt_embeds, val_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "test_token_loader_alt = create_data_loader(alt_embeds, test_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "\n",
    "# Sum aggregation\n",
    "train_sum_loader_alt = create_data_loader(alt_embeds, train_dict, 'sum', shuffle=True)\n",
    "val_sum_loader_alt = create_data_loader(alt_embeds, val_dict, 'sum', shuffle=False)\n",
    "test_sum_loader_alt = create_data_loader(alt_embeds, test_dict, 'sum', shuffle=False)\n",
    "\n",
    "# Product aggregation\n",
    "train_product_loader_alt = create_data_loader(alt_embeds, train_dict, 'product', shuffle=True)\n",
    "val_product_loader_alt = create_data_loader(alt_embeds, val_dict, 'product', shuffle=False)\n",
    "test_product_loader_alt = create_data_loader(alt_embeds, test_dict, 'product', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf4150-05f9-4c00-bcb3-8159b3143fbe",
   "metadata": {},
   "source": [
    "### alt - ref (mean, max, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a108ff10-8e67-4036-bdde-b1faea5675be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create alt - ref embeds\n",
    "altref_embeds = {}\n",
    "\n",
    "for var_id, alt_embed in alt_embeds.items():\n",
    "    altref_embeds[var_id] = alt_embed - ref_embeds[var_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39625f7c-524f-4924-a0ae-72a2a21b1e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean aggregation\n",
    "train_mean_loader_altref = create_data_loader(altref_embeds, train_dict, 'mean', shuffle=True)\n",
    "val_mean_loader_altref = create_data_loader(altref_embeds, val_dict, 'mean', shuffle=False)\n",
    "test_mean_loader_altref = create_data_loader(altref_embeds, test_dict, 'mean', shuffle=False)\n",
    "\n",
    "# Max aggregation\n",
    "train_max_loader_altref = create_data_loader(altref_embeds, train_dict, 'max', shuffle=True)\n",
    "val_max_loader_altref = create_data_loader(altref_embeds, val_dict, 'max', shuffle=False)\n",
    "test_max_loader_altref = create_data_loader(altref_embeds, test_dict, 'max', shuffle=False)\n",
    "\n",
    "# Token aggregation\n",
    "train_token_loader_altref = create_data_loader(altref_embeds, train_dict, 'token', shuffle=True, var_pos_idx_dict=var_pos_idx_dict)\n",
    "val_token_loader_altref = create_data_loader(altref_embeds, val_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "test_token_loader_altref = create_data_loader(altref_embeds, test_dict, 'token', shuffle=False, var_pos_idx_dict=var_pos_idx_dict)\n",
    "\n",
    "# Sum aggregation\n",
    "train_sum_loader_altref = create_data_loader(altref_embeds, train_dict, 'sum', shuffle=True)\n",
    "val_sum_loader_altref = create_data_loader(altref_embeds, val_dict, 'sum', shuffle=False)\n",
    "test_sum_loader_altref = create_data_loader(altref_embeds, test_dict, 'sum', shuffle=False)\n",
    "\n",
    "# Product aggregation\n",
    "train_product_loader_altref = create_data_loader(altref_embeds, train_dict, 'product', shuffle=True)\n",
    "val_product_loader_altref = create_data_loader(altref_embeds, val_dict, 'product', shuffle=False)\n",
    "test_product_loader_altref = create_data_loader(altref_embeds, test_dict, 'product', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad809648-1dae-4d77-adba-03cf51142181",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "150af4e8-f04f-43b6-a7f5-80f9e65d1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "HIDDEN_DIMS = [8, 8]\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "TRANSFORMATION = 'none'  # Options: 'z-score', 'min-max', 'log', 'none'\n",
    "\n",
    "# Evaluation Metrics\n",
    "def evaluate_regression_metrics(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy().flatten()\n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    loss = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(loss)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        spearman_corr = np.nan\n",
    "\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        pearson_corr = np.nan\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'pearson_corr': pearson_corr,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "# MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim, dtype=torch.float32))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1, dtype=torch.float32))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Transformation Functions\n",
    "def apply_transformation(y, transformation, stats):\n",
    "    if transformation == 'log':\n",
    "        return torch.log1p(torch.clamp(y - stats['min'] + 1, min=1e-8))\n",
    "    elif transformation == 'z-score':\n",
    "        return (y - stats['mean']) / (stats['std'] + 1e-8)\n",
    "    elif transformation == 'min-max':\n",
    "        return (y - stats['min']) / (stats['max'] - stats['min'] + 1e-8)\n",
    "    return y\n",
    "\n",
    "def inverse_transformation(y, stats, transformation):\n",
    "    if transformation == 'log':\n",
    "        return torch.expm1(y) + stats['min'] - 1\n",
    "    elif transformation == 'z-score':\n",
    "        return y * stats['std'] + stats['mean']\n",
    "    elif transformation == 'min-max':\n",
    "        return y * (stats['max'] - stats['min']) + stats['min']\n",
    "    return y\n",
    "\n",
    "# Calculate Transformation Statistics\n",
    "def calculate_transformation_stats(dataset, transformation):\n",
    "    if transformation == 'none':\n",
    "        return {}\n",
    "    \n",
    "    y = torch.cat([targets for _, targets in dataset], dim=0)\n",
    "    stats = {}\n",
    "    if transformation == 'log':\n",
    "        stats['min'] = y.min()\n",
    "    elif transformation == 'z-score':\n",
    "        stats['mean'] = y.mean()\n",
    "        stats['std'] = y.std()\n",
    "    elif transformation == 'min-max':\n",
    "        stats['min'] = y.min()\n",
    "        stats['max'] = y.max()\n",
    "    return stats\n",
    "\n",
    "# Training Function\n",
    "def train_mlp(train_loader: DataLoader, val_loader: DataLoader, test_loader: DataLoader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_dim = next(iter(train_loader))[0].shape[1]\n",
    "    model = MLP(input_dim, HIDDEN_DIMS, DROPOUT).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    # Calculate global transformation stats\n",
    "    transformation_stats = calculate_transformation_stats(train_loader, TRANSFORMATION)\n",
    "    \n",
    "    metrics = {'epoch': [], 'phase': [], 'loss': [], 'spearman_corr': [], 'pearson_corr':[], 'mae': [], 'rmse': [], 'r2': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "            \n",
    "            all_preds, all_targets = [], []\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                for inputs, targets in loader:\n",
    "                    inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device, dtype=torch.float32)\n",
    "                    original_targets = targets.clone()\n",
    "                    \n",
    "                    if TRANSFORMATION != 'none':\n",
    "                        targets = apply_transformation(targets, TRANSFORMATION, transformation_stats)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    all_preds.append(outputs.detach())\n",
    "                    all_targets.append(original_targets.detach())\n",
    "            \n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            \n",
    "            if TRANSFORMATION != 'none':\n",
    "                all_preds = inverse_transformation(all_preds, transformation_stats, TRANSFORMATION)\n",
    "            \n",
    "            epoch_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "            epoch_metrics['loss'] = epoch_loss\n",
    "            \n",
    "            # Log metrics\n",
    "            metrics['epoch'].append(epoch)\n",
    "            metrics['phase'].append(phase)\n",
    "            for key, value in epoch_metrics.items():\n",
    "                metrics[key].append(value)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    patience_counter = 0\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        print(f\"Early stopping at epoch {epoch} with best validation loss: {best_val_loss:.4f}\")\n",
    "                        model.load_state_dict(best_model_state)\n",
    "                        metrics_df = pd.DataFrame(metrics)\n",
    "                        return metrics_df, evaluate_regression_metrics(all_targets, all_preds)\n",
    "            \n",
    "            # Print metrics every 5 epochs for both phases\n",
    "            if epoch % 5 == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}] Phase: {phase:5s} | \"\n",
    "                    f\"Loss: {epoch_metrics['loss']:.4f} | \"\n",
    "                    f\"Spearman: {epoch_metrics['spearman_corr']:.4f} | \"\n",
    "                    f\"Pearson: {epoch_metrics['pearson_corr']:.4f} | \"\n",
    "                    f\"MAE: {epoch_metrics['mae']:.4f} | \"\n",
    "                    f\"RMSE: {epoch_metrics['rmse']:.4f} | \"\n",
    "                    f\"R²: {epoch_metrics['r2']:.4f}\"\n",
    "                )\n",
    "    \n",
    "    # Load the best model before final testing\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final Test Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device, dtype=torch.float32), targets.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            all_preds.append(outputs.detach())\n",
    "            all_targets.append(targets.detach())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    if TRANSFORMATION != 'none':\n",
    "        all_preds = inverse_transformation(all_preds, transformation_stats, TRANSFORMATION)\n",
    "    \n",
    "    test_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "    print(\n",
    "        \"\\n📊 Final Test Metrics | \"\n",
    "        f\"Loss: {test_metrics['loss']:.4f} | \"\n",
    "        f\"Spearman: {test_metrics['spearman_corr']:.4f} | \"\n",
    "        f\"Pearson: {epoch_metrics['pearson_corr']:.4f} | \"\n",
    "        f\"MAE: {test_metrics['mae']:.4f} | \"\n",
    "        f\"RMSE: {test_metrics['rmse']:.4f} | \"\n",
    "        f\"R²: {test_metrics['r2']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a7c0f24-25ac-4a23-a2e1-145345e3501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING REF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.6557 | Spearman: 0.0864 | Pearson: 0.0670 | MAE: 1.0480 | RMSE: 1.2867 | R²: -0.0462\n",
      "[Epoch 005] Phase: val   | Loss: 1.6853 | Spearman: 0.4560 | Pearson: 0.4350 | MAE: 1.0419 | RMSE: 1.2982 | R²: 0.0394\n",
      "[Epoch 010] Phase: train | Loss: 1.5316 | Spearman: 0.1867 | Pearson: 0.1972 | MAE: 1.0138 | RMSE: 1.2376 | R²: 0.0322\n",
      "[Epoch 010] Phase: val   | Loss: 1.5830 | Spearman: 0.4541 | Pearson: 0.4374 | MAE: 1.0123 | RMSE: 1.2582 | R²: 0.0977\n",
      "[Epoch 015] Phase: train | Loss: 1.4852 | Spearman: 0.2429 | Pearson: 0.2517 | MAE: 1.0012 | RMSE: 1.2187 | R²: 0.0615\n",
      "[Epoch 015] Phase: val   | Loss: 1.5307 | Spearman: 0.4449 | Pearson: 0.4310 | MAE: 1.0032 | RMSE: 1.2372 | R²: 0.1275\n",
      "[Epoch 020] Phase: train | Loss: 1.4801 | Spearman: 0.2718 | Pearson: 0.2579 | MAE: 1.0008 | RMSE: 1.2166 | R²: 0.0647\n",
      "[Epoch 020] Phase: val   | Loss: 1.5198 | Spearman: 0.4614 | Pearson: 0.4409 | MAE: 0.9995 | RMSE: 1.2328 | R²: 0.1338\n",
      "[Epoch 025] Phase: train | Loss: 1.4537 | Spearman: 0.3112 | Pearson: 0.2865 | MAE: 0.9884 | RMSE: 1.2057 | R²: 0.0815\n",
      "[Epoch 025] Phase: val   | Loss: 1.4942 | Spearman: 0.4631 | Pearson: 0.4422 | MAE: 0.9819 | RMSE: 1.2224 | R²: 0.1484\n",
      "[Epoch 030] Phase: train | Loss: 1.4301 | Spearman: 0.3044 | Pearson: 0.3114 | MAE: 0.9768 | RMSE: 1.1959 | R²: 0.0963\n",
      "[Epoch 030] Phase: val   | Loss: 1.4496 | Spearman: 0.4705 | Pearson: 0.4616 | MAE: 0.9743 | RMSE: 1.2040 | R²: 0.1737\n",
      "[Epoch 035] Phase: train | Loss: 1.4139 | Spearman: 0.3269 | Pearson: 0.3271 | MAE: 0.9725 | RMSE: 1.1891 | R²: 0.1066\n",
      "[Epoch 035] Phase: val   | Loss: 1.4290 | Spearman: 0.4810 | Pearson: 0.4688 | MAE: 0.9514 | RMSE: 1.1954 | R²: 0.1855\n",
      "[Epoch 040] Phase: train | Loss: 1.4146 | Spearman: 0.3146 | Pearson: 0.3268 | MAE: 0.9709 | RMSE: 1.1894 | R²: 0.1062\n",
      "[Epoch 040] Phase: val   | Loss: 1.4120 | Spearman: 0.4875 | Pearson: 0.4803 | MAE: 0.9476 | RMSE: 1.1883 | R²: 0.1952\n",
      "[Epoch 045] Phase: train | Loss: 1.3817 | Spearman: 0.3451 | Pearson: 0.3565 | MAE: 0.9581 | RMSE: 1.1754 | R²: 0.1270\n",
      "[Epoch 045] Phase: val   | Loss: 1.4063 | Spearman: 0.4800 | Pearson: 0.4839 | MAE: 0.9476 | RMSE: 1.1859 | R²: 0.1985\n",
      "[Epoch 050] Phase: train | Loss: 1.3895 | Spearman: 0.3356 | Pearson: 0.3495 | MAE: 0.9584 | RMSE: 1.1788 | R²: 0.1220\n",
      "[Epoch 050] Phase: val   | Loss: 1.3761 | Spearman: 0.4933 | Pearson: 0.4867 | MAE: 0.9278 | RMSE: 1.1731 | R²: 0.2157\n",
      "\n",
      "📊 Final Test Metrics | Loss: 1.3653 | Spearman: 0.4641 | Pearson: 0.4867 | MAE: 0.8968 | RMSE: 1.1685 | R²: 0.1607\n",
      "START TRAINING REF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.6847 | Spearman: 0.0642 | Pearson: 0.0593 | MAE: 1.0564 | RMSE: 1.2980 | R²: -0.0645\n",
      "[Epoch 005] Phase: val   | Loss: 1.7687 | Spearman: 0.3027 | Pearson: 0.2878 | MAE: 1.0371 | RMSE: 1.3299 | R²: -0.0081\n",
      "[Epoch 010] Phase: train | Loss: 1.6024 | Spearman: 0.0669 | Pearson: 0.0711 | MAE: 1.0421 | RMSE: 1.2659 | R²: -0.0126\n",
      "[Epoch 010] Phase: val   | Loss: 1.7883 | Spearman: 0.2551 | Pearson: 0.2198 | MAE: 1.0490 | RMSE: 1.3373 | R²: -0.0193\n",
      "[Epoch 015] Phase: train | Loss: 1.6094 | Spearman: -0.0150 | Pearson: -0.0105 | MAE: 1.0548 | RMSE: 1.2686 | R²: -0.0169\n",
      "[Epoch 015] Phase: val   | Loss: 1.7561 | Spearman: nan | Pearson: nan | MAE: 1.0837 | RMSE: 1.3252 | R²: -0.0009\n",
      "[Epoch 020] Phase: train | Loss: 1.6039 | Spearman: -0.0078 | Pearson: -0.0092 | MAE: 1.0577 | RMSE: 1.2665 | R²: -0.0135\n",
      "[Epoch 020] Phase: val   | Loss: 1.7584 | Spearman: nan | Pearson: nan | MAE: 1.0881 | RMSE: 1.3260 | R²: -0.0022\n",
      "Early stopping at epoch 21 with best validation loss: 1.6933\n",
      "START TRAINING REF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.6712 | Spearman: 0.0883 | Pearson: 0.0943 | MAE: 1.0505 | RMSE: 1.2927 | R²: -0.0560\n",
      "[Epoch 005] Phase: val   | Loss: 1.7471 | Spearman: 0.1566 | Pearson: 0.1161 | MAE: 1.0505 | RMSE: 1.3218 | R²: 0.0042\n",
      "[Epoch 010] Phase: train | Loss: 1.5818 | Spearman: 0.1310 | Pearson: 0.1602 | MAE: 1.0312 | RMSE: 1.2577 | R²: 0.0005\n",
      "[Epoch 010] Phase: val   | Loss: 1.7286 | Spearman: 0.1761 | Pearson: 0.1469 | MAE: 1.0459 | RMSE: 1.3147 | R²: 0.0148\n",
      "[Epoch 015] Phase: train | Loss: 1.5275 | Spearman: 0.1956 | Pearson: 0.2106 | MAE: 1.0143 | RMSE: 1.2359 | R²: 0.0348\n",
      "[Epoch 015] Phase: val   | Loss: 1.6888 | Spearman: 0.2219 | Pearson: 0.1979 | MAE: 1.0477 | RMSE: 1.2995 | R²: 0.0374\n",
      "[Epoch 020] Phase: train | Loss: 1.5055 | Spearman: 0.2297 | Pearson: 0.2303 | MAE: 1.0071 | RMSE: 1.2270 | R²: 0.0487\n",
      "[Epoch 020] Phase: val   | Loss: 1.6882 | Spearman: 0.2141 | Pearson: 0.1980 | MAE: 1.0481 | RMSE: 1.2993 | R²: 0.0378\n",
      "[Epoch 025] Phase: train | Loss: 1.4495 | Spearman: 0.2825 | Pearson: 0.2915 | MAE: 0.9903 | RMSE: 1.2039 | R²: 0.0841\n",
      "[Epoch 025] Phase: val   | Loss: 1.6804 | Spearman: 0.2330 | Pearson: 0.2079 | MAE: 1.0550 | RMSE: 1.2963 | R²: 0.0422\n",
      "[Epoch 030] Phase: train | Loss: 1.4411 | Spearman: 0.2846 | Pearson: 0.2996 | MAE: 0.9899 | RMSE: 1.2005 | R²: 0.0894\n",
      "[Epoch 030] Phase: val   | Loss: 1.6813 | Spearman: 0.2375 | Pearson: 0.2048 | MAE: 1.0514 | RMSE: 1.2967 | R²: 0.0417\n",
      "[Epoch 035] Phase: train | Loss: 1.4159 | Spearman: 0.3006 | Pearson: 0.3255 | MAE: 0.9821 | RMSE: 1.1899 | R²: 0.1053\n",
      "[Epoch 035] Phase: val   | Loss: 1.6752 | Spearman: 0.2448 | Pearson: 0.2181 | MAE: 1.0541 | RMSE: 1.2943 | R²: 0.0452\n",
      "[Epoch 040] Phase: train | Loss: 1.4029 | Spearman: 0.3359 | Pearson: 0.3374 | MAE: 0.9741 | RMSE: 1.1845 | R²: 0.1135\n",
      "[Epoch 040] Phase: val   | Loss: 1.6810 | Spearman: 0.2201 | Pearson: 0.2059 | MAE: 1.0397 | RMSE: 1.2965 | R²: 0.0418\n",
      "[Epoch 045] Phase: train | Loss: 1.3584 | Spearman: 0.3573 | Pearson: 0.3764 | MAE: 0.9592 | RMSE: 1.1655 | R²: 0.1416\n",
      "[Epoch 045] Phase: val   | Loss: 1.6839 | Spearman: 0.2324 | Pearson: 0.2065 | MAE: 1.0423 | RMSE: 1.2977 | R²: 0.0402\n",
      "Early stopping at epoch 46 with best validation loss: 1.6619\n",
      "START TRAINING REF SUM\n",
      "[Epoch 005] Phase: train | Loss: 2.2461 | Spearman: 0.0064 | Pearson: 0.0079 | MAE: 1.1316 | RMSE: 1.4987 | R²: -0.4193\n",
      "[Epoch 005] Phase: val   | Loss: 2.2044 | Spearman: 0.3015 | Pearson: 0.2688 | MAE: 1.0677 | RMSE: 1.4847 | R²: -0.2565\n",
      "[Epoch 010] Phase: train | Loss: 2.0891 | Spearman: 0.0127 | Pearson: 0.0237 | MAE: 1.1056 | RMSE: 1.4454 | R²: -0.3200\n",
      "[Epoch 010] Phase: val   | Loss: 2.0993 | Spearman: 0.3111 | Pearson: 0.2504 | MAE: 1.0477 | RMSE: 1.4489 | R²: -0.1966\n",
      "[Epoch 015] Phase: train | Loss: 1.9660 | Spearman: -0.0152 | Pearson: -0.0098 | MAE: 1.0833 | RMSE: 1.4021 | R²: -0.2423\n",
      "[Epoch 015] Phase: val   | Loss: 1.9894 | Spearman: 0.2893 | Pearson: 0.2393 | MAE: 1.0311 | RMSE: 1.4105 | R²: -0.1339\n",
      "[Epoch 020] Phase: train | Loss: 1.8524 | Spearman: 0.0067 | Pearson: 0.0126 | MAE: 1.0662 | RMSE: 1.3610 | R²: -0.1705\n",
      "[Epoch 020] Phase: val   | Loss: 1.8815 | Spearman: 0.2893 | Pearson: 0.2422 | MAE: 1.0198 | RMSE: 1.3717 | R²: -0.0724\n",
      "[Epoch 025] Phase: train | Loss: 1.7523 | Spearman: 0.0090 | Pearson: 0.0115 | MAE: 1.0532 | RMSE: 1.3237 | R²: -0.1072\n",
      "[Epoch 025] Phase: val   | Loss: 1.7916 | Spearman: 0.2892 | Pearson: 0.2443 | MAE: 1.0146 | RMSE: 1.3385 | R²: -0.0212\n",
      "[Epoch 030] Phase: train | Loss: 1.6683 | Spearman: -0.0163 | Pearson: 0.0147 | MAE: 1.0445 | RMSE: 1.2916 | R²: -0.0542\n",
      "[Epoch 030] Phase: val   | Loss: 1.7290 | Spearman: 0.2893 | Pearson: 0.2447 | MAE: 1.0167 | RMSE: 1.3149 | R²: 0.0145\n",
      "[Epoch 035] Phase: train | Loss: 1.6246 | Spearman: 0.0183 | Pearson: 0.0220 | MAE: 1.0433 | RMSE: 1.2746 | R²: -0.0265\n",
      "[Epoch 035] Phase: val   | Loss: 1.6920 | Spearman: 0.2893 | Pearson: 0.2459 | MAE: 1.0230 | RMSE: 1.3008 | R²: 0.0356\n",
      "[Epoch 040] Phase: train | Loss: 1.6043 | Spearman: 0.0186 | Pearson: 0.0223 | MAE: 1.0461 | RMSE: 1.2666 | R²: -0.0137\n",
      "[Epoch 040] Phase: val   | Loss: 1.6739 | Spearman: 0.2893 | Pearson: 0.2458 | MAE: 1.0292 | RMSE: 1.2938 | R²: 0.0459\n",
      "[Epoch 045] Phase: train | Loss: 1.5986 | Spearman: -0.0238 | Pearson: 0.0084 | MAE: 1.0507 | RMSE: 1.2644 | R²: -0.0101\n",
      "[Epoch 045] Phase: val   | Loss: 1.6994 | Spearman: 0.2519 | Pearson: 0.2063 | MAE: 1.0532 | RMSE: 1.3036 | R²: 0.0314\n",
      "[Epoch 050] Phase: train | Loss: 1.5941 | Spearman: -0.0296 | Pearson: 0.0121 | MAE: 1.0532 | RMSE: 1.2626 | R²: -0.0073\n",
      "Early stopping at epoch 50 with best validation loss: 1.6739\n"
     ]
    }
   ],
   "source": [
    "### Ref embeds\n",
    "\n",
    "# ref mean\n",
    "print(\"START TRAINING REF MEAN\")\n",
    "ref_mean_metrics_df, ref_mean_test_metrics = train_mlp(train_mean_loader_ref, val_mean_loader_ref, test_mean_loader_ref)\n",
    "ref_mean_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/ref_mean_history_df.csv', index=False)\n",
    "\n",
    "# ref max\n",
    "print(\"START TRAINING REF MAX\")\n",
    "ref_max_metrics_df, ref_max_test_metrics = train_mlp(train_max_loader_ref, val_max_loader_ref, test_max_loader_ref)\n",
    "ref_max_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/ref_max_metrics_df.csv', index=False)\n",
    "\n",
    "# ref token\n",
    "print(\"START TRAINING REF TOKEN\")\n",
    "ref_token_metrics_df, ref_token_test_metrics = train_mlp(train_token_loader_ref, val_token_loader_ref, test_token_loader_ref)\n",
    "ref_token_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/ref_token_metrics_df.csv', index=False)\n",
    "\n",
    "# ref sum\n",
    "print(\"START TRAINING REF SUM\")\n",
    "ref_sum_metrics_df, ref_sum_test_metrics = train_mlp(train_sum_loader_ref, val_sum_loader_ref, test_sum_loader_ref)\n",
    "ref_sum_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/ref_sum_metrics_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "952e0e1a-98c5-45d7-87ae-a699fc21226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALT MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5815 | Spearman: 0.1402 | Pearson: 0.1415 | MAE: 1.0288 | RMSE: 1.2576 | R²: 0.0007\n",
      "[Epoch 005] Phase: val   | Loss: 1.6117 | Spearman: 0.4397 | Pearson: 0.4295 | MAE: 1.0143 | RMSE: 1.2695 | R²: 0.0814\n",
      "[Epoch 010] Phase: train | Loss: 1.4400 | Spearman: 0.3103 | Pearson: 0.3040 | MAE: 0.9832 | RMSE: 1.2000 | R²: 0.0901\n",
      "[Epoch 010] Phase: val   | Loss: 1.4863 | Spearman: 0.4491 | Pearson: 0.4363 | MAE: 0.9732 | RMSE: 1.2191 | R²: 0.1528\n",
      "[Epoch 015] Phase: train | Loss: 1.4027 | Spearman: 0.3668 | Pearson: 0.3387 | MAE: 0.9635 | RMSE: 1.1843 | R²: 0.1137\n",
      "[Epoch 015] Phase: val   | Loss: 1.4399 | Spearman: 0.4638 | Pearson: 0.4518 | MAE: 0.9674 | RMSE: 1.2000 | R²: 0.1793\n",
      "[Epoch 020] Phase: train | Loss: 1.3761 | Spearman: 0.3699 | Pearson: 0.3619 | MAE: 0.9573 | RMSE: 1.1731 | R²: 0.1305\n",
      "[Epoch 020] Phase: val   | Loss: 1.4277 | Spearman: 0.4674 | Pearson: 0.4497 | MAE: 0.9573 | RMSE: 1.1949 | R²: 0.1862\n",
      "[Epoch 025] Phase: train | Loss: 1.3705 | Spearman: 0.3922 | Pearson: 0.3668 | MAE: 0.9529 | RMSE: 1.1707 | R²: 0.1340\n",
      "[Epoch 025] Phase: val   | Loss: 1.4189 | Spearman: 0.4876 | Pearson: 0.4618 | MAE: 0.9471 | RMSE: 1.1912 | R²: 0.1913\n",
      "[Epoch 030] Phase: train | Loss: 1.3682 | Spearman: 0.3793 | Pearson: 0.3696 | MAE: 0.9520 | RMSE: 1.1697 | R²: 0.1355\n",
      "[Epoch 030] Phase: val   | Loss: 1.4067 | Spearman: 0.4777 | Pearson: 0.4635 | MAE: 0.9476 | RMSE: 1.1860 | R²: 0.1982\n",
      "[Epoch 035] Phase: train | Loss: 1.3646 | Spearman: 0.3840 | Pearson: 0.3720 | MAE: 0.9476 | RMSE: 1.1681 | R²: 0.1378\n",
      "[Epoch 035] Phase: val   | Loss: 1.3946 | Spearman: 0.4897 | Pearson: 0.4766 | MAE: 0.9567 | RMSE: 1.1809 | R²: 0.2051\n",
      "[Epoch 040] Phase: train | Loss: 1.3482 | Spearman: 0.3886 | Pearson: 0.3850 | MAE: 0.9444 | RMSE: 1.1611 | R²: 0.1481\n",
      "[Epoch 040] Phase: val   | Loss: 1.4284 | Spearman: 0.4846 | Pearson: 0.4636 | MAE: 0.9399 | RMSE: 1.1952 | R²: 0.1858\n",
      "[Epoch 045] Phase: train | Loss: 1.3376 | Spearman: 0.3966 | Pearson: 0.3935 | MAE: 0.9406 | RMSE: 1.1566 | R²: 0.1548\n",
      "[Epoch 045] Phase: val   | Loss: 1.3980 | Spearman: 0.4885 | Pearson: 0.4682 | MAE: 0.9485 | RMSE: 1.1824 | R²: 0.2032\n",
      "[Epoch 050] Phase: train | Loss: 1.3369 | Spearman: 0.4066 | Pearson: 0.3945 | MAE: 0.9387 | RMSE: 1.1563 | R²: 0.1552\n",
      "[Epoch 050] Phase: val   | Loss: 1.3946 | Spearman: 0.4839 | Pearson: 0.4674 | MAE: 0.9449 | RMSE: 1.1809 | R²: 0.2051\n",
      "\n",
      "📊 Final Test Metrics | Loss: 1.3778 | Spearman: 0.4504 | Pearson: 0.4674 | MAE: 0.9027 | RMSE: 1.1738 | R²: 0.1530\n",
      "START TRAINING ALT MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.8336 | Spearman: 0.0265 | Pearson: 0.0151 | MAE: 1.0781 | RMSE: 1.3541 | R²: -0.1586\n",
      "[Epoch 005] Phase: val   | Loss: 1.8118 | Spearman: 0.2859 | Pearson: 0.2703 | MAE: 1.0297 | RMSE: 1.3460 | R²: -0.0327\n",
      "[Epoch 010] Phase: train | Loss: 1.6607 | Spearman: 0.0694 | Pearson: 0.0707 | MAE: 1.0505 | RMSE: 1.2887 | R²: -0.0494\n",
      "[Epoch 010] Phase: val   | Loss: 1.7865 | Spearman: 0.3533 | Pearson: 0.3310 | MAE: 1.0315 | RMSE: 1.3366 | R²: -0.0183\n",
      "[Epoch 015] Phase: train | Loss: 1.6344 | Spearman: 0.0164 | Pearson: 0.0137 | MAE: 1.0506 | RMSE: 1.2784 | R²: -0.0327\n",
      "[Epoch 015] Phase: val   | Loss: 1.7544 | Spearman: nan | Pearson: nan | MAE: 1.0760 | RMSE: 1.3246 | R²: -0.0000\n",
      "[Epoch 020] Phase: train | Loss: 1.6372 | Spearman: -0.0265 | Pearson: -0.0248 | MAE: 1.0630 | RMSE: 1.2795 | R²: -0.0345\n",
      "[Epoch 020] Phase: val   | Loss: 1.7562 | Spearman: nan | Pearson: nan | MAE: 1.0840 | RMSE: 1.3252 | R²: -0.0010\n",
      "Early stopping at epoch 22 with best validation loss: 1.7000\n",
      "START TRAINING ALT TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.6226 | Spearman: 0.0891 | Pearson: 0.0958 | MAE: 1.0400 | RMSE: 1.2738 | R²: -0.0253\n",
      "[Epoch 005] Phase: val   | Loss: 1.7385 | Spearman: 0.2084 | Pearson: 0.1293 | MAE: 1.0462 | RMSE: 1.3185 | R²: 0.0091\n",
      "[Epoch 010] Phase: train | Loss: 1.5410 | Spearman: 0.1834 | Pearson: 0.1857 | MAE: 1.0192 | RMSE: 1.2414 | R²: 0.0263\n",
      "[Epoch 010] Phase: val   | Loss: 1.6877 | Spearman: 0.2628 | Pearson: 0.2019 | MAE: 1.0486 | RMSE: 1.2991 | R²: 0.0380\n",
      "[Epoch 015] Phase: train | Loss: 1.5114 | Spearman: 0.2274 | Pearson: 0.2226 | MAE: 1.0087 | RMSE: 1.2294 | R²: 0.0450\n",
      "[Epoch 015] Phase: val   | Loss: 1.6595 | Spearman: 0.3165 | Pearson: 0.2438 | MAE: 1.0411 | RMSE: 1.2882 | R²: 0.0541\n",
      "[Epoch 020] Phase: train | Loss: 1.4552 | Spearman: 0.2764 | Pearson: 0.2844 | MAE: 0.9917 | RMSE: 1.2063 | R²: 0.0805\n",
      "[Epoch 020] Phase: val   | Loss: 1.6415 | Spearman: 0.3114 | Pearson: 0.2614 | MAE: 1.0304 | RMSE: 1.2812 | R²: 0.0644\n",
      "[Epoch 025] Phase: train | Loss: 1.4371 | Spearman: 0.2943 | Pearson: 0.3037 | MAE: 0.9868 | RMSE: 1.1988 | R²: 0.0919\n",
      "[Epoch 025] Phase: val   | Loss: 1.6680 | Spearman: 0.2757 | Pearson: 0.2245 | MAE: 1.0449 | RMSE: 1.2915 | R²: 0.0493\n",
      "[Epoch 030] Phase: train | Loss: 1.4037 | Spearman: 0.3291 | Pearson: 0.3371 | MAE: 0.9715 | RMSE: 1.1848 | R²: 0.1130\n",
      "[Epoch 030] Phase: val   | Loss: 1.6370 | Spearman: 0.3216 | Pearson: 0.2625 | MAE: 1.0336 | RMSE: 1.2795 | R²: 0.0669\n",
      "[Epoch 035] Phase: train | Loss: 1.3830 | Spearman: 0.3446 | Pearson: 0.3554 | MAE: 0.9658 | RMSE: 1.1760 | R²: 0.1261\n",
      "[Epoch 035] Phase: val   | Loss: 1.6237 | Spearman: 0.3298 | Pearson: 0.2802 | MAE: 1.0410 | RMSE: 1.2743 | R²: 0.0745\n",
      "[Epoch 040] Phase: train | Loss: 1.3670 | Spearman: 0.3561 | Pearson: 0.3693 | MAE: 0.9554 | RMSE: 1.1692 | R²: 0.1362\n",
      "[Epoch 040] Phase: val   | Loss: 1.6296 | Spearman: 0.3428 | Pearson: 0.2704 | MAE: 1.0420 | RMSE: 1.2766 | R²: 0.0712\n",
      "[Epoch 045] Phase: train | Loss: 1.3391 | Spearman: 0.3877 | Pearson: 0.3924 | MAE: 0.9390 | RMSE: 1.1572 | R²: 0.1538\n",
      "[Epoch 045] Phase: val   | Loss: 1.6461 | Spearman: 0.3212 | Pearson: 0.2512 | MAE: 1.0434 | RMSE: 1.2830 | R²: 0.0617\n",
      "[Epoch 050] Phase: train | Loss: 1.3192 | Spearman: 0.3987 | Pearson: 0.4081 | MAE: 0.9376 | RMSE: 1.1485 | R²: 0.1665\n",
      "[Epoch 050] Phase: val   | Loss: 1.6729 | Spearman: 0.2938 | Pearson: 0.2235 | MAE: 1.0535 | RMSE: 1.2934 | R²: 0.0465\n",
      "\n",
      "📊 Final Test Metrics | Loss: 1.6092 | Spearman: 0.2676 | Pearson: 0.2235 | MAE: 0.9877 | RMSE: 1.2685 | R²: 0.0108\n",
      "START TRAINING ALT SUM\n",
      "[Epoch 005] Phase: train | Loss: 2.2890 | Spearman: -0.0204 | Pearson: -0.0149 | MAE: 1.1420 | RMSE: 1.5129 | R²: -0.4464\n",
      "[Epoch 005] Phase: val   | Loss: 2.2585 | Spearman: nan | Pearson: nan | MAE: 1.0868 | RMSE: 1.5028 | R²: -0.2873\n",
      "[Epoch 010] Phase: train | Loss: 2.0292 | Spearman: 0.0155 | Pearson: 0.0021 | MAE: 1.0954 | RMSE: 1.4245 | R²: -0.2822\n",
      "[Epoch 010] Phase: val   | Loss: 2.0654 | Spearman: nan | Pearson: nan | MAE: 1.0564 | RMSE: 1.4371 | R²: -0.1772\n",
      "[Epoch 015] Phase: train | Loss: 1.8594 | Spearman: -0.0044 | Pearson: -0.0262 | MAE: 1.0671 | RMSE: 1.3636 | R²: -0.1749\n",
      "[Epoch 015] Phase: val   | Loss: 1.9211 | Spearman: nan | Pearson: nan | MAE: 1.0436 | RMSE: 1.3860 | R²: -0.0950\n",
      "[Epoch 020] Phase: train | Loss: 1.7227 | Spearman: 0.0015 | Pearson: 0.0061 | MAE: 1.0494 | RMSE: 1.3125 | R²: -0.0885\n",
      "[Epoch 020] Phase: val   | Loss: 1.8215 | Spearman: nan | Pearson: nan | MAE: 1.0458 | RMSE: 1.3496 | R²: -0.0382\n",
      "[Epoch 025] Phase: train | Loss: 1.6502 | Spearman: 0.0044 | Pearson: -0.0075 | MAE: 1.0461 | RMSE: 1.2846 | R²: -0.0427\n",
      "[Epoch 025] Phase: val   | Loss: 1.7715 | Spearman: nan | Pearson: nan | MAE: 1.0571 | RMSE: 1.3310 | R²: -0.0097\n",
      "[Epoch 030] Phase: train | Loss: 1.6092 | Spearman: 0.0253 | Pearson: 0.0162 | MAE: 1.0454 | RMSE: 1.2685 | R²: -0.0168\n",
      "[Epoch 030] Phase: val   | Loss: 1.7556 | Spearman: nan | Pearson: nan | MAE: 1.0709 | RMSE: 1.3250 | R²: -0.0006\n",
      "[Epoch 035] Phase: train | Loss: 1.5969 | Spearman: 0.0168 | Pearson: 0.0149 | MAE: 1.0509 | RMSE: 1.2637 | R²: -0.0090\n",
      "[Epoch 035] Phase: val   | Loss: 1.7552 | Spearman: nan | Pearson: nan | MAE: 1.0813 | RMSE: 1.3248 | R²: -0.0004\n",
      "[Epoch 040] Phase: train | Loss: 1.6037 | Spearman: -0.0138 | Pearson: -0.0146 | MAE: 1.0554 | RMSE: 1.2664 | R²: -0.0134\n",
      "[Epoch 040] Phase: val   | Loss: 1.7576 | Spearman: nan | Pearson: nan | MAE: 1.0867 | RMSE: 1.3257 | R²: -0.0018\n",
      "Early stopping at epoch 42 with best validation loss: 1.7545\n"
     ]
    }
   ],
   "source": [
    "### Alt embeds\n",
    "\n",
    "# alt mean\n",
    "print(\"START TRAINING ALT MEAN\")\n",
    "alt_mean_metrics_df, alt_mean_test_metrics = train_mlp(train_mean_loader_alt, val_mean_loader_alt, test_mean_loader_alt)\n",
    "alt_mean_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/alt_mean_metrics_df.csv', index=False)\n",
    "\n",
    "# alt max\n",
    "print(\"START TRAINING ALT MAX\")\n",
    "alt_max_metrics_df, alt_max_test_metrics = train_mlp(train_max_loader_alt, val_max_loader_alt, test_max_loader_alt)\n",
    "alt_max_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/alt_max_metrics_df.csv', index=False)\n",
    "\n",
    "# alt token\n",
    "print(\"START TRAINING ALT TOKEN\")\n",
    "alt_token_metrics_df, alt_token_test_metrics = train_mlp(train_token_loader_alt, val_token_loader_alt, test_token_loader_alt)\n",
    "alt_token_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/alt_token_metrics_df.csv', index=False)\n",
    "\n",
    "# alt sum\n",
    "print(\"START TRAINING ALT SUM\")\n",
    "alt_sum_metrics_df, alt_sum_test_metrics = train_mlp(train_sum_loader_alt, val_sum_loader_alt, test_sum_loader_alt)\n",
    "alt_sum_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/alt_sum_metrics_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ee12b9f-276e-48a8-8bd9-9b57366e842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALTREF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5977 | Spearman: 0.0552 | Pearson: 0.0610 | MAE: 1.0480 | RMSE: 1.2640 | R²: -0.0095\n",
      "[Epoch 005] Phase: val   | Loss: 1.7273 | Spearman: 0.2779 | Pearson: 0.1955 | MAE: 1.0686 | RMSE: 1.3143 | R²: 0.0155\n",
      "[Epoch 010] Phase: train | Loss: 1.5154 | Spearman: 0.2123 | Pearson: 0.2101 | MAE: 1.0186 | RMSE: 1.2310 | R²: 0.0424\n",
      "[Epoch 010] Phase: val   | Loss: 1.6991 | Spearman: 0.3062 | Pearson: 0.2250 | MAE: 1.0602 | RMSE: 1.3035 | R²: 0.0316\n",
      "[Epoch 015] Phase: train | Loss: 1.4869 | Spearman: 0.2601 | Pearson: 0.2471 | MAE: 1.0078 | RMSE: 1.2194 | R²: 0.0604\n",
      "[Epoch 015] Phase: val   | Loss: 1.6652 | Spearman: 0.3279 | Pearson: 0.2489 | MAE: 1.0401 | RMSE: 1.2904 | R²: 0.0509\n",
      "[Epoch 020] Phase: train | Loss: 1.4261 | Spearman: 0.3431 | Pearson: 0.3165 | MAE: 0.9791 | RMSE: 1.1942 | R²: 0.0989\n",
      "[Epoch 020] Phase: val   | Loss: 1.6243 | Spearman: 0.3660 | Pearson: 0.2876 | MAE: 1.0163 | RMSE: 1.2745 | R²: 0.0742\n",
      "[Epoch 025] Phase: train | Loss: 1.3608 | Spearman: 0.4137 | Pearson: 0.3794 | MAE: 0.9529 | RMSE: 1.1665 | R²: 0.1401\n",
      "[Epoch 025] Phase: val   | Loss: 1.5834 | Spearman: 0.4020 | Pearson: 0.3263 | MAE: 0.9960 | RMSE: 1.2583 | R²: 0.0975\n",
      "[Epoch 030] Phase: train | Loss: 1.3261 | Spearman: 0.4456 | Pearson: 0.4071 | MAE: 0.9332 | RMSE: 1.1515 | R²: 0.1621\n",
      "[Epoch 030] Phase: val   | Loss: 1.5461 | Spearman: 0.4245 | Pearson: 0.3562 | MAE: 0.9822 | RMSE: 1.2434 | R²: 0.1188\n",
      "[Epoch 035] Phase: train | Loss: 1.3016 | Spearman: 0.4637 | Pearson: 0.4234 | MAE: 0.9206 | RMSE: 1.1409 | R²: 0.1775\n",
      "[Epoch 035] Phase: val   | Loss: 1.5202 | Spearman: 0.4380 | Pearson: 0.3711 | MAE: 0.9683 | RMSE: 1.2330 | R²: 0.1335\n",
      "[Epoch 040] Phase: train | Loss: 1.2811 | Spearman: 0.4749 | Pearson: 0.4374 | MAE: 0.9076 | RMSE: 1.1318 | R²: 0.1905\n",
      "[Epoch 040] Phase: val   | Loss: 1.5009 | Spearman: 0.4494 | Pearson: 0.3872 | MAE: 0.9653 | RMSE: 1.2251 | R²: 0.1445\n",
      "[Epoch 045] Phase: train | Loss: 1.2465 | Spearman: 0.5012 | Pearson: 0.4619 | MAE: 0.8926 | RMSE: 1.1165 | R²: 0.2123\n",
      "[Epoch 045] Phase: val   | Loss: 1.4861 | Spearman: 0.4603 | Pearson: 0.3976 | MAE: 0.9593 | RMSE: 1.2190 | R²: 0.1530\n",
      "[Epoch 050] Phase: train | Loss: 1.2215 | Spearman: 0.5170 | Pearson: 0.4785 | MAE: 0.8830 | RMSE: 1.1052 | R²: 0.2282\n",
      "[Epoch 050] Phase: val   | Loss: 1.4759 | Spearman: 0.4665 | Pearson: 0.4003 | MAE: 0.9476 | RMSE: 1.2149 | R²: 0.1588\n",
      "\n",
      "📊 Final Test Metrics | Loss: 1.4919 | Spearman: 0.3915 | Pearson: 0.4003 | MAE: 0.9352 | RMSE: 1.2214 | R²: 0.0829\n",
      "START TRAINING ALTREF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.7610 | Spearman: 0.0350 | Pearson: 0.0382 | MAE: 1.0664 | RMSE: 1.3270 | R²: -0.1127\n",
      "[Epoch 005] Phase: val   | Loss: 1.7729 | Spearman: 0.0106 | Pearson: 0.0368 | MAE: 1.0641 | RMSE: 1.3315 | R²: -0.0105\n",
      "[Epoch 010] Phase: train | Loss: 1.6698 | Spearman: 0.0619 | Pearson: 0.0649 | MAE: 1.0473 | RMSE: 1.2922 | R²: -0.0551\n",
      "[Epoch 010] Phase: val   | Loss: 1.7391 | Spearman: 0.0757 | Pearson: 0.0962 | MAE: 1.0688 | RMSE: 1.3188 | R²: 0.0087\n",
      "[Epoch 015] Phase: train | Loss: 1.5916 | Spearman: 0.1208 | Pearson: 0.1219 | MAE: 1.0378 | RMSE: 1.2616 | R²: -0.0057\n",
      "[Epoch 015] Phase: val   | Loss: 1.7234 | Spearman: 0.1561 | Pearson: 0.1716 | MAE: 1.0424 | RMSE: 1.3128 | R²: 0.0177\n",
      "[Epoch 020] Phase: train | Loss: 1.5406 | Spearman: 0.1732 | Pearson: 0.1753 | MAE: 1.0230 | RMSE: 1.2412 | R²: 0.0265\n",
      "[Epoch 020] Phase: val   | Loss: 1.7275 | Spearman: 0.1621 | Pearson: 0.1592 | MAE: 1.0836 | RMSE: 1.3143 | R²: 0.0154\n",
      "[Epoch 025] Phase: train | Loss: 1.5414 | Spearman: 0.1685 | Pearson: 0.1690 | MAE: 1.0248 | RMSE: 1.2415 | R²: 0.0260\n",
      "[Epoch 025] Phase: val   | Loss: 1.7051 | Spearman: 0.2250 | Pearson: 0.1772 | MAE: 1.0581 | RMSE: 1.3058 | R²: 0.0281\n",
      "[Epoch 030] Phase: train | Loss: 1.5359 | Spearman: 0.1534 | Pearson: 0.1741 | MAE: 1.0297 | RMSE: 1.2393 | R²: 0.0295\n",
      "[Epoch 030] Phase: val   | Loss: 1.7241 | Spearman: 0.2351 | Pearson: 0.1973 | MAE: 1.0544 | RMSE: 1.3130 | R²: 0.0173\n",
      "[Epoch 035] Phase: train | Loss: 1.5457 | Spearman: 0.1352 | Pearson: 0.1541 | MAE: 1.0319 | RMSE: 1.2433 | R²: 0.0233\n",
      "[Epoch 035] Phase: val   | Loss: 1.7070 | Spearman: 0.2486 | Pearson: 0.2068 | MAE: 1.0662 | RMSE: 1.3065 | R²: 0.0270\n",
      "Early stopping at epoch 36 with best validation loss: 1.7009\n",
      "START TRAINING ALTREF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.7367 | Spearman: 0.0949 | Pearson: 0.0998 | MAE: 1.0625 | RMSE: 1.3178 | R²: -0.0974\n",
      "[Epoch 005] Phase: val   | Loss: 1.7640 | Spearman: 0.0946 | Pearson: 0.0763 | MAE: 1.0551 | RMSE: 1.3282 | R²: -0.0055\n",
      "[Epoch 010] Phase: train | Loss: 1.6159 | Spearman: 0.1338 | Pearson: 0.1551 | MAE: 1.0326 | RMSE: 1.2712 | R²: -0.0210\n",
      "[Epoch 010] Phase: val   | Loss: 1.7566 | Spearman: 0.1339 | Pearson: 0.1231 | MAE: 1.0387 | RMSE: 1.3254 | R²: -0.0012\n",
      "[Epoch 015] Phase: train | Loss: 1.5488 | Spearman: 0.1916 | Pearson: 0.1988 | MAE: 1.0165 | RMSE: 1.2445 | R²: 0.0214\n",
      "[Epoch 015] Phase: val   | Loss: 1.7264 | Spearman: 0.1674 | Pearson: 0.1488 | MAE: 1.0692 | RMSE: 1.3139 | R²: 0.0160\n",
      "[Epoch 020] Phase: train | Loss: 1.5190 | Spearman: 0.1941 | Pearson: 0.2220 | MAE: 1.0163 | RMSE: 1.2325 | R²: 0.0402\n",
      "[Epoch 020] Phase: val   | Loss: 1.6542 | Spearman: 0.2528 | Pearson: 0.2506 | MAE: 1.0471 | RMSE: 1.2862 | R²: 0.0571\n",
      "[Epoch 025] Phase: train | Loss: 1.4830 | Spearman: 0.2465 | Pearson: 0.2579 | MAE: 1.0000 | RMSE: 1.2178 | R²: 0.0629\n",
      "[Epoch 025] Phase: val   | Loss: 1.6889 | Spearman: 0.2155 | Pearson: 0.1943 | MAE: 1.0518 | RMSE: 1.2996 | R²: 0.0374\n",
      "[Epoch 030] Phase: train | Loss: 1.4584 | Spearman: 0.2666 | Pearson: 0.2821 | MAE: 0.9900 | RMSE: 1.2076 | R²: 0.0785\n",
      "Early stopping at epoch 30 with best validation loss: 1.6542\n",
      "START TRAINING ALTREF SUM\n",
      "[Epoch 005] Phase: train | Loss: 1.5822 | Spearman: 0.2957 | Pearson: 0.2706 | MAE: 0.9897 | RMSE: 1.2579 | R²: 0.0002\n",
      "[Epoch 005] Phase: val   | Loss: 1.6563 | Spearman: 0.3364 | Pearson: 0.2692 | MAE: 0.9826 | RMSE: 1.2870 | R²: 0.0560\n",
      "[Epoch 010] Phase: train | Loss: 1.3215 | Spearman: 0.4325 | Pearson: 0.4173 | MAE: 0.9170 | RMSE: 1.1496 | R²: 0.1650\n",
      "[Epoch 010] Phase: val   | Loss: 1.5863 | Spearman: 0.4045 | Pearson: 0.3222 | MAE: 0.9681 | RMSE: 1.2595 | R²: 0.0958\n",
      "[Epoch 015] Phase: train | Loss: 1.1858 | Spearman: 0.5224 | Pearson: 0.5012 | MAE: 0.8682 | RMSE: 1.0889 | R²: 0.2507\n",
      "[Epoch 015] Phase: val   | Loss: 1.5083 | Spearman: 0.4561 | Pearson: 0.3758 | MAE: 0.9569 | RMSE: 1.2281 | R²: 0.1403\n",
      "[Epoch 020] Phase: train | Loss: 1.1757 | Spearman: 0.5412 | Pearson: 0.5082 | MAE: 0.8629 | RMSE: 1.0843 | R²: 0.2571\n",
      "[Epoch 020] Phase: val   | Loss: 1.5405 | Spearman: 0.4446 | Pearson: 0.3495 | MAE: 0.9556 | RMSE: 1.2412 | R²: 0.1219\n",
      "[Epoch 025] Phase: train | Loss: 1.0634 | Spearman: 0.5888 | Pearson: 0.5729 | MAE: 0.8186 | RMSE: 1.0312 | R²: 0.3280\n",
      "[Epoch 025] Phase: val   | Loss: 1.4960 | Spearman: 0.4455 | Pearson: 0.3847 | MAE: 0.9495 | RMSE: 1.2231 | R²: 0.1473\n",
      "[Epoch 030] Phase: train | Loss: 1.0615 | Spearman: 0.5869 | Pearson: 0.5739 | MAE: 0.8138 | RMSE: 1.0303 | R²: 0.3292\n",
      "[Epoch 030] Phase: val   | Loss: 1.5408 | Spearman: 0.4245 | Pearson: 0.3509 | MAE: 0.9664 | RMSE: 1.2413 | R²: 0.1217\n",
      "[Epoch 035] Phase: train | Loss: 1.0199 | Spearman: 0.6127 | Pearson: 0.5964 | MAE: 0.7945 | RMSE: 1.0099 | R²: 0.3556\n",
      "Early stopping at epoch 35 with best validation loss: 1.4960\n"
     ]
    }
   ],
   "source": [
    "### Alt - Ref embeds\n",
    "\n",
    "# altref mean\n",
    "print(\"START TRAINING ALTREF MEAN\")\n",
    "altref_mean_metrics_df, altref_mean_test_metrics = train_mlp(train_mean_loader_altref, val_mean_loader_altref, test_mean_loader_altref)\n",
    "altref_mean_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/altref_mean_metrics_df.csv', index=False)\n",
    "\n",
    "# altref max\n",
    "print(\"START TRAINING ALTREF MAX\")\n",
    "altref_max_metrics_df, altref_max_test_metrics = train_mlp(train_max_loader_altref, val_max_loader_altref, test_max_loader_altref)\n",
    "altref_max_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/altref_max_metrics_df.csv', index=False)\n",
    "\n",
    "# altref token\n",
    "print(\"START TRAINING ALTREF TOKEN\")\n",
    "altref_token_metrics_df, altref_token_test_metrics = train_mlp(train_token_loader_altref, val_token_loader_altref, test_token_loader_altref)\n",
    "altref_token_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/altref_token_metrics_df.csv', index=False)\n",
    "\n",
    "# altref sum\n",
    "print(\"START TRAINING ALTREF SUM\")\n",
    "altref_sum_metrics_df, altref_sum_test_metrics = train_mlp(train_sum_loader_altref, val_sum_loader_altref, test_sum_loader_altref)\n",
    "altref_sum_metrics_df.to_csv('../res/metrics/per_epoch_agg1st/altref_sum_metrics_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f68a8a07-004c-4a21-b1b7-bc44ac0e1afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.365319</td>\n",
       "      <td>1.377834</td>\n",
       "      <td>1.491862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.464140</td>\n",
       "      <td>0.450427</td>\n",
       "      <td>0.391467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.444606</td>\n",
       "      <td>0.414521</td>\n",
       "      <td>0.320416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.896750</td>\n",
       "      <td>0.902667</td>\n",
       "      <td>0.935173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.168469</td>\n",
       "      <td>1.173812</td>\n",
       "      <td>1.221418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.160706</td>\n",
       "      <td>0.153013</td>\n",
       "      <td>0.082917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.365319  1.377834  1.491862\n",
       "spearman_corr  0.464140  0.450427  0.391467\n",
       "pearson_corr   0.444606  0.414521  0.320416\n",
       "mae            0.896750  0.902667  0.935173\n",
       "rmse           1.168469  1.173812  1.221418\n",
       "r2             0.160706  0.153013  0.082917"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = pd.DataFrame({ 'ref':ref_mean_test_metrics,\n",
    "                'alt': alt_mean_test_metrics, \n",
    "                'altref': altref_mean_test_metrics})\n",
    "\n",
    "mean_metrics.to_csv('../res/metrics/test_agg1st/mean_metrics.csv', index=False)\n",
    "mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cae80ec0-4361-4ebf-9262-206d3defe40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.758156</td>\n",
       "      <td>1.757412</td>\n",
       "      <td>1.708832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.241421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.198627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.087674</td>\n",
       "      <td>1.086360</td>\n",
       "      <td>1.061846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.325955</td>\n",
       "      <td>1.325674</td>\n",
       "      <td>1.307223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-0.002118</td>\n",
       "      <td>-0.001694</td>\n",
       "      <td>0.025995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.758156  1.757412  1.708832\n",
       "spearman_corr       NaN       NaN  0.241421\n",
       "pearson_corr        NaN       NaN  0.198627\n",
       "mae            1.087674  1.086360  1.061846\n",
       "rmse           1.325955  1.325674  1.307223\n",
       "r2            -0.002118 -0.001694  0.025995"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_metrics = pd.DataFrame({ 'ref':ref_max_test_metrics,\n",
    "                'alt': alt_max_test_metrics, \n",
    "                'altref': altref_max_test_metrics})\n",
    "\n",
    "max_metrics.to_csv('../res/metrics/test_agg1st/max_metrics.csv', index=False)\n",
    "max_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87d62ef2-50f6-4208-a6f7-73d040900b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.669656</td>\n",
       "      <td>1.609213</td>\n",
       "      <td>1.720941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.256755</td>\n",
       "      <td>0.267639</td>\n",
       "      <td>0.200032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.225077</td>\n",
       "      <td>0.201883</td>\n",
       "      <td>0.153836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.048005</td>\n",
       "      <td>0.987705</td>\n",
       "      <td>1.068090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.292152</td>\n",
       "      <td>1.268547</td>\n",
       "      <td>1.311847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.048325</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.669656  1.609213  1.720941\n",
       "spearman_corr  0.256755  0.267639  0.200032\n",
       "pearson_corr   0.225077  0.201883  0.153836\n",
       "mae            1.048005  0.987705  1.068090\n",
       "rmse           1.292152  1.268547  1.311847\n",
       "r2             0.048325  0.010779  0.019093"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_metrics = pd.DataFrame({ 'ref':ref_token_test_metrics,\n",
    "                'alt': alt_token_test_metrics, \n",
    "                'altref': altref_token_test_metrics})\n",
    "\n",
    "token_metrics.to_csv('../res/metrics/test_agg1st/token_metrics.csv', index=False)\n",
    "token_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e147a11d-fb0f-4ee6-bea7-950433cdfc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.692386</td>\n",
       "      <td>1.759012</td>\n",
       "      <td>1.503381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.261613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.445995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.212503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.053869</td>\n",
       "      <td>1.089069</td>\n",
       "      <td>0.949296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.300917</td>\n",
       "      <td>1.326277</td>\n",
       "      <td>1.226124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.035370</td>\n",
       "      <td>-0.002606</td>\n",
       "      <td>0.143099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.692386  1.759012  1.503381\n",
       "spearman_corr  0.261613       NaN  0.445995\n",
       "pearson_corr   0.212503       NaN  0.378416\n",
       "mae            1.053869  1.089069  0.949296\n",
       "rmse           1.300917  1.326277  1.226124\n",
       "r2             0.035370 -0.002606  0.143099"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_metrics = pd.DataFrame({ 'ref':ref_sum_test_metrics,\n",
    "                'alt': alt_sum_test_metrics, \n",
    "                'altref': altref_sum_test_metrics})\n",
    "\n",
    "sum_metrics.to_csv('../res/metrics/test_agg1st/sum_metrics.csv', index=False)\n",
    "sum_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
