{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f48303-26fe-408a-b5e1-78b7519c9819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dde42ec-11da-4d0e-b3ad-4d7219300f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc2910e-a305-4d5d-8616-de067743de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>build</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>Hugo_Symbol</th>\n",
       "      <th>Transcript_ID</th>\n",
       "      <th>HGVSc</th>\n",
       "      <th>HGVSp</th>\n",
       "      <th>PTC_pos_codon</th>\n",
       "      <th>PTC_to_start_codon</th>\n",
       "      <th>...</th>\n",
       "      <th>var_token_idx</th>\n",
       "      <th>NMD_efficiency</th>\n",
       "      <th>t_vaf</th>\n",
       "      <th>t_depth</th>\n",
       "      <th>n_vaf</th>\n",
       "      <th>n_depth</th>\n",
       "      <th>VAF_RNA</th>\n",
       "      <th>depth_RNA</th>\n",
       "      <th>VAF_DNA_RNA_ratio</th>\n",
       "      <th>tpm_unstranded_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>944753</td>\n",
       "      <td>944753</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.2191C&gt;T</td>\n",
       "      <td>p.Gln731Ter</td>\n",
       "      <td>731</td>\n",
       "      <td>2193</td>\n",
       "      <td>...</td>\n",
       "      <td>2240</td>\n",
       "      <td>-0.508648</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>244.0</td>\n",
       "      <td>1.422717</td>\n",
       "      <td>76.8018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>952113</td>\n",
       "      <td>952113</td>\n",
       "      <td>NOC2L</td>\n",
       "      <td>ENST00000327044</td>\n",
       "      <td>c.1218G&gt;A</td>\n",
       "      <td>p.Trp406Ter</td>\n",
       "      <td>406</td>\n",
       "      <td>1218</td>\n",
       "      <td>...</td>\n",
       "      <td>1267</td>\n",
       "      <td>1.629509</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.323198</td>\n",
       "      <td>49.1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1255304</td>\n",
       "      <td>1255304</td>\n",
       "      <td>UBE2J2</td>\n",
       "      <td>ENST00000349431</td>\n",
       "      <td>c.679G&gt;T</td>\n",
       "      <td>p.Gly227Ter</td>\n",
       "      <td>227</td>\n",
       "      <td>681</td>\n",
       "      <td>...</td>\n",
       "      <td>898</td>\n",
       "      <td>0.047557</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.464435</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.967573</td>\n",
       "      <td>47.4110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1338573</td>\n",
       "      <td>1338573</td>\n",
       "      <td>DVL1</td>\n",
       "      <td>ENST00000378888</td>\n",
       "      <td>c.1288G&gt;T</td>\n",
       "      <td>p.Glu430Ter</td>\n",
       "      <td>430</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>1572</td>\n",
       "      <td>1.505167</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.145511</td>\n",
       "      <td>323.0</td>\n",
       "      <td>0.352289</td>\n",
       "      <td>50.0535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chr1</td>\n",
       "      <td>1387314</td>\n",
       "      <td>1387314</td>\n",
       "      <td>CCNL2</td>\n",
       "      <td>ENST00000400809</td>\n",
       "      <td>c.1480C&gt;T</td>\n",
       "      <td>p.Arg494Ter</td>\n",
       "      <td>494</td>\n",
       "      <td>1482</td>\n",
       "      <td>...</td>\n",
       "      <td>1485</td>\n",
       "      <td>-0.424007</td>\n",
       "      <td>0.407692</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.546980</td>\n",
       "      <td>298.0</td>\n",
       "      <td>1.341649</td>\n",
       "      <td>27.1883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4088</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>152920717</td>\n",
       "      <td>152920717</td>\n",
       "      <td>ZNF185</td>\n",
       "      <td>ENST00000370268</td>\n",
       "      <td>c.622C&gt;T</td>\n",
       "      <td>p.Gln208Ter</td>\n",
       "      <td>208</td>\n",
       "      <td>624</td>\n",
       "      <td>...</td>\n",
       "      <td>658</td>\n",
       "      <td>1.785660</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.290043</td>\n",
       "      <td>52.3474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4089</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>153650171</td>\n",
       "      <td>153650171</td>\n",
       "      <td>DUSP9</td>\n",
       "      <td>ENST00000342782</td>\n",
       "      <td>c.1021G&gt;T</td>\n",
       "      <td>p.Glu341Ter</td>\n",
       "      <td>341</td>\n",
       "      <td>1023</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>-1.360998</td>\n",
       "      <td>0.389313</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2.568627</td>\n",
       "      <td>16.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154030948</td>\n",
       "      <td>154030948</td>\n",
       "      <td>MECP2</td>\n",
       "      <td>ENST00000303391</td>\n",
       "      <td>c.880C&gt;T</td>\n",
       "      <td>p.Arg294Ter</td>\n",
       "      <td>294</td>\n",
       "      <td>882</td>\n",
       "      <td>...</td>\n",
       "      <td>1129</td>\n",
       "      <td>-1.166585</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.244797</td>\n",
       "      <td>11.5544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154354015</td>\n",
       "      <td>154354015</td>\n",
       "      <td>FLNA</td>\n",
       "      <td>ENST00000369850</td>\n",
       "      <td>c.5586C&gt;A</td>\n",
       "      <td>p.Tyr1862Ter</td>\n",
       "      <td>1862</td>\n",
       "      <td>5586</td>\n",
       "      <td>...</td>\n",
       "      <td>5834</td>\n",
       "      <td>0.332048</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.794408</td>\n",
       "      <td>63.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>GRCh38</td>\n",
       "      <td>chrX</td>\n",
       "      <td>154402790</td>\n",
       "      <td>154402790</td>\n",
       "      <td>DNASE1L1</td>\n",
       "      <td>ENST00000014935</td>\n",
       "      <td>c.826C&gt;T</td>\n",
       "      <td>p.Gln276Ter</td>\n",
       "      <td>276</td>\n",
       "      <td>828</td>\n",
       "      <td>...</td>\n",
       "      <td>1619</td>\n",
       "      <td>3.569856</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>3.9940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4093 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       build chromosome      start        end Hugo_Symbol    Transcript_ID  \\\n",
       "0     GRCh38       chr1     944753     944753       NOC2L  ENST00000327044   \n",
       "1     GRCh38       chr1     952113     952113       NOC2L  ENST00000327044   \n",
       "2     GRCh38       chr1    1255304    1255304      UBE2J2  ENST00000349431   \n",
       "3     GRCh38       chr1    1338573    1338573        DVL1  ENST00000378888   \n",
       "4     GRCh38       chr1    1387314    1387314       CCNL2  ENST00000400809   \n",
       "...      ...        ...        ...        ...         ...              ...   \n",
       "4088  GRCh38       chrX  152920717  152920717      ZNF185  ENST00000370268   \n",
       "4089  GRCh38       chrX  153650171  153650171       DUSP9  ENST00000342782   \n",
       "4090  GRCh38       chrX  154030948  154030948       MECP2  ENST00000303391   \n",
       "4091  GRCh38       chrX  154354015  154354015        FLNA  ENST00000369850   \n",
       "4092  GRCh38       chrX  154402790  154402790    DNASE1L1  ENST00000014935   \n",
       "\n",
       "          HGVSc         HGVSp  PTC_pos_codon  PTC_to_start_codon  ...  \\\n",
       "0     c.2191C>T   p.Gln731Ter            731                2193  ...   \n",
       "1     c.1218G>A   p.Trp406Ter            406                1218  ...   \n",
       "2      c.679G>T   p.Gly227Ter            227                 681  ...   \n",
       "3     c.1288G>T   p.Glu430Ter            430                1290  ...   \n",
       "4     c.1480C>T   p.Arg494Ter            494                1482  ...   \n",
       "...         ...           ...            ...                 ...  ...   \n",
       "4088   c.622C>T   p.Gln208Ter            208                 624  ...   \n",
       "4089  c.1021G>T   p.Glu341Ter            341                1023  ...   \n",
       "4090   c.880C>T   p.Arg294Ter            294                 882  ...   \n",
       "4091  c.5586C>A  p.Tyr1862Ter           1862                5586  ...   \n",
       "4092   c.826C>T   p.Gln276Ter            276                 828  ...   \n",
       "\n",
       "      var_token_idx  NMD_efficiency     t_vaf  t_depth  n_vaf  n_depth  \\\n",
       "0              2240       -0.508648  0.311111     45.0    0.0     43.0   \n",
       "1              1267        1.629509  0.390244     41.0    0.0     14.0   \n",
       "2               898        0.047557  0.480000     50.0    0.0     32.0   \n",
       "3              1572        1.505167  0.413043     46.0    0.0     63.0   \n",
       "4              1485       -0.424007  0.407692    130.0    0.0    152.0   \n",
       "...             ...             ...       ...      ...    ...      ...   \n",
       "4088            658        1.785660  0.328358     67.0    0.0     65.0   \n",
       "4089           1285       -1.360998  0.389313    131.0    0.0    185.0   \n",
       "4090           1129       -1.166585  0.406977     86.0    0.0     70.0   \n",
       "4091           5834        0.332048  0.904762    189.0    0.0    134.0   \n",
       "4092           1619        3.569856  0.339286     56.0    0.0     66.0   \n",
       "\n",
       "       VAF_RNA  depth_RNA VAF_DNA_RNA_ratio tpm_unstranded_x  \n",
       "0     0.442623      244.0          1.422717          76.8018  \n",
       "1     0.126126      111.0          0.323198          49.1731  \n",
       "2     0.464435      239.0          0.967573          47.4110  \n",
       "3     0.145511      323.0          0.352289          50.0535  \n",
       "4     0.546980      298.0          1.341649          27.1883  \n",
       "...        ...        ...               ...              ...  \n",
       "4088  0.095238       42.0          0.290043          52.3474  \n",
       "4089  1.000000       35.0          2.568627          16.9024  \n",
       "4090  0.913580       81.0          2.244797          11.5544  \n",
       "4091  0.718750       32.0          0.794408          63.2541  \n",
       "4092  0.028571      105.0          0.084211           3.9940  \n",
       "\n",
       "[4093 rows x 51 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/tcga_processed.tsv.gz', sep='\\t')\n",
    "\n",
    "### some variants are repeated multiple times\n",
    "df = df.drop(columns=['Cancer_type', 'Cancer_type_count', 'NMF_cluster'])\n",
    "\n",
    "agg_columns = [\n",
    "    'NMD_efficiency', 't_vaf', 't_depth', 'n_vaf', 'n_depth',\n",
    "    'VAF_RNA', 'depth_RNA', 'VAF_DNA_RNA_ratio', 'tpm_unstranded_x'\n",
    "]\n",
    "\n",
    "# Group by all other columns except the ones to aggregate\n",
    "group_by_columns = [col for col in df.columns if col not in agg_columns]\n",
    "\n",
    "# Perform grouping and aggregation without dropping NaNs\n",
    "df_unq = df.groupby(\n",
    "    group_by_columns, dropna=False, as_index=False\n",
    ").agg({col: 'mean' for col in agg_columns})\n",
    "\n",
    "df_unq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986f3012-8628-4708-9f98-7a247333993a",
   "metadata": {},
   "source": [
    "### Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd76f990-e27c-483b-bdd2-0d2dfe8f6f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000263207:c.433G>T',\n",
       "  'ENST00000359003:c.6013G>T',\n",
       "  'ENST00000279068:c.682C>T',\n",
       "  'ENST00000217244:c.916C>T',\n",
       "  'ENST00000373271:c.3826C>T'],\n",
       " 232)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var_ids = df_unq[df_unq['chromosome'].isin(['chr20', 'chr21', 'chr22'])]['var_id'].values.tolist()\n",
    "test_var_ids = list(set(test_var_ids))\n",
    "test_var_ids[0:5], len(test_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921ff515-7955-4fa4-a719-141e346e317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000245222:c.1709G>A',\n",
       "  'ENST00000541714:c.2695C>T',\n",
       "  'ENST00000356487:c.1576G>T',\n",
       "  'ENST00000221735:c.1504C>T',\n",
       "  'ENST00000222145:c.2839C>T'],\n",
       " 210)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_var_ids = df_unq[df_unq['chromosome']=='chr19']['var_id'].values.tolist()\n",
    "val_var_ids = list(set(val_var_ids))\n",
    "val_var_ids[0:5], len(val_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28f8153-7e43-4d33-9f7a-8fbe34ad327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENST00000380006:c.10C>T',\n",
       "  'ENST00000304698:c.2449C>T',\n",
       "  'ENST00000394434:c.2326G>T',\n",
       "  'ENST00000410053:c.1345G>T',\n",
       "  'ENST00000356657:c.456G>A'],\n",
       " 3651)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_var_ids = list(set(df_unq.var_id.values.tolist()) - set(val_var_ids) - set(test_var_ids))\n",
    "train_var_ids[0:5], len(train_var_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d265b37-0476-48e3-8e1b-d6d693a47d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries with NMD_efficiency as values\n",
    "test_dict = {}\n",
    "val_dict = {}\n",
    "train_dict = {}\n",
    "var_pos_idx_dict = {}\n",
    "\n",
    "for index, row in df_unq.iterrows():\n",
    "    if row['var_id'] in test_var_ids:\n",
    "        test_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    elif row['var_id'] in val_var_ids:\n",
    "        val_dict[row['var_id']] = row['NMD_efficiency']\n",
    "    else:\n",
    "        train_dict[row['var_id']] = row['NMD_efficiency']\n",
    "\n",
    "    var_pos_idx_dict[row['var_id']] = row['var_token_idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500916de-9ea7-482c-a762-6e928d6f913e",
   "metadata": {},
   "source": [
    "### Read embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89804ac4-08b3-4aca-a8a0-cb53ef3b9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tcga_ref_embeds.pkl', 'rb') as file:\n",
    "    ref_embeds = pickle.load(file)\n",
    "\n",
    "with open('../data/tcga_alt_embeds.pkl', 'rb') as file:\n",
    "    alt_embeds = pickle.load(file)\n",
    "\n",
    "# create alt - ref embeds\n",
    "altref_embeds = {}\n",
    "\n",
    "for var_id, alt_embed in alt_embeds.items():\n",
    "    altref_embeds[var_id] = alt_embed - ref_embeds[var_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07cd3e-36bd-4795-bb39-4d1b61ae6b42",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2731070d-47cd-42df-a957-9104061bfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameters\n",
    "# ---------------------------\n",
    "LEARNING_RATE = 5e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "HIDDEN_DIMS_phi = [8, 8]\n",
    "DROPOUT = 0.25\n",
    "N_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "VALIDATION_INTERVAL = 5  # Perform validation every 5 epochs\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Definition\n",
    "# ---------------------------\n",
    "class DeepSetDataset(Dataset):\n",
    "    def __init__(self, embeds_dict, target_dict, var_pos_idx_dict=None):\n",
    "        self.ids = list(target_dict.keys())\n",
    "        self.embeds = [embeds_dict[i] for i in self.ids]\n",
    "        self.targets = [target_dict[i] for i in self.ids]\n",
    "        self.var_pos_idx = [var_pos_idx_dict[i] if var_pos_idx_dict else None for i in self.ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embed = self.embeds[idx]\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        var_pos_idx = self.var_pos_idx[idx]\n",
    "        return embed, target, var_pos_idx\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeds, targets, var_pos_idx = zip(*batch)\n",
    "    embeds = [torch.as_tensor(embed, dtype=torch.float32) for embed in embeds]\n",
    "    embeds_padded = torch.nn.utils.rnn.pad_sequence(embeds, batch_first=True, padding_value=0.0)\n",
    "    targets = torch.tensor(targets, dtype=torch.float32)\n",
    "    mask = (embeds_padded != 0).any(dim=-1).float()\n",
    "    return embeds_padded, targets, mask, var_pos_idx\n",
    "\n",
    "# ---------------------------\n",
    "# DeepSet Model without Rho\n",
    "# ---------------------------\n",
    "class DeepSetModel_noRho(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        phi_hidden_dims: List[int],\n",
    "        output_dim: int = 1,\n",
    "        activation: str = \"ReLU\",\n",
    "        pool: str = \"mean\",\n",
    "        dropout: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = getattr(nn, activation)()\n",
    "        self.pool = pool\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "        \n",
    "        # Phi network\n",
    "        phi = []\n",
    "        in_dim = input_dim\n",
    "        for i, dim in enumerate(phi_hidden_dims):\n",
    "            phi.append((f\"phi_linear_{i}\", nn.Linear(in_dim, dim)))\n",
    "            if dropout:\n",
    "                phi.append((f\"phi_dropout_{i}\", self.dropout))\n",
    "            phi.append((f\"phi_activation_{i}\", self.activation))\n",
    "            in_dim = dim\n",
    "        phi.append((f\"phi_output\", nn.Linear(in_dim, output_dim)))\n",
    "        self.phi = nn.Sequential(OrderedDict(phi))\n",
    "\n",
    "    def forward(self, x, mask, var_pos_idx=None):\n",
    "        x = self.phi(x)\n",
    "        \n",
    "        if self.pool == \"sum\":\n",
    "            x = (x * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        elif self.pool == \"max\":\n",
    "            x = (x * mask.unsqueeze(-1)).masked_fill(mask.unsqueeze(-1) == 0, float('-inf')).max(dim=1).values\n",
    "        elif self.pool == \"mean\":\n",
    "            x = (x * mask.unsqueeze(-1)).sum(dim=1) / (mask.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        elif self.pool == \"token\" and var_pos_idx is not None:\n",
    "            x = torch.stack([x[i, idx] for i, idx in enumerate(var_pos_idx)])\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling operation: {self.pool}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation Function\n",
    "# ---------------------------\n",
    "'''def evaluate_regression_metrics(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }'''\n",
    "\n",
    "# Evaluation Metrics\n",
    "def evaluate_regression_metrics(y_true, y_pred):\n",
    "    y_true = y_true.cpu().numpy().flatten()\n",
    "    y_pred = y_pred.cpu().numpy().flatten()\n",
    "    loss = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(loss)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    try:\n",
    "        spearman_corr, _ = spearmanr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        spearman_corr = np.nan\n",
    "\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        pearson_corr = np.nan\n",
    "    \n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'pearson_corr': pearson_corr,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Training Function\n",
    "# ---------------------------\n",
    "def train_model(train_dict, val_dict, test_dict, embeds_dict, var_pos_idx_dict=None, pool='mean'):\n",
    "    train_loader = DataLoader(DeepSetDataset(embeds_dict, train_dict, var_pos_idx_dict), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(DeepSetDataset(embeds_dict, val_dict, var_pos_idx_dict), batch_size=32, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(DeepSetDataset(embeds_dict, test_dict, var_pos_idx_dict), batch_size=32, collate_fn=collate_fn)\n",
    "    \n",
    "    model = DeepSetModel_noRho(\n",
    "        input_dim=embeds_dict[list(embeds_dict.keys())[0]].shape[1],\n",
    "        phi_hidden_dims=HIDDEN_DIMS_phi,\n",
    "        pool=pool,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = nn.MSELoss()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    metrics = {'epoch': [], 'phase': [], 'loss': [], 'spearman_corr': [], 'pearson_corr':[], 'mae': [], 'rmse': [], 'r2': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = train_loader\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = val_loader\n",
    "            \n",
    "            all_preds, all_targets = [], []\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for embeds, targets, mask, var_pos_idx in loader:\n",
    "                embeds, targets, mask = embeds.to(device), targets.to(device), mask.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(embeds, mask, var_pos_idx).squeeze()\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * embeds.size(0)\n",
    "                all_preds.append(outputs.detach())\n",
    "                all_targets.append(targets.detach())\n",
    "            \n",
    "            epoch_loss = running_loss / len(loader.dataset)\n",
    "            all_preds = torch.cat(all_preds)\n",
    "            all_targets = torch.cat(all_targets)\n",
    "            \n",
    "            epoch_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "            epoch_metrics['loss'] = epoch_loss\n",
    "            \n",
    "            # Early stopping for validation phase\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        metrics_df = pd.DataFrame(metrics)\n",
    "                        test_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "                        test_metrics['loss'] = epoch_loss\n",
    "                        return metrics_df, test_metrics\n",
    "            \n",
    "            # Log metrics every 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}] Phase: {phase:5s} | \"\n",
    "                    f\"Loss: {epoch_metrics['loss']:.4f} | \"\n",
    "                    f\"Spearman: {epoch_metrics['spearman_corr']:.4f} | \"\n",
    "                    f\"Pearson: {epoch_metrics['pearson_corr']:.4f} | \"\n",
    "                    f\"MAE: {epoch_metrics['mae']:.4f} | \"\n",
    "                    f\"RMSE: {epoch_metrics['rmse']:.4f} | \"\n",
    "                    f\"R²: {epoch_metrics['r2']:.4f}\"\n",
    "                )\n",
    "                metrics['epoch'].append(epoch)\n",
    "                metrics['phase'].append(phase)\n",
    "                for key, value in epoch_metrics.items():\n",
    "                    metrics[key].append(value)\n",
    "    \n",
    "    # Final Test Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for embeds, targets, mask, var_pos_idx in test_loader:\n",
    "        embeds, targets, mask = embeds.to(device), targets.to(device), mask.to(device)\n",
    "        outputs = model(embeds, mask, var_pos_idx).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item() * embeds.size(0)\n",
    "        all_preds.append(outputs.detach())\n",
    "        all_targets.append(targets.detach())\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader.dataset)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    test_metrics = evaluate_regression_metrics(all_targets, all_preds)\n",
    "    test_metrics['loss'] = epoch_loss\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df, test_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab1bc25-546c-4d53-b7c5-8436732ee7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING REF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5586 | Spearman: 0.1374 | Pearson: 0.1389 | MAE: 1.0426 | RMSE: 1.2484 | R²: 0.0152\n",
      "[Epoch 005] Phase: val   | Loss: 1.6970 | Spearman: 0.3720 | Pearson: 0.3631 | MAE: 1.0454 | RMSE: 1.3027 | R²: 0.0328\n",
      "[Epoch 010] Phase: train | Loss: 1.4432 | Spearman: 0.3404 | Pearson: 0.3476 | MAE: 1.0007 | RMSE: 1.2013 | R²: 0.0881\n",
      "[Epoch 010] Phase: val   | Loss: 1.5489 | Spearman: 0.4375 | Pearson: 0.4374 | MAE: 0.9760 | RMSE: 1.2445 | R²: 0.1172\n",
      "[Epoch 015] Phase: train | Loss: 1.3682 | Spearman: 0.3719 | Pearson: 0.3767 | MAE: 0.9642 | RMSE: 1.1697 | R²: 0.1355\n",
      "[Epoch 015] Phase: val   | Loss: 1.4223 | Spearman: 0.4614 | Pearson: 0.4562 | MAE: 0.9526 | RMSE: 1.1926 | R²: 0.1893\n",
      "[Epoch 020] Phase: train | Loss: 1.3341 | Spearman: 0.3960 | Pearson: 0.3983 | MAE: 0.9446 | RMSE: 1.1550 | R²: 0.1570\n",
      "[Epoch 020] Phase: val   | Loss: 1.3835 | Spearman: 0.4729 | Pearson: 0.4681 | MAE: 0.9171 | RMSE: 1.1762 | R²: 0.2114\n",
      "[Epoch 025] Phase: train | Loss: 1.3282 | Spearman: 0.3956 | Pearson: 0.4016 | MAE: 0.9432 | RMSE: 1.1525 | R²: 0.1607\n",
      "[Epoch 025] Phase: val   | Loss: 1.3597 | Spearman: 0.4793 | Pearson: 0.4772 | MAE: 0.9115 | RMSE: 1.1661 | R²: 0.2250\n",
      "[Epoch 030] Phase: train | Loss: 1.3166 | Spearman: 0.4065 | Pearson: 0.4104 | MAE: 0.9332 | RMSE: 1.1474 | R²: 0.1681\n",
      "[Epoch 030] Phase: val   | Loss: 1.3505 | Spearman: 0.4877 | Pearson: 0.4854 | MAE: 0.9001 | RMSE: 1.1621 | R²: 0.2302\n",
      "[Epoch 035] Phase: train | Loss: 1.3059 | Spearman: 0.4134 | Pearson: 0.4187 | MAE: 0.9306 | RMSE: 1.1427 | R²: 0.1749\n",
      "[Epoch 035] Phase: val   | Loss: 1.3512 | Spearman: 0.4931 | Pearson: 0.4916 | MAE: 0.8961 | RMSE: 1.1624 | R²: 0.2298\n",
      "[Epoch 040] Phase: train | Loss: 1.2968 | Spearman: 0.4197 | Pearson: 0.4252 | MAE: 0.9242 | RMSE: 1.1388 | R²: 0.1806\n",
      "[Epoch 040] Phase: val   | Loss: 1.3538 | Spearman: 0.4960 | Pearson: 0.4947 | MAE: 0.8938 | RMSE: 1.1635 | R²: 0.2284\n",
      "[Epoch 045] Phase: train | Loss: 1.2964 | Spearman: 0.4216 | Pearson: 0.4255 | MAE: 0.9232 | RMSE: 1.1386 | R²: 0.1808\n",
      "[Epoch 045] Phase: val   | Loss: 1.3696 | Spearman: 0.5002 | Pearson: 0.4987 | MAE: 0.8947 | RMSE: 1.1703 | R²: 0.2194\n",
      "[Epoch 050] Phase: train | Loss: 1.2855 | Spearman: 0.4270 | Pearson: 0.4337 | MAE: 0.9200 | RMSE: 1.1338 | R²: 0.1877\n",
      "[Epoch 050] Phase: val   | Loss: 1.3478 | Spearman: 0.5063 | Pearson: 0.4996 | MAE: 0.8943 | RMSE: 1.1610 | R²: 0.2318\n",
      "START TRAINING REF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.4449 | Spearman: 0.3112 | Pearson: 0.2986 | MAE: 0.9917 | RMSE: 1.2020 | R²: 0.0870\n",
      "[Epoch 005] Phase: val   | Loss: 1.5965 | Spearman: 0.4521 | Pearson: 0.4432 | MAE: 0.9626 | RMSE: 1.2635 | R²: 0.0900\n",
      "[Epoch 010] Phase: train | Loss: 1.4103 | Spearman: 0.3459 | Pearson: 0.3340 | MAE: 0.9761 | RMSE: 1.1876 | R²: 0.1088\n",
      "[Epoch 010] Phase: val   | Loss: 1.6494 | Spearman: 0.4675 | Pearson: 0.4584 | MAE: 0.9637 | RMSE: 1.2843 | R²: 0.0598\n",
      "[Epoch 015] Phase: train | Loss: 1.3870 | Spearman: 0.3577 | Pearson: 0.3575 | MAE: 0.9700 | RMSE: 1.1777 | R²: 0.1236\n",
      "[Epoch 015] Phase: val   | Loss: 1.6994 | Spearman: 0.4422 | Pearson: 0.4185 | MAE: 0.9649 | RMSE: 1.3036 | R²: 0.0314\n",
      "[Epoch 020] Phase: train | Loss: 1.3674 | Spearman: 0.3764 | Pearson: 0.3723 | MAE: 0.9573 | RMSE: 1.1693 | R²: 0.1360\n",
      "[Epoch 020] Phase: val   | Loss: 1.6374 | Spearman: 0.4677 | Pearson: 0.4476 | MAE: 0.9485 | RMSE: 1.2796 | R²: 0.0667\n",
      "Early stopping at epoch 22\n",
      "START TRAINING REF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.6766 | Spearman: 0.0983 | Pearson: 0.0987 | MAE: 1.0463 | RMSE: 1.2948 | R²: -0.0594\n",
      "[Epoch 005] Phase: val   | Loss: 1.7301 | Spearman: 0.1645 | Pearson: 0.1463 | MAE: 1.0498 | RMSE: 1.3153 | R²: 0.0139\n",
      "[Epoch 010] Phase: train | Loss: 1.5563 | Spearman: 0.1858 | Pearson: 0.1969 | MAE: 1.0203 | RMSE: 1.2475 | R²: 0.0166\n",
      "[Epoch 010] Phase: val   | Loss: 1.7050 | Spearman: 0.2497 | Pearson: 0.1995 | MAE: 1.0258 | RMSE: 1.3058 | R²: 0.0282\n",
      "[Epoch 015] Phase: train | Loss: 1.5022 | Spearman: 0.2226 | Pearson: 0.2448 | MAE: 1.0036 | RMSE: 1.2257 | R²: 0.0508\n",
      "[Epoch 015] Phase: val   | Loss: 1.6963 | Spearman: 0.2105 | Pearson: 0.1830 | MAE: 1.0533 | RMSE: 1.3024 | R²: 0.0331\n",
      "[Epoch 020] Phase: train | Loss: 1.4803 | Spearman: 0.2558 | Pearson: 0.2652 | MAE: 0.9972 | RMSE: 1.2167 | R²: 0.0646\n",
      "[Epoch 020] Phase: val   | Loss: 1.7157 | Spearman: 0.1604 | Pearson: 0.1557 | MAE: 1.0554 | RMSE: 1.3098 | R²: 0.0221\n",
      "Early stopping at epoch 22\n",
      "START TRAINING REF SUM\n",
      "[Epoch 005] Phase: train | Loss: 1795.4828 | Spearman: 0.0781 | Pearson: 0.0484 | MAE: 28.3144 | RMSE: 42.3731 | R²: -1133.5293\n",
      "[Epoch 005] Phase: val   | Loss: 35205.0860 | Spearman: -0.0372 | Pearson: -0.0113 | MAE: 157.1498 | RMSE: 187.6302 | R²: -20065.2891\n",
      "[Epoch 010] Phase: train | Loss: 1134.7888 | Spearman: 0.0391 | Pearson: 0.0141 | MAE: 23.2514 | RMSE: 33.6866 | R²: -716.0501\n",
      "[Epoch 010] Phase: val   | Loss: 19712.8988 | Spearman: -0.0676 | Pearson: -0.0339 | MAE: 117.0361 | RMSE: 140.4026 | R²: -11235.0098\n",
      "[Epoch 015] Phase: train | Loss: 812.1001 | Spearman: 0.0241 | Pearson: 0.0081 | MAE: 20.5491 | RMSE: 28.4974 | R²: -512.1496\n",
      "[Epoch 015] Phase: val   | Loss: 18728.0703 | Spearman: -0.0629 | Pearson: -0.0280 | MAE: 115.3991 | RMSE: 136.8505 | R²: -10673.6729\n",
      "[Epoch 020] Phase: train | Loss: 650.9212 | Spearman: -0.0057 | Pearson: -0.0133 | MAE: 18.5172 | RMSE: 25.5132 | R²: -410.3039\n",
      "[Epoch 020] Phase: val   | Loss: 13430.9493 | Spearman: -0.0657 | Pearson: -0.0282 | MAE: 97.9668 | RMSE: 115.8920 | R²: -7654.4067\n",
      "[Epoch 025] Phase: train | Loss: 657.7257 | Spearman: -0.0067 | Pearson: -0.0087 | MAE: 18.4100 | RMSE: 25.6462 | R²: -414.6035\n",
      "[Epoch 025] Phase: val   | Loss: 18562.4314 | Spearman: -0.0519 | Pearson: -0.0200 | MAE: 115.8003 | RMSE: 136.2440 | R²: -10579.2627\n",
      "[Epoch 030] Phase: train | Loss: 477.7974 | Spearman: -0.0155 | Pearson: -0.0135 | MAE: 15.9267 | RMSE: 21.8586 | R²: -300.9105\n",
      "[Epoch 030] Phase: val   | Loss: 16395.6623 | Spearman: -0.0409 | Pearson: -0.0091 | MAE: 109.6806 | RMSE: 128.0455 | R²: -9344.2412\n",
      "Early stopping at epoch 34\n"
     ]
    }
   ],
   "source": [
    "### Ref embeds\n",
    "\n",
    "print(\"START TRAINING REF MEAN\")\n",
    "ref_mean_metrics_df, ref_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "ref_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/ref_mean_history_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF MAX\")\n",
    "ref_max_metrics_df, ref_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "ref_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/ref_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF TOKEN\")\n",
    "ref_token_metrics_df, ref_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "ref_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/ref_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING REF SUM\")\n",
    "ref_sum_metrics_df, ref_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=ref_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "ref_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/ref_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fe4d1d9-ab8e-4751-9113-413d9464b24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALT MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.4438 | Spearman: 0.3593 | Pearson: 0.3581 | MAE: 1.0007 | RMSE: 1.2016 | R²: 0.0877\n",
      "[Epoch 005] Phase: val   | Loss: 1.5431 | Spearman: 0.4301 | Pearson: 0.4408 | MAE: 1.0151 | RMSE: 1.2422 | R²: 0.1204\n",
      "[Epoch 010] Phase: train | Loss: 1.3429 | Spearman: 0.3973 | Pearson: 0.3958 | MAE: 0.9519 | RMSE: 1.1589 | R²: 0.1514\n",
      "[Epoch 010] Phase: val   | Loss: 1.4103 | Spearman: 0.4569 | Pearson: 0.4624 | MAE: 0.9469 | RMSE: 1.1875 | R²: 0.1962\n",
      "[Epoch 015] Phase: train | Loss: 1.3190 | Spearman: 0.4111 | Pearson: 0.4097 | MAE: 0.9364 | RMSE: 1.1485 | R²: 0.1666\n",
      "[Epoch 015] Phase: val   | Loss: 1.3823 | Spearman: 0.4731 | Pearson: 0.4750 | MAE: 0.9190 | RMSE: 1.1757 | R²: 0.2121\n",
      "[Epoch 020] Phase: train | Loss: 1.3026 | Spearman: 0.4203 | Pearson: 0.4218 | MAE: 0.9283 | RMSE: 1.1413 | R²: 0.1769\n",
      "[Epoch 020] Phase: val   | Loss: 1.3429 | Spearman: 0.4902 | Pearson: 0.4889 | MAE: 0.9162 | RMSE: 1.1588 | R²: 0.2346\n",
      "[Epoch 025] Phase: train | Loss: 1.2853 | Spearman: 0.4304 | Pearson: 0.4341 | MAE: 0.9190 | RMSE: 1.1337 | R²: 0.1879\n",
      "[Epoch 025] Phase: val   | Loss: 1.3500 | Spearman: 0.5011 | Pearson: 0.4960 | MAE: 0.8950 | RMSE: 1.1619 | R²: 0.2305\n",
      "[Epoch 030] Phase: train | Loss: 1.2784 | Spearman: 0.4342 | Pearson: 0.4394 | MAE: 0.9181 | RMSE: 1.1306 | R²: 0.1922\n",
      "[Epoch 030] Phase: val   | Loss: 1.3302 | Spearman: 0.5072 | Pearson: 0.4996 | MAE: 0.8918 | RMSE: 1.1533 | R²: 0.2418\n",
      "[Epoch 035] Phase: train | Loss: 1.2636 | Spearman: 0.4417 | Pearson: 0.4490 | MAE: 0.9076 | RMSE: 1.1241 | R²: 0.2015\n",
      "[Epoch 035] Phase: val   | Loss: 1.3272 | Spearman: 0.5111 | Pearson: 0.5015 | MAE: 0.8896 | RMSE: 1.1520 | R²: 0.2435\n",
      "[Epoch 040] Phase: train | Loss: 1.2715 | Spearman: 0.4369 | Pearson: 0.4443 | MAE: 0.9157 | RMSE: 1.1276 | R²: 0.1966\n",
      "[Epoch 040] Phase: val   | Loss: 1.3136 | Spearman: 0.5129 | Pearson: 0.5016 | MAE: 0.8949 | RMSE: 1.1461 | R²: 0.2513\n",
      "[Epoch 045] Phase: train | Loss: 1.2488 | Spearman: 0.4513 | Pearson: 0.4595 | MAE: 0.9030 | RMSE: 1.1175 | R²: 0.2109\n",
      "[Epoch 045] Phase: val   | Loss: 1.3074 | Spearman: 0.5187 | Pearson: 0.5070 | MAE: 0.8907 | RMSE: 1.1434 | R²: 0.2548\n",
      "[Epoch 050] Phase: train | Loss: 1.2423 | Spearman: 0.4557 | Pearson: 0.4638 | MAE: 0.8979 | RMSE: 1.1146 | R²: 0.2150\n",
      "[Epoch 050] Phase: val   | Loss: 1.3107 | Spearman: 0.5224 | Pearson: 0.5074 | MAE: 0.8863 | RMSE: 1.1449 | R²: 0.2529\n",
      "START TRAINING ALT MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.4478 | Spearman: 0.3059 | Pearson: 0.2954 | MAE: 0.9959 | RMSE: 1.2032 | R²: 0.0852\n",
      "[Epoch 005] Phase: val   | Loss: 1.6426 | Spearman: 0.4048 | Pearson: 0.4088 | MAE: 0.9808 | RMSE: 1.2816 | R²: 0.0637\n",
      "[Epoch 010] Phase: train | Loss: 1.4258 | Spearman: 0.3236 | Pearson: 0.3164 | MAE: 0.9833 | RMSE: 1.1941 | R²: 0.0990\n",
      "[Epoch 010] Phase: val   | Loss: 1.6074 | Spearman: 0.4727 | Pearson: 0.4640 | MAE: 0.9545 | RMSE: 1.2678 | R²: 0.0838\n",
      "[Epoch 015] Phase: train | Loss: 1.3994 | Spearman: 0.3424 | Pearson: 0.3408 | MAE: 0.9716 | RMSE: 1.1830 | R²: 0.1157\n",
      "[Epoch 015] Phase: val   | Loss: 1.6279 | Spearman: 0.4395 | Pearson: 0.4524 | MAE: 0.9545 | RMSE: 1.2759 | R²: 0.0722\n",
      "[Epoch 020] Phase: train | Loss: 1.3727 | Spearman: 0.3709 | Pearson: 0.3668 | MAE: 0.9596 | RMSE: 1.1716 | R²: 0.1326\n",
      "[Epoch 020] Phase: val   | Loss: 1.6264 | Spearman: 0.4221 | Pearson: 0.4327 | MAE: 0.9507 | RMSE: 1.2753 | R²: 0.0730\n",
      "Early stopping at epoch 22\n",
      "START TRAINING ALT TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.5893 | Spearman: 0.1485 | Pearson: 0.1556 | MAE: 1.0298 | RMSE: 1.2607 | R²: -0.0043\n",
      "[Epoch 005] Phase: val   | Loss: 1.6833 | Spearman: 0.2490 | Pearson: 0.2038 | MAE: 1.0451 | RMSE: 1.2974 | R²: 0.0405\n",
      "[Epoch 010] Phase: train | Loss: 1.5263 | Spearman: 0.2100 | Pearson: 0.2201 | MAE: 1.0124 | RMSE: 1.2354 | R²: 0.0355\n",
      "[Epoch 010] Phase: val   | Loss: 1.6826 | Spearman: 0.2576 | Pearson: 0.2072 | MAE: 1.0514 | RMSE: 1.2971 | R²: 0.0410\n",
      "[Epoch 015] Phase: train | Loss: 1.4700 | Spearman: 0.2748 | Pearson: 0.2736 | MAE: 0.9914 | RMSE: 1.2124 | R²: 0.0711\n",
      "[Epoch 015] Phase: val   | Loss: 1.6645 | Spearman: 0.2719 | Pearson: 0.2417 | MAE: 1.0216 | RMSE: 1.2901 | R²: 0.0513\n",
      "[Epoch 020] Phase: train | Loss: 1.4549 | Spearman: 0.2772 | Pearson: 0.2878 | MAE: 0.9867 | RMSE: 1.2062 | R²: 0.0807\n",
      "[Epoch 020] Phase: val   | Loss: 1.6340 | Spearman: 0.3141 | Pearson: 0.2660 | MAE: 1.0302 | RMSE: 1.2783 | R²: 0.0687\n",
      "Early stopping at epoch 24\n",
      "START TRAINING ALT SUM\n",
      "[Epoch 005] Phase: train | Loss: 1487.1563 | Spearman: -0.0149 | Pearson: 0.0089 | MAE: 26.7129 | RMSE: 38.5637 | R²: -938.7040\n",
      "[Epoch 005] Phase: val   | Loss: 2817.4719 | Spearman: -0.0493 | Pearson: -0.0134 | MAE: 42.6687 | RMSE: 53.0799 | R²: -1604.9100\n",
      "[Epoch 010] Phase: train | Loss: 837.5747 | Spearman: 0.0001 | Pearson: 0.0098 | MAE: 20.8044 | RMSE: 28.9409 | R²: -528.2465\n",
      "[Epoch 010] Phase: val   | Loss: 438.6599 | Spearman: -0.1024 | Pearson: -0.0565 | MAE: 13.1865 | RMSE: 20.9442 | R²: -249.0285\n",
      "[Epoch 015] Phase: train | Loss: 600.1223 | Spearman: 0.0081 | Pearson: 0.0133 | MAE: 17.8016 | RMSE: 24.4974 | R²: -378.2050\n",
      "[Epoch 015] Phase: val   | Loss: 742.9257 | Spearman: -0.0726 | Pearson: -0.0220 | MAE: 19.4257 | RMSE: 27.2567 | R²: -422.4547\n",
      "[Epoch 020] Phase: train | Loss: 487.6414 | Spearman: 0.0149 | Pearson: 0.0197 | MAE: 16.2594 | RMSE: 22.0826 | R²: -307.1307\n",
      "[Epoch 020] Phase: val   | Loss: 771.1687 | Spearman: -0.1039 | Pearson: -0.0317 | MAE: 19.9665 | RMSE: 27.7699 | R²: -438.5527\n",
      "[Epoch 025] Phase: train | Loss: 547.6545 | Spearman: -0.0190 | Pearson: -0.0086 | MAE: 16.7029 | RMSE: 23.4020 | R²: -345.0518\n",
      "[Epoch 025] Phase: val   | Loss: 495.0894 | Spearman: -0.1867 | Pearson: -0.0853 | MAE: 15.7275 | RMSE: 22.2506 | R²: -281.1923\n",
      "[Epoch 030] Phase: train | Loss: 406.7745 | Spearman: 0.0166 | Pearson: 0.0120 | MAE: 14.6672 | RMSE: 20.1687 | R²: -256.0326\n",
      "[Epoch 030] Phase: val   | Loss: 232.9803 | Spearman: -0.1550 | Pearson: -0.0762 | MAE: 9.2913 | RMSE: 15.2637 | R²: -131.7947\n",
      "[Epoch 035] Phase: train | Loss: 388.7555 | Spearman: 0.0133 | Pearson: 0.0259 | MAE: 14.3331 | RMSE: 19.7169 | R²: -244.6468\n",
      "[Epoch 035] Phase: val   | Loss: 156.5980 | Spearman: -0.1828 | Pearson: -0.0897 | MAE: 7.7979 | RMSE: 12.5139 | R²: -88.2581\n",
      "[Epoch 040] Phase: train | Loss: 382.2045 | Spearman: 0.0228 | Pearson: 0.0243 | MAE: 14.0993 | RMSE: 19.5501 | R²: -240.5072\n",
      "[Epoch 040] Phase: val   | Loss: 129.5008 | Spearman: -0.0106 | Pearson: -0.0129 | MAE: 7.3283 | RMSE: 11.3798 | R²: -72.8132\n",
      "[Epoch 045] Phase: train | Loss: 462.7676 | Spearman: 0.0011 | Pearson: -0.0027 | MAE: 15.0079 | RMSE: 21.5120 | R²: -291.4134\n",
      "[Epoch 045] Phase: val   | Loss: 131.6499 | Spearman: -0.1729 | Pearson: -0.1071 | MAE: 7.7207 | RMSE: 11.4739 | R²: -74.0382\n",
      "[Epoch 050] Phase: train | Loss: 308.6300 | Spearman: 0.0491 | Pearson: 0.0390 | MAE: 12.6726 | RMSE: 17.5679 | R²: -194.0170\n",
      "[Epoch 050] Phase: val   | Loss: 65.2568 | Spearman: -0.1212 | Pearson: -0.0993 | MAE: 5.7357 | RMSE: 8.0782 | R²: -36.1952\n"
     ]
    }
   ],
   "source": [
    "### Alt embeds\n",
    "\n",
    "print(\"START TRAINING ALT MEAN\")\n",
    "alt_mean_metrics_df, alt_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "alt_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/alt_mean_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT MAX\")\n",
    "alt_max_metrics_df, alt_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "alt_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/alt_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT TOKEN\")\n",
    "alt_token_metrics_df, alt_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "alt_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/alt_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALT SUM\")\n",
    "alt_sum_metrics_df, alt_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=alt_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "alt_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/alt_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd77d45-1703-4072-834f-96b0d8cf0c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING ALTREF MEAN\n",
      "[Epoch 005] Phase: train | Loss: 1.5692 | Spearman: 0.2031 | Pearson: 0.1678 | MAE: 1.0388 | RMSE: 1.2527 | R²: 0.0085\n",
      "[Epoch 005] Phase: val   | Loss: 1.7162 | Spearman: 0.3852 | Pearson: 0.3337 | MAE: 1.0662 | RMSE: 1.3100 | R²: 0.0218\n",
      "[Epoch 010] Phase: train | Loss: 1.4367 | Spearman: 0.4062 | Pearson: 0.3503 | MAE: 0.9931 | RMSE: 1.1986 | R²: 0.0922\n",
      "[Epoch 010] Phase: val   | Loss: 1.6241 | Spearman: 0.3492 | Pearson: 0.2870 | MAE: 1.0233 | RMSE: 1.2744 | R²: 0.0743\n",
      "[Epoch 015] Phase: train | Loss: 1.3360 | Spearman: 0.4593 | Pearson: 0.4127 | MAE: 0.9453 | RMSE: 1.1559 | R²: 0.1558\n",
      "[Epoch 015] Phase: val   | Loss: 1.5403 | Spearman: 0.4029 | Pearson: 0.3527 | MAE: 0.9800 | RMSE: 1.2411 | R²: 0.1220\n",
      "[Epoch 020] Phase: train | Loss: 1.2870 | Spearman: 0.4835 | Pearson: 0.4402 | MAE: 0.9210 | RMSE: 1.1344 | R²: 0.1868\n",
      "[Epoch 020] Phase: val   | Loss: 1.5035 | Spearman: 0.4364 | Pearson: 0.3836 | MAE: 0.9721 | RMSE: 1.2262 | R²: 0.1430\n",
      "[Epoch 025] Phase: train | Loss: 1.2552 | Spearman: 0.5001 | Pearson: 0.4607 | MAE: 0.9039 | RMSE: 1.1204 | R²: 0.2069\n",
      "[Epoch 025] Phase: val   | Loss: 1.4855 | Spearman: 0.4555 | Pearson: 0.3979 | MAE: 0.9665 | RMSE: 1.2188 | R²: 0.1533\n",
      "[Epoch 030] Phase: train | Loss: 1.2332 | Spearman: 0.5181 | Pearson: 0.4740 | MAE: 0.8941 | RMSE: 1.1105 | R²: 0.2208\n",
      "[Epoch 030] Phase: val   | Loss: 1.4667 | Spearman: 0.4640 | Pearson: 0.4076 | MAE: 0.9535 | RMSE: 1.2111 | R²: 0.1640\n",
      "[Epoch 035] Phase: train | Loss: 1.2086 | Spearman: 0.5318 | Pearson: 0.4909 | MAE: 0.8858 | RMSE: 1.0993 | R²: 0.2363\n",
      "[Epoch 035] Phase: val   | Loss: 1.4562 | Spearman: 0.4753 | Pearson: 0.4156 | MAE: 0.9440 | RMSE: 1.2067 | R²: 0.1700\n",
      "[Epoch 040] Phase: train | Loss: 1.1911 | Spearman: 0.5445 | Pearson: 0.5010 | MAE: 0.8780 | RMSE: 1.0914 | R²: 0.2473\n",
      "[Epoch 040] Phase: val   | Loss: 1.4484 | Spearman: 0.4827 | Pearson: 0.4223 | MAE: 0.9418 | RMSE: 1.2035 | R²: 0.1744\n",
      "[Epoch 045] Phase: train | Loss: 1.1759 | Spearman: 0.5536 | Pearson: 0.5109 | MAE: 0.8708 | RMSE: 1.0844 | R²: 0.2570\n",
      "[Epoch 045] Phase: val   | Loss: 1.4370 | Spearman: 0.4933 | Pearson: 0.4327 | MAE: 0.9460 | RMSE: 1.1987 | R²: 0.1809\n",
      "[Epoch 050] Phase: train | Loss: 1.1570 | Spearman: 0.5668 | Pearson: 0.5222 | MAE: 0.8644 | RMSE: 1.0756 | R²: 0.2689\n",
      "[Epoch 050] Phase: val   | Loss: 1.4359 | Spearman: 0.5019 | Pearson: 0.4372 | MAE: 0.9465 | RMSE: 1.1983 | R²: 0.1816\n",
      "START TRAINING ALTREF MAX\n",
      "[Epoch 005] Phase: train | Loss: 1.6168 | Spearman: 0.0750 | Pearson: 0.0722 | MAE: 1.0435 | RMSE: 1.2715 | R²: -0.0216\n",
      "[Epoch 005] Phase: val   | Loss: 1.7367 | Spearman: 0.1415 | Pearson: 0.2099 | MAE: 1.0362 | RMSE: 1.3178 | R²: 0.0101\n",
      "[Epoch 010] Phase: train | Loss: 1.5390 | Spearman: 0.1666 | Pearson: 0.1693 | MAE: 1.0291 | RMSE: 1.2405 | R²: 0.0276\n",
      "[Epoch 010] Phase: val   | Loss: 1.7000 | Spearman: 0.1574 | Pearson: 0.2111 | MAE: 1.0438 | RMSE: 1.3038 | R²: 0.0310\n",
      "[Epoch 015] Phase: train | Loss: 1.5392 | Spearman: 0.1621 | Pearson: 0.1660 | MAE: 1.0336 | RMSE: 1.2407 | R²: 0.0274\n",
      "[Epoch 015] Phase: val   | Loss: 1.7093 | Spearman: 0.2103 | Pearson: 0.2562 | MAE: 1.0333 | RMSE: 1.3074 | R²: 0.0257\n",
      "[Epoch 020] Phase: train | Loss: 1.5334 | Spearman: 0.1821 | Pearson: 0.1765 | MAE: 1.0287 | RMSE: 1.2383 | R²: 0.0310\n",
      "Early stopping at epoch 20\n",
      "START TRAINING ALTREF TOKEN\n",
      "[Epoch 005] Phase: train | Loss: 1.6307 | Spearman: 0.0691 | Pearson: 0.0814 | MAE: 1.0462 | RMSE: 1.2770 | R²: -0.0304\n",
      "[Epoch 005] Phase: val   | Loss: 1.7650 | Spearman: 0.0428 | Pearson: 0.0529 | MAE: 1.0560 | RMSE: 1.3285 | R²: -0.0060\n",
      "[Epoch 010] Phase: train | Loss: 1.5710 | Spearman: 0.1226 | Pearson: 0.1322 | MAE: 1.0351 | RMSE: 1.2534 | R²: 0.0073\n",
      "[Epoch 010] Phase: val   | Loss: 1.6961 | Spearman: 0.1933 | Pearson: 0.1990 | MAE: 1.0564 | RMSE: 1.3023 | R²: 0.0333\n",
      "[Epoch 015] Phase: train | Loss: 1.5238 | Spearman: 0.1928 | Pearson: 0.1972 | MAE: 1.0207 | RMSE: 1.2344 | R²: 0.0372\n",
      "[Epoch 015] Phase: val   | Loss: 1.6817 | Spearman: 0.2127 | Pearson: 0.2218 | MAE: 1.0465 | RMSE: 1.2968 | R²: 0.0414\n",
      "[Epoch 020] Phase: train | Loss: 1.5136 | Spearman: 0.1976 | Pearson: 0.2109 | MAE: 1.0223 | RMSE: 1.2303 | R²: 0.0436\n",
      "[Epoch 020] Phase: val   | Loss: 1.7080 | Spearman: 0.2115 | Pearson: 0.1729 | MAE: 1.0523 | RMSE: 1.3069 | R²: 0.0265\n",
      "[Epoch 025] Phase: train | Loss: 1.4888 | Spearman: 0.2278 | Pearson: 0.2436 | MAE: 1.0109 | RMSE: 1.2202 | R²: 0.0593\n",
      "[Epoch 025] Phase: val   | Loss: 1.6695 | Spearman: 0.2309 | Pearson: 0.2397 | MAE: 1.0465 | RMSE: 1.2921 | R²: 0.0484\n",
      "[Epoch 030] Phase: train | Loss: 1.4467 | Spearman: 0.2830 | Pearson: 0.2942 | MAE: 0.9906 | RMSE: 1.2028 | R²: 0.0858\n",
      "[Epoch 030] Phase: val   | Loss: 1.6495 | Spearman: 0.2675 | Pearson: 0.2813 | MAE: 1.0291 | RMSE: 1.2843 | R²: 0.0598\n",
      "[Epoch 035] Phase: train | Loss: 1.3979 | Spearman: 0.3108 | Pearson: 0.3441 | MAE: 0.9785 | RMSE: 1.1823 | R²: 0.1167\n",
      "[Epoch 035] Phase: val   | Loss: 1.6638 | Spearman: 0.2546 | Pearson: 0.2327 | MAE: 1.0406 | RMSE: 1.2899 | R²: 0.0517\n",
      "[Epoch 040] Phase: train | Loss: 1.4077 | Spearman: 0.3192 | Pearson: 0.3340 | MAE: 0.9772 | RMSE: 1.1865 | R²: 0.1105\n",
      "[Epoch 040] Phase: val   | Loss: 1.6747 | Spearman: 0.2342 | Pearson: 0.2142 | MAE: 1.0353 | RMSE: 1.2941 | R²: 0.0455\n",
      "[Epoch 045] Phase: train | Loss: 1.3472 | Spearman: 0.3725 | Pearson: 0.3868 | MAE: 0.9480 | RMSE: 1.1607 | R²: 0.1487\n",
      "[Epoch 045] Phase: val   | Loss: 1.6681 | Spearman: 0.2487 | Pearson: 0.2248 | MAE: 1.0313 | RMSE: 1.2915 | R²: 0.0492\n",
      "Early stopping at epoch 49\n",
      "START TRAINING ALTREF SUM\n",
      "[Epoch 005] Phase: train | Loss: 236.1186 | Spearman: 0.0230 | Pearson: 0.0158 | MAE: 11.1050 | RMSE: 15.3662 | R²: -148.1986\n",
      "[Epoch 005] Phase: val   | Loss: 110.5434 | Spearman: 0.1370 | Pearson: 0.0700 | MAE: 6.8755 | RMSE: 10.5140 | R²: -62.0078\n",
      "[Epoch 010] Phase: train | Loss: 105.1504 | Spearman: 0.0098 | Pearson: -0.0017 | MAE: 7.6225 | RMSE: 10.2543 | R²: -65.4424\n",
      "[Epoch 010] Phase: val   | Loss: 52.2765 | Spearman: 0.0895 | Pearson: 0.0358 | MAE: 4.6313 | RMSE: 7.2303 | R²: -28.7967\n",
      "[Epoch 015] Phase: train | Loss: 62.7519 | Spearman: 0.0030 | Pearson: -0.0097 | MAE: 5.9996 | RMSE: 7.9216 | R²: -38.6517\n",
      "[Epoch 015] Phase: val   | Loss: 22.2091 | Spearman: 0.1183 | Pearson: 0.0576 | MAE: 3.0899 | RMSE: 4.7126 | R²: -11.6588\n",
      "[Epoch 020] Phase: train | Loss: 41.5091 | Spearman: 0.0184 | Pearson: 0.0104 | MAE: 4.8948 | RMSE: 6.4428 | R²: -25.2288\n",
      "[Epoch 020] Phase: val   | Loss: 13.9499 | Spearman: 0.1217 | Pearson: 0.0676 | MAE: 2.6647 | RMSE: 3.7350 | R²: -6.9512\n",
      "[Epoch 025] Phase: train | Loss: 30.3356 | Spearman: 0.0416 | Pearson: 0.0386 | MAE: 4.2430 | RMSE: 5.5078 | R²: -18.1685\n",
      "[Epoch 025] Phase: val   | Loss: 7.6102 | Spearman: 0.1579 | Pearson: 0.0889 | MAE: 1.9680 | RMSE: 2.7587 | R²: -3.3377\n",
      "[Epoch 030] Phase: train | Loss: 26.5662 | Spearman: 0.0254 | Pearson: 0.0293 | MAE: 3.9095 | RMSE: 5.1542 | R²: -15.7866\n",
      "[Epoch 030] Phase: val   | Loss: 7.4932 | Spearman: 0.1904 | Pearson: 0.1292 | MAE: 2.1143 | RMSE: 2.7374 | R²: -3.2710\n",
      "[Epoch 035] Phase: train | Loss: 21.7501 | Spearman: 0.0503 | Pearson: 0.0496 | MAE: 3.5302 | RMSE: 4.6637 | R²: -12.7435\n",
      "[Epoch 035] Phase: val   | Loss: 3.9736 | Spearman: 0.2433 | Pearson: 0.1341 | MAE: 1.4569 | RMSE: 1.9934 | R²: -1.2649\n",
      "[Epoch 040] Phase: train | Loss: 16.9787 | Spearman: 0.0868 | Pearson: 0.0788 | MAE: 3.1194 | RMSE: 4.1205 | R²: -9.7285\n",
      "[Epoch 040] Phase: val   | Loss: 3.3323 | Spearman: 0.3110 | Pearson: 0.2144 | MAE: 1.3649 | RMSE: 1.8255 | R²: -0.8994\n",
      "[Epoch 045] Phase: train | Loss: 17.3672 | Spearman: 0.0828 | Pearson: 0.0681 | MAE: 3.0775 | RMSE: 4.1674 | R²: -9.9740\n",
      "[Epoch 045] Phase: val   | Loss: 5.8883 | Spearman: 0.3010 | Pearson: 0.2631 | MAE: 1.9224 | RMSE: 2.4266 | R²: -2.3562\n",
      "[Epoch 050] Phase: train | Loss: 15.5076 | Spearman: 0.1423 | Pearson: 0.1157 | MAE: 2.8815 | RMSE: 3.9380 | R²: -8.7990\n",
      "[Epoch 050] Phase: val   | Loss: 9.5457 | Spearman: 0.2816 | Pearson: 0.2524 | MAE: 2.5077 | RMSE: 3.0896 | R²: -4.4409\n"
     ]
    }
   ],
   "source": [
    "### Alt - Ref embeds\n",
    "\n",
    "print(\"START TRAINING ALTREF MEAN\")\n",
    "altref_mean_metrics_df, altref_mean_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='mean'  \n",
    "    )\n",
    "altref_mean_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/altref_mean_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF MAX\")\n",
    "altref_max_metrics_df, altref_max_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='max'  \n",
    "    )\n",
    "altref_max_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/altref_max_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF TOKEN\")\n",
    "altref_token_metrics_df, altref_token_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='token',\n",
    "        var_pos_idx_dict=var_pos_idx_dict\n",
    "    )\n",
    "altref_token_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/altref_token_metrics_df.csv')\n",
    "\n",
    "print(\"START TRAINING ALTREF SUM\")\n",
    "altref_sum_metrics_df, altref_sum_test_metrics = train_model(\n",
    "        train_dict=train_dict,\n",
    "        val_dict=val_dict,\n",
    "        test_dict=test_dict,\n",
    "        embeds_dict=altref_embeds,\n",
    "        pool='sum'  \n",
    "    )\n",
    "altref_sum_metrics_df.to_csv('../res/metrics/per_epoch_deepset_norho/altref_sum_metrics_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8676dd98-3ad8-4ca9-897e-653d4dd95efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.389480</td>\n",
       "      <td>1.358351</td>\n",
       "      <td>1.399738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.472282</td>\n",
       "      <td>0.466998</td>\n",
       "      <td>0.462107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.443535</td>\n",
       "      <td>0.436449</td>\n",
       "      <td>0.385084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.890945</td>\n",
       "      <td>0.890734</td>\n",
       "      <td>0.930894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.178762</td>\n",
       "      <td>1.165483</td>\n",
       "      <td>1.183105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.145854</td>\n",
       "      <td>0.164989</td>\n",
       "      <td>0.139548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.389480  1.358351  1.399738\n",
       "spearman_corr  0.472282  0.466998  0.462107\n",
       "pearson_corr   0.443535  0.436449  0.385084\n",
       "mae            0.890945  0.890734  0.930894\n",
       "rmse           1.178762  1.165483  1.183105\n",
       "r2             0.145854  0.164989  0.139548"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = pd.DataFrame({ 'ref':ref_mean_test_metrics,\n",
    "                'alt': alt_mean_test_metrics, \n",
    "                'altref': altref_mean_test_metrics})\n",
    "\n",
    "mean_metrics.to_csv('../res/metrics/test_deepset_norho/mean_metrics.csv')\n",
    "mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2894507d-b0c9-48fa-bf6a-137d9e77b464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.622690</td>\n",
       "      <td>1.642522</td>\n",
       "      <td>1.724663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.457282</td>\n",
       "      <td>0.187671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.444923</td>\n",
       "      <td>0.450530</td>\n",
       "      <td>0.209485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.942375</td>\n",
       "      <td>0.944849</td>\n",
       "      <td>1.039607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.273849</td>\n",
       "      <td>1.281609</td>\n",
       "      <td>1.313264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.075095</td>\n",
       "      <td>0.063791</td>\n",
       "      <td>0.016972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.622690  1.642522  1.724663\n",
       "spearman_corr  0.463316  0.457282  0.187671\n",
       "pearson_corr   0.444923  0.450530  0.209485\n",
       "mae            0.942375  0.944849  1.039607\n",
       "rmse           1.273849  1.281609  1.313264\n",
       "r2             0.075095  0.063791  0.016972"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_metrics = pd.DataFrame({ 'ref':ref_max_test_metrics,\n",
    "                'alt': alt_max_test_metrics, \n",
    "                'altref': altref_max_test_metrics})\n",
    "\n",
    "max_metrics.to_csv('../res/metrics/test_deepset_norho/max_metrics.csv')\n",
    "max_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "489e15e2-f48f-490d-8975-64ee98d4a1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>1.680131</td>\n",
       "      <td>1.654160</td>\n",
       "      <td>1.681668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>0.231788</td>\n",
       "      <td>0.307548</td>\n",
       "      <td>0.242521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>0.207613</td>\n",
       "      <td>0.244855</td>\n",
       "      <td>0.212494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>1.043321</td>\n",
       "      <td>1.038370</td>\n",
       "      <td>1.032396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>1.296199</td>\n",
       "      <td>1.286142</td>\n",
       "      <td>1.296792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.042355</td>\n",
       "      <td>0.057158</td>\n",
       "      <td>0.041478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ref       alt    altref\n",
       "loss           1.680131  1.654160  1.681668\n",
       "spearman_corr  0.231788  0.307548  0.242521\n",
       "pearson_corr   0.207613  0.244855  0.212494\n",
       "mae            1.043321  1.038370  1.032396\n",
       "rmse           1.296199  1.286142  1.296792\n",
       "r2             0.042355  0.057158  0.041478"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_metrics = pd.DataFrame({'ref':ref_token_test_metrics,\n",
    "                'alt': alt_token_test_metrics, \n",
    "                'altref': altref_token_test_metrics})\n",
    "\n",
    "token_metrics.to_csv('../res/metrics/test_deepset_norho/token_metrics.csv')\n",
    "token_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "983f9dfd-18e8-422d-9e23-f4708a103184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ref</th>\n",
       "      <th>alt</th>\n",
       "      <th>altref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>22509.743899</td>\n",
       "      <td>100.776833</td>\n",
       "      <td>11.577704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spearman_corr</th>\n",
       "      <td>-0.040267</td>\n",
       "      <td>0.078616</td>\n",
       "      <td>0.221082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pearson_corr</th>\n",
       "      <td>-0.007305</td>\n",
       "      <td>0.042932</td>\n",
       "      <td>0.185423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>130.014893</td>\n",
       "      <td>6.597245</td>\n",
       "      <td>2.832499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>150.032486</td>\n",
       "      <td>10.038767</td>\n",
       "      <td>3.402603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>-12829.163086</td>\n",
       "      <td>-60.949894</td>\n",
       "      <td>-6.117087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ref         alt     altref\n",
       "loss           22509.743899  100.776833  11.577704\n",
       "spearman_corr     -0.040267    0.078616   0.221082\n",
       "pearson_corr      -0.007305    0.042932   0.185423\n",
       "mae              130.014893    6.597245   2.832499\n",
       "rmse             150.032486   10.038767   3.402603\n",
       "r2            -12829.163086  -60.949894  -6.117087"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_metrics = pd.DataFrame({'ref':ref_sum_test_metrics,\n",
    "                'alt': alt_sum_test_metrics, \n",
    "                'altref': altref_sum_test_metrics})\n",
    "\n",
    "sum_metrics.to_csv('../res/metrics/test_deepset_norho/sum_metrics.csv')\n",
    "sum_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
